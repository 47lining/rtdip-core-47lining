{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Title","text":""},{"location":"api/authentication/","title":"Authentication","text":"<p>RTDIP REST APIs require Azure Active Directory Authentication and passing the token received as an <code>authorization</code> header in the form of a Bearer token. An example of the REST API header is <code>Authorization: Bearer &lt;&lt;token&gt;&gt;</code></p>"},{"location":"api/authentication/#end-user-authentication","title":"End User Authentication","text":"<p>If a developer or business user would like to leverage the RTDIP REST API suite, it is recommended that they use the Identity Packages provided by Azure to obtain a token.</p> <ul> <li>REST API</li> <li>.NET</li> <li>Java</li> <li>Python</li> <li>Javascript</li> </ul> <p>Note</p> <p>Note that the above packages have the ability to obtain tokens for end users and service principals and support all available authentication options. </p> <p>Ensure to install the relevant package and obtain a token.</p> <p>See the examples section to see various authentication methods implemented.</p>"},{"location":"api/examples/","title":"Examples","text":"<p>Below are examples of how to execute APIs using various authentication options and API methods.</p>"},{"location":"api/examples/#end-user-authentication","title":"End User Authentication","text":""},{"location":"api/examples/#python","title":"Python","text":"<p>A python example of obtaining a token as a user can be found below using the <code>azure-identity</code> python package to authenticate with Azure AD.</p> <p>POST Requests</p> <p>The POST request can be used to pass many tags to the API. This is the preferred method when passing large volumes of tags to the API.</p> GET RequestPOST Request <pre><code>from azure.identity import DefaultAzureCredential\nimport requests\n\nauthentication = DefaultAzureCredential()\naccess_token = authentication.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\n\nparams = {\n\"business_unit\": \"Business Unit\",\n\"region\": \"Region\",\n\"asset\": \"Asset Name\",\n\"data_security_level\": \"Security Level\",\n\"data_type\": \"float\",\n\"tag_name\": \"TAG1\",\n\"tag_name\": \"TAG2\",\n\"start_date\": \"2022-01-01\",\n\"end_date\": \"2022-01-01\",\n\"include_bad_data\": True\n}\n\nurl = \"https://example.com/api/v1/events/raw\"\n\npayload={}\nheaders = {\n'Authorization': 'Bearer {}'.format(access_token)\n}\n\nresponse = requests.request(\"GET\", url, headers=headers, params=params, data=payload)\n\nprint(response.json())\n</code></pre> <pre><code>from azure.identity import DefaultAzureCredential\nimport requests\n\nauthentication = DefaultAzureCredential()\naccess_token = authentication.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\n\nparams = {\n\"business_unit\": \"Business Unit\",\n\"region\": \"Region\",\n\"asset\": \"Asset Name\",\n\"data_security_level\": \"Security Level\",\n\"data_type\": \"float\",\n\"start_date\": \"2022-01-01T15:00:00\",\n\"end_date\": \"2022-01-01T16:00:00\",\n\"include_bad_data\": True    \n}\n\nurl = \"https://example.com/api/v1/events/raw\"\n\npayload={\"tag_name\": [\"TAG1\", \"TAG2\"]}\n\nheaders = {\n\"Authorization\": \"Bearer {}\".format(access_token),\n}\n\n# Requests automatically sets the Content-Type to application/json when the request body is passed via the json parameter\nresponse = requests.request(\"POST\", url, headers=headers, params=params, json=payload)\n\nprint(response.json())\n</code></pre>"},{"location":"api/examples/#service-principal-authentication","title":"Service Principal Authentication","text":""},{"location":"api/examples/#get-request","title":"GET Request","text":"<p>Authentication using Service Principals is similar to end user authentication. An example, using Python is provided below where the <code>azure-identity</code> package is not used, instead a direct REST API call is made to retrieve the token.</p> cURLPython <pre><code>curl --location --request POST 'https://login.microsoftonline.com/{tenant id}/oauth2/v2.0/token' \\\n--form 'grant_type=\"client_credentials\"' \\\n--form 'client_id=\"&lt;&lt;client id&gt;&gt;\"' \\\n--form 'client_secret=\"&lt;&lt;client secret&gt;&gt;\"' \\\n--form 'scope=\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\"'\n</code></pre> <pre><code>import requests\n\nurl = \"https://login.microsoftonline.com/{tenant id}/oauth2/v2.0/token\"\n\npayload={'grant_type': 'client_credentials',\n'client_id': '&lt;&lt;client id&gt;&gt;',\n'client_secret': '&lt;&lt;client secret&gt;&gt;',\n'scope': '2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default'}\nfiles=[]\nheaders = {}\n\nresponse = requests.request(\"POST\", url, headers=headers, data=payload, files=files)\n\naccess_token  = response.json()[\"access_token\"]\n\nparams = {\n    \"business_unit\": \"Business Unit\",\n    \"region\": \"Region\",\n    \"asset\": \"Asset Name\",\n    \"data_security_level\": \"Security Level\",\n    \"data_type\": \"float\",\n    \"tag_name\": \"TAG1\",\n    \"tag_name\": \"TAG2\",\n    \"start_date\": \"2022-01-01\",\n    \"end_date\": \"2022-01-01\",\n    \"include_bad_data\": True\n}\n\nurl = \"https://example.com/api/v1/events/raw\"\n\npayload={}\nheaders = {\n'Authorization': 'Bearer {}'.format(access_token)\n}\n\nresponse = requests.request(\"GET\", url, headers=headers, params=params, data=payload)\n\nprint(response.text)\n</code></pre>"},{"location":"api/overview/","title":"Overview","text":""},{"location":"api/overview/#rtdip-rest-apis","title":"RTDIP REST APIs","text":"<p>RTDIP provides REST API endpoints for querying data in the platform. The APIs are a wrapper to the python RTDIP SDK and provide similar functionality for users and applications that are unable to leverage the python RTDIP SDK. It is recommended to read the RTDIP SDK documentation and in particular the Functions section for more information about the options and logic behind each API.</p>"},{"location":"api/rest_apis/","title":"RTDIP REST API Endpoints","text":"<p>RTDIP REST API documentation is available in a number of formats, as described below. </p> <p> </p> <p>RTDIP REST APIs are built to OpenAPI standard 3.0.2. You can obtain the OpenAPI JSON schema at the following endpoint of your deployed APIs <code>https://{domain name}/api/openapi.json</code></p> <p> </p> <p>It is recommended to review the Swagger documentation that can be found at the following endpoint of your deployed APIs <code>https://{domain name}/docs</code> for more information about the parameters and options for each API. It is also possible to try out each API from this link.</p> <p> </p> <p>Additionally, further information about each API can be found in Redoc format at  the following endpoint of your deployed APIs <code>https://{domain name}/redoc</code></p>"},{"location":"api/deployment/azure/","title":"Deploy RTDIP APIs to Azure","text":"<p>The RTDIP repository contains the code to deploy the RTDIP REST APIs to your own Azure Cloud environment. The APIs are built as part of the rtdip repository CI/CD pipelines and the image is deployed to Docker Hub repo <code>rtdip/api</code>. Below contains information on how to build and deploy the containers from source or to setup your function app to use the deployed container image provided by RTDIP.</p>"},{"location":"api/deployment/azure/#deploying-the-rtdip-apis","title":"Deploying the RTDIP APIs","text":""},{"location":"api/deployment/azure/#deployment-from-build","title":"Deployment from Build","text":"<p>To deploy the RTDIP APIs directly from the repository, follow the steps below:</p> <ol> <li>Build the docker image using the following command:     <pre><code>docker build --tag &lt;container_registry_url&gt;/rtdip-api:v0.1.0 -f src/api/Dockerfile .\n</code></pre></li> <li>Login to your container registry     <pre><code>docker login &lt;container_registry_url&gt;\n</code></pre></li> <li>Push the docker image to your container registry     <pre><code>docker push &lt;container_registry_url&gt;/rtdip-api:v0.1.0\n</code></pre></li> <li>Configure your Function App to use the docker image     <pre><code>az functionapp config container set --name &lt;function_app_name&gt; --resource-group &lt;resource_group_name&gt; --docker-custom-image-name &lt;container_registry_url&gt;/rtdip-api:v0.1.0\n</code></pre></li> </ol>"},{"location":"api/deployment/azure/#deployment-from-docker-hub","title":"Deployment from Docker Hub","text":"<p>To deploy the RTDIP APIs from Docker Hub, follow the steps below:</p> <ol> <li>Configure your Function App to use the docker image     <pre><code>az functionapp config container set --name &lt;function_app_name&gt; --resource-group &lt;resource_group_name&gt; --docker-custom-image-name rtdip/api:azure-&lt;version&gt;\n</code></pre></li> </ol>"},{"location":"api/deployment/azure/#environment-variables","title":"Environment Variables","text":""},{"location":"api/deployment/azure/#azure-active-directory","title":"Azure Active Directory","text":"<ol> <li>Once Authentication has been configured on the Azure Function App correctly, it is required to set the following Environment Variable with the Tenant ID of the relevant Active Directory:<ul> <li>TENANT_ID</li> </ul> </li> </ol>"},{"location":"api/deployment/azure/#databricks","title":"Databricks","text":"<ol> <li>The following Environment Variables are required and the values can be retrieved from your Databricks SQL Warehouse or Databricks Cluster:<ul> <li>DATABRICKS_SQL_SERVER_HOSTNAME</li> <li>DATABRICKS_SQL_HTTP_PATH</li> </ul> </li> </ol>"},{"location":"api/deployment/azure/#odbc-driver","title":"ODBC Driver","text":"<ol> <li>To allow the APIs to leverage Turbodbc for connectivity and possible performance improvements, it is possible to set the following environment variable:<ul> <li>RTDIP_ODBC_CONNECTION=turbodbc</li> </ul> </li> </ol>"},{"location":"blog/2022/11/01/delta-lakehouse-and-real-time-data-ingestion-platform/","title":"Delta Lakehouse and Real Time Data Ingestion Platform","text":"<p>Real Time Data Ingestion Platform leverages Delta and the concept of a Lakehouse to ingest, store and manage it's data. There are many benefits to Delta for performing data engineering tasks on files stored in a data lake including ACID transactions, maintenance, SQL query capability and performance at scale. To find out more about Delta Lakehouse please see here.</p> <p>The Real Time Data Ingestion Platform team would like to share some lessons learnt from the implementation of Delta and the Lakehouse concept so that hopefully it helps others on their Delta Lakehouse journey.</p> <p>For reference, please consider the typical layout of timeseries data ingested by RTDIP:</p> <p>Events</p> Column Name Description TagName Typically represents a sensor name or a measurement EventTime A timestamp for a recorded value Status Status of the recording, normally indicating if the measurement value is good or bad Value The value of the measurement and can be of a number of types - float, double, string, integer <p>Metadata</p> Column Name Description TagName Typically represents a sensor name or a measurement Description A description for the sensor UoM UoM for the measurement <p>Note</p> <p>Metadata can include a number of additional columns and depends on the system that provides the metadata. The above are the required columns for any sensor data ingested by RTDIP.</p>","tags":["Pipelines","Delta"]},{"location":"blog/2022/11/01/delta-lakehouse-and-real-time-data-ingestion-platform/#design-considerations","title":"Design Considerations","text":"<p>Delta, in its simplest definition, is a set of parquet files managed by an index file. This allows Spark to perform tasks like partition pruning and file pruning to find the exact parquet file to be used by any ACID transactions being performed on it. By reducing the number of files and the amount of data that Spark needs to read in a query means that it will perform much better. It is important to consider the following when designing a Delta Table to achieve performance benefits:</p> <ul> <li>Columns that are likely to be used in most reads and writes</li> <li>Partition column(s)</li> <li>File Sizes</li> </ul>","tags":["Pipelines","Delta"]},{"location":"blog/2022/11/01/delta-lakehouse-and-real-time-data-ingestion-platform/#partition-columns","title":"Partition Columns","text":"<p>The biggest benefit achieved using Delta is to include a partition column in the design of a Delta Table. This is the fastest way for Spark to isolate the dataset it needs to work with in a query. The general rule of thumb is that each partition size should be roughly 1gb in size, and ideally would be a column or columns that are used in every query to filter data for that table.</p> <p>This can be difficult to achieve. The most queried columns in RTDIP event data are TagName and EventTime, however, partitioning data by TagName creates far too many small partitions and a timestamp column like EventTime can not be used for partitioning for the same reason. The best outcome is typically to create an additional column that is an aggregation of the EventTime column, such as a Date, Month or Year Column, depending on the frequency of the data being ingested. </p> <p>Note</p> <p>Given the above, always query RTDIP delta events tables using EventDate in the filter to achieve the best results.</p> <p>One of the best methods to analyse Spark query performance is to analyse the query plan of a query. It is essential that a Spark query plan leverages a partition column. This can be identified by reviewing the query plan in Spark. As per the below query plan, it can be seen that for this particular query only one partition was read by Spark. Make sure to try different queries to identify that the expected number of partitions are being used by Spark in every query. If it does not match your expected number of partitions, it is important to investigate why partition pruning is not being leveraged in your query. </p> <p> </p>","tags":["Pipelines","Delta"]},{"location":"blog/2022/11/01/delta-lakehouse-and-real-time-data-ingestion-platform/#zorder-columns","title":"ZORDER Columns","text":"<p>Even though the rule is to achieve roughly 1gb partitions for a Delta Table, Delta is likely to divide that partition into a number of files. The default target size is around 128gb per file. Due to this, it is possible to improve performance above and beyond partitioning by telling Spark which files within in a partition to read. This is where ZORDER becomes useful. </p> <p>Zordering organises the data within each file, and along with the Delta Index file, directs Spark to the exact files to use in its reads(and merge writes) on the table. It is important to find the right number of columns to ZORDER - the best outcome would be a combination of columns that does not cause the index to grow too large. For example, ZORDERING by TagName creates a small index, but ZORDERING by TagName and EventTime created a huge index as there are far more combinations to be indexed.</p> <p>The most obvious column to ZORDER on in RTDIP is the TagName as every query is likely to use this in its filter. Like partition pruning, it is possible to identify the impact of ZORDERING on your queries by reviewing the files read attribute in the query plan. As per the query plan below, you can see that two files were read within the one partition.</p> <p> </p>","tags":["Pipelines","Delta"]},{"location":"blog/2022/11/01/delta-lakehouse-and-real-time-data-ingestion-platform/#merge-and-file-sizes","title":"MERGE and File Sizes","text":"<p>As stated above, the default target size for file sizes within a partition is 128gb. However, this is not always ideal and in certain scenarios, it is possible to improve performance of Spark jobs by reducing file sizes in certain scenarios: - MERGE commands - Queries that target very small subsets of data within a file</p> <p>Due to the nature of Merges, its typically an action where small updates are being made to the dataset. Due to this, it is possible to get much better MERGE performance by setting the following attribute on the Delta Table <code>delta.tuneFileSizesForRewrites=true</code>. This targets smaller file sizes to reduce the amount of data in each read a MERGE operation performs on the data. RTDIP gained a significant performance improvement in reading and writing and was able to reduce the Spark cluster size by half by implementing this setting on its Delta Tables.</p> <p>However, even more performance gain was achieved when Databricks released Low Shuffle Merge from DBR 9.0 onwards. This assists Spark to merge data into files without disrupting the ZORDER layout of Delta files, in turn assisting Merge commands to continue leveraging ZORDER performance benefits on an ongoing basis. RTDIP was able to improve MERGE performance by 5x with this change. To leverage Low Shuffle Merge, set the following Spark config in your notebook <code>spark.databricks.delta.merge.enableLowShuffle=true</code>.</p>","tags":["Pipelines","Delta"]},{"location":"blog/2022/11/01/delta-lakehouse-and-real-time-data-ingestion-platform/#delta-table-additional-attributes","title":"Delta Table Additional Attributes","text":"<p>It is recommended to consider setting the following two attributes on all Delta Tables:</p> <ul> <li><code>delta.autoOptimize.autoCompact=true</code></li> <li><code>delta.autoOptimize.optimizeWrite=true</code></li> </ul> <p>To understand more about optimization options you can set on Delta Tables, please refer to this link.</p>","tags":["Pipelines","Delta"]},{"location":"blog/2022/11/01/delta-lakehouse-and-real-time-data-ingestion-platform/#maintenance-tasks","title":"Maintenance Tasks","text":"<p>One important step to be included with every Delta Table is maintenance. Most developers forget these very important maintenance tasks that need to run on a regular basis to maintain performance and cost on Delta Tables.</p> <p>As a standard, run a maintenance job every 24 hours to perform OPTIMIZE and VACUUM commands on Delta Tables.</p>","tags":["Pipelines","Delta"]},{"location":"blog/2022/11/01/delta-lakehouse-and-real-time-data-ingestion-platform/#optimize","title":"OPTIMIZE","text":"<p>OPTIMIZE is a Spark SQL command that can be run on any Delta Table and is the simplest way to optimize the file layout of a Delta Table. The biggest benefit of running OPTIMIZE however, is to organize Delta files using ZORDER. Due to how effective ZORDER is on queries, its unlikely that OPTIMIZE would not be executed on a Delta Table regularly.</p> <p>It may be a question as to why one would run OPTIMIZE as well as set <code>delta.autoOptimize.autoCompact=true</code> on all its tables. Auto Compact does not ZORDER its data(at the time of writing this article), its task is simply to attempt to create larger files during writing and avoid the small file problem. Therefore, autoCompact does not provide ZORDER capability. Due to this, consider an OPTIMIZE strategy as follows: - Auto Compact is used by default for any new files written to an RTDIP Delta Table between the execution of maintenance jobs. This ensures that any new data ingested by RTDIP is still being written in a suitable and performant manner. - OPTIMIZE with ZORDER is run on a daily basis on any partitions that have changed(excluding the current day) to ensure ZORDER and updating of the Delta Index file is done. </p> <p>Note</p> <p>RTDIP data is going to typically be ingesting using Spark Streaming - given the nature of a real time data ingestion platform, it makes sense that data ingestion is performed in real time. One complication this introduces is the impact of the OPTIMIZE command being executed at the same time as files being written to a partition. Due to this, execute OPTIMIZE on partitions where the EventDate is not equal to the current date, minimizing the possibility of an OPTIMIZE command and a file write command being executed on a partition at the same time. This logic reduces issues experienced by both the maintenance job and Spark streaming job.</p>","tags":["Pipelines","Delta"]},{"location":"blog/2022/11/01/delta-lakehouse-and-real-time-data-ingestion-platform/#vacuum","title":"VACUUM","text":"<p>One of the most powerful features of Delta is time travel. This allows querying of data as at a certain point of time in the past, or a particular version of the Delta Table. Whilst incredibly useful, it does consume storage space and if these historical files are never removed, the size of Delta Tables can grow exponentially large and increase cost.</p> <p>To ensure only the required historical versions of a Delta Table are stored, its important to execute the VACUUM command every 24 hours. This deletes any files or versions that are outside the time travel retention period.</p>","tags":["Pipelines","Delta"]},{"location":"blog/2022/11/01/delta-lakehouse-and-real-time-data-ingestion-platform/#conclusion","title":"Conclusion","text":"<p>Delta and the Lakehouse transformed the way RTDIP ingests its data and provides integration with other projects, applications and platforms. We hope the above assists others with their Delta development and we look forward to posting more content on RTDIP and its use of Spark in the future.</p>","tags":["Pipelines","Delta"]},{"location":"blog/2023/11/27/beyond-the-traditional-data-historian/","title":"Beyond the Traditional Data Historian","text":"<p>The Fourth Industrial Revolution, commonly known as IR4.0, has ushered in profound transformations in the landscape of industrial operations and manufacturing. Digital technologies and data derived from sensors are driving a wide array of innovations, spanning from advanced analytics and machine learning to the realms of augmented and virtual reality models.</p> <p>Handling sensor-based data poses a unique challenge for conventional relational databases, which is why data historians were originally conceived in the latter part of the 1980s. They were specifically designed for integration with industrial automation systems like SCADA (supervisory control and data acquisition). Initially, their primary application was within the process manufacturing sector, encompassing industries such as oil and gas, chemicals, pharmaceuticals, pipelines, and refining.</p> <p>This Historian system was developed as an ecosystem that provided a comprehensive solution, ranging from data interface software to data storage and data visualization. The industry 4.0 revolution has spurred automation in manufacturing, leveraging smart sensors and IoT devices to capture real-time data from the field. Furthermore, it has seen the increased utilization of Artificial Intelligence and Machine Learning for predictive analytics and decision support, both of which are data-hungry applications. So, with IR4.0 development what are the options available to cater requirements for real time data ingestion.</p>"},{"location":"blog/2023/11/27/beyond-the-traditional-data-historian/#what-are-the-alternatives","title":"What Are the Alternatives?","text":"<p>Hyperscalers have become a central part of global IT infrastructure and essential to operations and they also started providing industrial connectivity options out of many in this article I am going to focus on Azure , OPC UA &amp; Open source stack which provides you all the options of data ingestion from OPC UA source to your cloud database.</p> <p> </p>"},{"location":"blog/2023/11/27/beyond-the-traditional-data-historian/#components","title":"Components","text":""},{"location":"blog/2023/11/27/beyond-the-traditional-data-historian/#microsoft-opc-publisher-as-interface","title":"Microsoft OPC Publisher as Interface","text":"<p>OPC Publisher is a Microsoft-endorsed solution that acts as a vital link between industrial equipment and the Azure cloud infrastructure. Its core function involves linking OPC UA-enabled devices or industrial connectivity software to your Azure cloud environment, transmitting collected telemetry data to Azure IoT Hub in multiple formats, one of which is the standardized IEC62541 OPC UA PubSub format (from version 2.6 onwards). </p> <p>OPC Publisher is versatile in its deployment options, running either as an Azure IoT Edge Module or a Docker container on various platforms, thanks to its compatibility with the .NET cross-platform runtime. Notably, it can seamlessly operate on both Linux and Windows 10 systems.</p> <p>Refer to Microsoft OPC Publisher documentation for more information.</p>"},{"location":"blog/2023/11/27/beyond-the-traditional-data-historian/#rtdip-for-data-engineering","title":"RTDIP for Data Engineering","text":"<p>The Real-Time Data Ingestion Platform (RTDIP) is designed to offer convenient access to large volumes of historical and live operational data for analytical purposes, serving engineers and data scientists no matter where they are located.</p>"},{"location":"blog/2023/11/27/beyond-the-traditional-data-historian/#key-components-of-rtdip","title":"Key Components of RTDIP:","text":""},{"location":"blog/2023/11/27/beyond-the-traditional-data-historian/#delta-ingestion-engine","title":"Delta Ingestion Engine:","text":"<p>This component operates in the cloud and is responsible for processing time series data sourced from various streaming endpoints such as Eventhub, Kafka, and others, as well as data from files. It ingests and organizes this data into a Delta Lakehouse.</p> <p>To simplify the data ingestion from IoT hub, RTDIP has built Transformer components to convert the payload's defaultmessage encoding to json. The architecture above is using the transformer  OPCPublisherOPCUAJsonToPCDMTransformer to handle that scenario.</p>"},{"location":"blog/2023/11/27/beyond-the-traditional-data-historian/#query-engine","title":"Query Engine","text":"<p>Python SDK: The Python Software Development Kit (SDK) empowers users to interact with the data stored within the Delta Lakehouse. It provides a programmatic interface for working with the data.</p>"},{"location":"blog/2023/11/27/beyond-the-traditional-data-historian/#rest-apis","title":"REST APIs","text":"<p>In addition to the Python SDK, RTDIP offers RESTful Application Programming Interfaces (APIs) that deliver the same capabilities as the Python SDK. These APIs enable users to interact  and manipulate the data stored in the Delta Lakehouse through HTTP requests, making it accessible for a wide range of applications and services.</p> <p>See Real Time Data Ingestion Platform for more information.</p>"},{"location":"blog/2023/11/27/beyond-the-traditional-data-historian/#visualization","title":"Visualization","text":"<p>Real-time data visualization is a dynamic and essential component of data analytics and monitoring processes. It involves the presentation of data as it is generated or updated in real-time, allowing users to gain instant insights and make informed decisions. But for visualization there is always different requirement, in current open source scenario we have different options available which can be used as required:</p> <ul> <li>Plotly</li> <li>Dash</li> <li>Grafana</li> </ul>"},{"location":"blog/2023/11/27/beyond-the-traditional-data-historian/#conclusion","title":"Conclusion","text":"<p>To conclude this article, it is important to emphasize the wide array of open-source options at the disposal of historians. In today\u2019s market, numerous open-source components can be harnessed to create bespoke solutions that seamlessly link with your process control systems, enabling the integration of real-time data into modern data platforms. While this article focused on Azure as an illustration, it\u2019s worth noting that AWS offers similar capabilities, providing flexibility and adaptability to cater to your specific needs.</p>"},{"location":"blog/2023/06/21/rtdip-generative-ai-sql-agent/","title":"Rtdip generative ai sql agent","text":"<p>"},{"location":"blog/2023/06/21/rtdip-generative-ai-sql-agent/#rtdip-generative-ai-sql-agent","title":"RTDIP Generative AI SQL Agent","text":"<p>Generative AI is changing how we think about data, particularly the knowledge it can unlock from unstructured data that simply wasn't possible before. However, it's also peaked our curiosity about structured data - Can Generative AI also query structured data? Could it query time series data to answer questions such as:</p> <p><code>What was the average actual power generated by Turbine 1 at ACME Wind Farm on 6 May?\"</code></p> <p>Considering this, RTDIP Developers have been conducting experiments to see how Generative AI could help query structured data and found some very exciting and novel approaches that are available for you to try out in the RTDIP SDK v0.5.0! </p> <p>Let's explore how Chat GPT and langchain can execute SQL queries on tables in Databricks using questions like the above. Keep in mind that you can try this out on any data in Databricks, it is not limited to data ingested by RTDIP pipelines.</p> <p>Note</p> <p>This is experimental and you will likely experience variable responses to your questions depending on the complexity of the data you use in this setup. Start small, with only a 2 - 3 tables before scaling up.</p>"},{"location":"blog/2023/06/21/rtdip-generative-ai-sql-agent/#overview","title":"Overview","text":"<p>Langchain is making it much simpler to leverage Large Language Models(LLMs) in various contexts, such as AI powered SQL Agents, through toolkits. RTDIP SDK v0.5.0 takes advantage of the Langchain SQL Database Agent which can build sql queries from text questions and execute them on Databricks. It is worthwhile spending time reading the langchain documentation, the available toolkits and the SQL Database Agent used by RTDIP SDK to understand in more detail how it technically works.</p> <p>Let's get straight to it.</p>"},{"location":"blog/2023/06/21/rtdip-generative-ai-sql-agent/#prerequisites","title":"Prerequisites","text":"<ol> <li>Obtain an Open AI API key. Register on the Open AI platform to obtain an Open AI API key, follow these instructions to obtain your API key and familiarize yourself with the Open AI documentation. </li> <li>Access to Databricks SQL or a Databricks Cluster and data stored as tables in Unity Catalog or Hive Metastore. </li> </ol> <p>Warning</p> <p>Consider the implications of exposing your data with Open AI models and seek approval prior to registering with any of these services. </p>"},{"location":"blog/2023/06/21/rtdip-generative-ai-sql-agent/#setup-the-sql-ai-agent","title":"Setup the SQL AI Agent","text":"<p>With all the prerequisites in place, it's time to setup the SQL AI Agent.</p> <p>Firstly, import the required RTDIP SDK components</p> <pre><code>from rtdip_sdk.connectors import ChatOpenAIDatabricksConnection\n</code></pre> <p>Next, configure the component with all the relevant connection information:</p> <pre><code>agent = ChatOpenAIDatabricksConnection(\n    catalog=\"&lt;databricks catalog&gt;\", \n    schema=\"&lt;databricks schema&gt;\", \n    server_hostname=\"&lt;databricks host name&gt;\",                   \n    http_path=\"&lt;databricks http path&gt;\",                         \n    access_token=\"&lt;Azure AD token or databricks PAT token&gt;\",\n    openai_api_key=\"&lt;Open AI API key&gt;\",\n    openai_model = \"gpt-4\",                                     \n    sample_rows_in_table_info = 5, \n    verbose_logging = True\n)\n</code></pre> <p>Some notes on the above:</p> <ul> <li><code>server_hostname</code> and <code>http_path</code> can be obtained from your Databricks SQL Warehouse or Databricks Cluster</li> <li><code>access_token</code> can be either a Databricks PAT Token or Azure AD Token. To obtain an Azure AD token, please refer to this documentation</li> <li><code>open_ai_model</code> defaults to <code>gpt-4</code> but is not easily available at the time of writing. Alternatively, the <code>gpt-3.5-turbo-16k-0613</code> has worked well in our tests</li> <li><code>sample_rows_in_table_info</code> limits the number of rows queried in a table when the SQL Database Agent is looking context in the data. Be careful to not increase this too much as its then possible to exceed token limits on the gpt models</li> </ul>"},{"location":"blog/2023/06/21/rtdip-generative-ai-sql-agent/#execute-queries","title":"Execute Queries","text":"<p>With the <code>ChatOpenAIDatabricksConnection</code> class configured, its now possible to execute queries. Try out some questions with the following code:</p> <pre><code>response = agent.run(\"What was the average actual power generated by Turbine 1 at ACME Wind Farm on 6 May?\")\nprint(response)\n</code></pre> <p>And the answer:</p> <pre><code>\"The average power generated for turbine 1 at Acme on 6 May is 847.4923418083226.\"\n</code></pre>"},{"location":"blog/2023/06/21/rtdip-generative-ai-sql-agent/#how-did-it-work-out-the-answer","title":"How did it work out the answer?","text":"<p>It's possible to trace how the SQL AI Agent determined the answer using the logs. To switch on the logs, set <code>verbose_logging = True</code> in <code>ChatOpenAIDatabricksConnection</code>. </p> <p>Below are the logs generated by the SQL AI Agent to solve the above question:</p> <pre><code>Action: list_tables_sql_db\nAction Input: \"\"\nObservation: acme_events, descartes_events\n\nThought: I can query the table acme_events. I should check the schema of this table to see if it contains the information I need.\nAction: schema_sql_db\nAction Input: \"acme_events\"\nObservation: \nCREATE TABLE acme_events (\n`TagName` STRING NOT NULL, `Source` STRING NOT NULL, `EventDate` DATE NOT NULL, `EventTime` TIMESTAMP NOT NULL, `Status` STRING, `Value` FLOAT\n) USING DELTA\n\n/*\n5 rows from acme_events table:\nTagName Source  EventDate       EventTime       Status  Value\nZZ:ACM_WF_WTG02_analog_GriActPwrMaxMet  2023-04-03      2023-04-03 11:26:16.525000+00:00        Good    4100.0\nZZ:ACM_WF_WTG02_analog_GriActPwrMaxMan  2023-04-03      2023-04-03 11:26:16.525000+00:00        Good    4100.0\nZZ:ACM_WF_WTG02_analog_GriActPwrMaxWtgProt      2023-04-03      2023-04-03 11:26:16.525000+00:00        Good    3660.0\nZZ:ACM_WF_WTG02_analog_GriActPwrMaxGcmErr       2023-04-03      2023-04-03 11:26:16.525000+00:00        Good    4100.0\nZZ:ACM_WF_WTG02_analog_GriActPwrMaxGcmVer       2023-04-03      2023-04-03 11:26:16.525000+00:00        Good    4100.0\n*/\n\nThought: I can query the table \"acme_events\" to find the average power generated for turbine 1 at Acme on 6 May. I will use the \"query_sql_db\" action to execute the query.\nAction: query_sql_db\nAction Input: \"SELECT AVG(Value) FROM acme_events WHERE TagName = 'ZZ:ACM_WF_WTG01_analog_PwrAct' AND EventDate = '2023-05-06'\"\nObservation: [(847.4923418083226,)]\n\nThought: The average power generated for turbine 1 at Acme on 6 May is 847.4923418083226.\nFinal Answer: 847.4923418083226\n</code></pre> <p>What is the most interesting from the above is how the SQL AI Agent is able to work out the <code>TagName</code> from the available data to then construct the right SQL statement and filters! </p> <p>Now try some additional queries, such as:</p> <ul> <li><code>What data is available?</code></li> <li><code>How many rows are available for ACME?</code></li> <li><code>How many tags are there for Descartes?</code></li> </ul>"},{"location":"blog/2023/06/21/rtdip-generative-ai-sql-agent/#limitations","title":"Limitations","text":"<p>The biggest limitation experienced to date is the token limit on the Chat GPT models. These limits can be quickly exceeded as the SQL AI Agent queries the data and uses the responses as tokens to find context and answers to questions. Some of this can be reduced by using the Chat GPT models that allow for higher token limits, like the <code>gpt-3.5-turbo-16k-0613</code> model. However, if the Databricks schema has many tables and needs to do a number queries to determine the answer, 16k tokens is still easily exceeded.</p> <p>We did not experience much cross querying of tables to solve answers. For example, the agent did not try to query a table of users to get their User IDs from their provided name to then solve a question for that user id in another table. This seems like an obvious next step and is likely be improved soon.</p> <p>The answers are not always the same and asking the same question multiple times did not always provide the same answer(however, because its an Intelligence based solution, there could be good reasons for that).</p>"},{"location":"blog/2023/06/21/rtdip-generative-ai-sql-agent/#next-steps","title":"Next Steps","text":"<p>First and foremost, we want to be able to run the langchain SQL agent on self hosted, open source LLMs. This could significantly reduce challenges around exposure of data to public models, allow for development of LLMs that are built specifically for these SQL based tasks and in turn can increase limits around tokens. We are exploring options in this domain, such as Mosaic's MPT LLM running on self-hosted GPUs. Please reach out if you know LLMs that work well in the context of SQL queries. We will build additional connectors if suitable open source LLMs become or are available.</p> <p>We would also like to see an increase in accuracy. The model and approach needs fine tuning to get more accurate answers:</p> <ul> <li>Langchain are actively developing the SQL Database Agent and this will no doubt improve the complexity and capability of the SQL Database Agent over time</li> <li>Microsoft will continue to improve their GPT Models to construct better and more complex sql queries</li> </ul>"},{"location":"blog/2023/06/21/rtdip-generative-ai-sql-agent/#conclusion","title":"Conclusion","text":"<p>Watching the langchain SQL Database Agent solve problems with no input or context from us was a very exciting moment. To see it explore tables, construct SQL queries and attempt to decipher patterns in text columns to attempt to answer non trivial questions about specific tags or dimensions was fascinating and far more advanced than we imagined. It is fair to conclude that Generative AI is going to play a significant role in data, structured and unstructured, and it will interesting to see how data platforms adopt this capability into their base services and across the different layers of the data landscape. RTDIP will continue to research and provide capability in this area so stay in touch to get the latest updates about RTDIP and its use of Generative AI.</p>"},{"location":"blog/2023/02/24/rtdip-ingestion-pipelines/","title":"Rtdip ingestion pipelines","text":"<p>"},{"location":"blog/2023/02/24/rtdip-ingestion-pipelines/#rtdip-ingestion-pipeline-framework","title":"RTDIP Ingestion Pipeline Framework","text":"<p>RTDIP has been built to simplify ingesting and querying time series data. One of the most anticipated features of the Real Time Data Ingestion Platform for 2023 is the ability to create streaming and batch ingestion pipelines according to requirements of the source of the data and needs of the data consumer. Of equal importance is the need to query this data and an article that focuses on egress will follow in due course. </p>"},{"location":"blog/2023/02/24/rtdip-ingestion-pipelines/#overview","title":"Overview","text":"<p>The goal of the RTDIP Ingestion Pipeline framework is:</p> <ol> <li>Support python and pyspark to build pipeline components</li> <li>Enable execution of sources, transformers, sinks/destinations and utilities components in a framework that can execute them in a defined order</li> <li>Create modular components that can be leveraged as a step in a pipeline task using Object Oriented Programming techniques included Interfaces and Implementations per component type</li> <li>Deploy pipelines to popular orchestration engines</li> <li>Ensure pipelines can be constructed and executed using the RTDIP SDK and rest APIs</li> </ol>"},{"location":"blog/2023/02/24/rtdip-ingestion-pipelines/#pipeline-jobs","title":"Pipeline Jobs","text":"<p>The RTDIP Data Ingestion Pipeline Framework will follow the typical convention of a job that users will be familiar with if they have used orchestration engines such as Apache Airflow or Databricks Workflows.</p> <p>A pipeline job consists of the following components:</p> <pre><code>erDiagram\n  JOB ||--|{ TASK : contains\n  TASK ||--|{ STEP : contains\n  JOB {\n    string name\n    string description\n    list task_list\n  }\n  TASK {\n    string name\n    string description\n    string depends_on_task\n    list step_list\n    bool batch_task\n  }\n  STEP {\n    string name\n    string description\n    list depends_on_step\n    list provides_output_to_step\n    class component\n    dict component_parameters\n  }</code></pre> <p>As per the above, a pipeline job consists of a list of tasks. Each task consists of a list of steps. Each step consists of a component and a set of parameters that are passed to the component. Dependency Injection will ensure that each component is instantiated with the correct parameters.</p>"},{"location":"blog/2023/02/24/rtdip-ingestion-pipelines/#pipeline-runtime-environments","title":"Pipeline Runtime Environments","text":"Python Apache Spark Databricks Delta Live Tables <p>Pipelines will be able to run in multiple environment types. These will include:</p> <ul> <li>Python: Components will be written in python and executed on a python runtime</li> <li>Pyspark: Components will be written in pyspark and executed on an open source Apache Spark runtime</li> <li>Databricks: Components will be written in pyspark and executed on a Databricks runtime</li> <li>Delta Live Tables: Components will be written in pyspark and executed on a Databricks runtime and will write to Delta Live Tables</li> </ul> <p>Runtimes will take precedence depending on the list of components in a pipeline task.</p> <ul> <li>Pipelines with at least one Databricks or DLT component will be executed in a Databricks environment</li> <li>Pipelines with at least one Pyspark component will be executed in a Pyspark environment</li> <li>Pipelines with only Python components will be executed in a Python environment</li> </ul>"},{"location":"blog/2023/02/24/rtdip-ingestion-pipelines/#pipeline-clouds","title":"Pipeline Clouds","text":"<p>Certain components are related to cloud providers and in the tables below, it is indicated which cloud provider is related to its specific component. It does not mean that the component can only run in that cloud, instead its highlighting that the component is related to that cloud provider.</p> Cloud Target Azure Q1-Q2 2023 AWS Q2-Q4 2023 GCP 2024"},{"location":"blog/2023/02/24/rtdip-ingestion-pipelines/#pipeline-orchestration","title":"Pipeline Orchestration","text":"Airflow Databricks Dagster <p>Pipelines will be able to be deployed to orchestration engines so that users can schedule and execute jobs using their preferred orchestration engine.</p> Orchestration Engine Target Databricks Workflows Q2 2023 Airflow Q2 2023 Delta Live Tables Q3 2023 Dagster Q4 2023"},{"location":"blog/2023/02/24/rtdip-ingestion-pipelines/#pipeline-components","title":"Pipeline Components","text":"<p>The Real Time Data Ingestion Pipeline Framework will support the following components:</p> <ul> <li>Sources - connectors to source systems</li> <li>Transformers - perform transformations on data, including data cleansing, data enrichment, data aggregation, data masking, data encryption, data decryption, data validation, data conversion, data normalization, data de-normalization, data partitioning etc</li> <li>Destinations - connectors to sink/destination systems </li> <li>Utilities - components that perform utility functions such as logging, error handling, data object creation, authentication, maintenance etc</li> <li>Edge - components that will perform edge functionality such as connectors to protocols like OPC</li> </ul>"},{"location":"blog/2023/02/24/rtdip-ingestion-pipelines/#pipeline-component-types","title":"Pipeline Component Types","text":"Python Apache Spark Databricks <p>Component Types determine system requirements to execute the component:</p> <ul> <li>Python - components that are written in python and can be executed on a python runtime</li> <li>Pyspark - components that are written in pyspark can be executed on an open source Apache Spark runtime</li> <li>Databricks - components that require a Databricks runtime</li> </ul>"},{"location":"blog/2023/02/24/rtdip-ingestion-pipelines/#sources","title":"Sources","text":"<p>Sources are components that connect to source systems and extract data from them. These will typically be real time data sources, but will also support batch components as these are still important and necessary data souces in a number of circumstances in the real world.</p> Source Type Python Apache Spark Databricks Azure AWS Target Delta * Q1 2023 Delta Sharing * Q1 2023 Autoloader Q1 2023 Eventhub * Q1 2023 IoT Hub * Q2 2023 Kafka Q2 2023 Kinesis Q2 2023 SSIP PI Connector Q2 2023 Rest API Q2 2023 MongoDB Q3 2023 <p>* - target to deliver in the following quarter</p> <p>There is currently no spark connector for IoT Core. If you know a way to add it as a source component, please raise it by creating an issue on the GitHub repo.</p>"},{"location":"blog/2023/02/24/rtdip-ingestion-pipelines/#transformers","title":"Transformers","text":"<p>Transformers are components that perform transformations on data. These will target certain data models and common transformations that sources or destination components require to be performed on data before it can be ingested or consumed.</p> Transformer Type Python Apache Spark Databricks Azure AWS Target Eventhub Body Q1 2023 OPC UA Q2 2023 OPC AE Q2 2023 SSIP PI Q2 2023 OPC DA Q3 2023 <p>* - target to deliver in the following quarter</p> <p>This list will dynamically change as the framework is developed and new components are added.</p>"},{"location":"blog/2023/02/24/rtdip-ingestion-pipelines/#destinations","title":"Destinations","text":"<p>Destinations are components that connect to sink/destination systems and write data to them. </p> Destination Type Python Apache Spark Databricks Azure AWS Target Delta Append * Q1 2023 Eventhub * Q1 2023 Delta Merge Q2 2023 Kafka Q2 2023 Kinesis Q2 2023 Rest API Q2 2023 MongoDB Q3 2023 Polygon Blockchain Q3 2023 <p>* - target to deliver in the following quarter</p>"},{"location":"blog/2023/02/24/rtdip-ingestion-pipelines/#utilities","title":"Utilities","text":"<p>Utilities are components that perform utility functions such as logging, error handling, data object creation, maintenance and are normally components that can be executed as part of a pipeline or standalone.</p> Utility Type Python Apache Spark Databricks Azure AWS Target Delta Table Create * Q1 2023 Delta Optimize Q2 2023 Delta Vacuum * Q2 2023 Set ADLS Gen2 ACLs Q2 2023 Set S3 ACLs Q2 2023 Great Expectations Q3 2023 <p>* - target to deliver in the following quarter</p>"},{"location":"blog/2023/02/24/rtdip-ingestion-pipelines/#secrets","title":"Secrets","text":"<p>Secrets are components that perform authentication functions and are normally components that can be executed as part of a pipeline or standalone.</p> Secrets Type Python Apache Spark Databricks Azure AWS Target Databricks Secrets Q2 2023 Hashicorp Vault Q2 2023 Azure Key Vault Q3 2023 AWS Secrets Manager Q3 2023"},{"location":"blog/2023/02/24/rtdip-ingestion-pipelines/#edge","title":"Edge","text":"<p>Edge components are designed to provide a lightweight, low latency, low resource consumption, data ingestion framework for edge devices. These components will be designed to run on edge devices such as Raspberry Pi, Jetson Nano, etc. For cloud providers, this will be designed to run on AWS Greengrass and Azure IoT Edge.</p> Edge Type Azure IoT Edge AWS Greengrass Target OPC CloudPublisher Q3-Q4 2023 Fledge Q3-Q4 2023 Edge X Q3-Q4 2023"},{"location":"blog/2023/02/24/rtdip-ingestion-pipelines/#conclusion","title":"Conclusion","text":"<p>This is a very high level overview of the framework and the components that will be developed. As the framework is open source, the lists defined above and timelines can change depending on circumstances and resource availability. Its an exciting year for 2023 for the Real Time Data Ingestion Platform. Check back in regularly for updates and new features! If you would like to contribute, please visit our repository on Github and connect with us on our Slack channel on the LF Energy Foundation Slack workspace.</p>"},{"location":"domains/process_control/data_model/","title":"Process Control Data Model","text":"<p>The Process Control Data Model consists of two key components:</p> <ol> <li>Metadata about the sensor/object/measurement such as Description, Unit of Measure, Status and also provides metadata used in queries such as Step logic used in interpolation.</li> <li>Events contains transactional data and is simply capturing the name of the sensor/object/measurement, the timestamp of the event, the status of the event recording and the value.</li> </ol>"},{"location":"domains/process_control/data_model/#data-model","title":"Data Model","text":"<pre><code>erDiagram\n  METADATA ||--o{ EVENTS : contains\n  METADATA {\n    string TagName PK\n    string Description\n    string UoM\n    string DataType\n    boolean Step\n    string Status\n    dict Properties \"Key Value pairs of varying metadata\"\n  }\n  EVENTS {\n    string TagName PK\n    timestamp EventTime PK\n    string Status\n    dynamic Value \"Value can be of different Data Types\"\n  }</code></pre>"},{"location":"domains/process_control/data_model/#references","title":"References","text":"Reference Description IEC 61850 Relevant description to IEC 61850 IEC CIM Relevant description to IEC CIM"},{"location":"domains/process_control/data_model/#mappings","title":"Mappings","text":""},{"location":"domains/process_control/data_model/#fledge-opc-ua-south-plugin","title":"Fledge OPC UA South Plugin","text":"<p>Fledge provides support for sending data between various data sources and data destinations. The mapping below is for the OPC UA South Plugin that can be sent to message brokers like Kafka, Azure IoT Hub etc.</p> <p>This mapping is performed by the RTDIP Fledge to PCDM Component and can be used in an RTDIP Ingestion Pipeline.</p> From Data Model From Field From Type To Data Model To Field To Type Mapping Logic Fledge OPC UA Object ID string EVENTS TagName string Fledge OPC UA EventTime string EVENTS EventTime timestamp Converted to a timestamp EVENTS Status string Can be defaulted in RTDIP Fledge to PCDM Component otherwise Null Fledge OPC UA Value string EVENTS Value dynamic Converts Value into either a float number or string based on how it is received in the message"},{"location":"domains/process_control/data_model/#opc-publisher","title":"OPC Publisher","text":"<p>OPC Publisher connects to OPC UA assets and publishes data to the Microsoft Azure Cloud's IoT Hub.</p> <p>The mapping below is performed by the RTDIP OPC Publisher to PCDM Component and can be used in an RTDIP Ingestion Pipeline.</p> From Data Model From Field From Type To Data Model To Field To Type Mapping Logic OPC Publisher DisplayName string EVENTS TagName string From Field can be specified in Component OPC Publisher SourceTimestamp string EVENTS EventTime timestamp Converted to a timestamp OPC Publisher StatusCode.Symbol string EVENTS Status string Null values can be overridden in the RTDIP OPC Publisher to PCDM Component OPC Publisher Value.Value string EVENTS Value dynamic Converts Value into either a float number or string based on how it is received in the message"},{"location":"domains/process_control/data_model/#edgex","title":"EdgeX","text":"<p>EdgeX provides support for sending data between various data sources and data destinations. </p> <p>This mapping is performed by the RTDIP EdgeX to PCDM Component and can be used in an RTDIP Ingestion Pipeline.</p> From Data Model From Field From Type To Data Model To Field To Type Mapping Logic EdgeX deviceName string EVENTS TagName string EdgeX origin string EVENTS EventTime timestamp Converted to a timestamp EVENTS Status string Can be defaulted in RTDIP EdgeX to PCDM Component otherwise Null EdgeX value string EVENTS Value dynamic Converts Value into either a float number or string based on how it is received in the message"},{"location":"domains/process_control/data_model/#ssip-pi","title":"SSIP PI","text":"<p>SSIP PI connects to Osisoft PI Historians and sends the data to the Cloud.</p> <p>The mapping below is performed by the RTDIP SSIP PI to PCDM Component and can be used in an RTDIP Ingestion Pipeline.</p> From Data Model From Field From Type To Data Model To Field To Type Mapping Logic SSIP PI TagName string EVENTS TagName string SSIP PI EventTime string EVENTS EventTime timestamp SSIP PI Status string EVENTS Status string SSIP PI Value dynamic EVENTS Value dynamic"},{"location":"domains/process_control/overview/","title":"Process Control Domain Overview","text":"<p>For process control systems, RTDIP provides the ability to consume data from these sources, transform it and store the data in an open source format to enable:</p> <ul> <li>Data Science, ML and AI applications to consume the data</li> <li>Real time data in Digital Twins</li> <li>BI and Analytics </li> <li>Reporting</li> </ul>"},{"location":"domains/process_control/overview/#process-control-systems","title":"Process Control Systems","text":"<p>Process control systems monitor, control and safeguard production operations and generate vast amounts of data. Typical industry use cases include:</p> <ul> <li>Electricity Generation, Transmission and Distribution</li> <li>Chemicals, Gas, Oil Production and Distribution</li> <li>LNG Processing and Product Refining </li> </ul> <p>Process control systems record variables such as temperature, pressure, flow etc and automatically make adjustments to maintain preset specifications in a technical process.</p> <p>This data can be made available to other systems over a number of protocols, such as OPC UA. The protocols in turn make the data available to connectors that can send the data onwards to other systems and the cloud.</p>"},{"location":"domains/process_control/overview/#architecture","title":"Architecture","text":"<pre><code>graph LR\n  A(Process Control) --&gt; B(OPC UA Server);\n  B --&gt; C(Connectors);\n  C --&gt; D(Message Broker);\n  D --&gt; E(RTDIP);\n  E --&gt; F[(Destinations)];</code></pre>"},{"location":"domains/process_control/overview/#connectors","title":"Connectors","text":"<p>A number of connectors are available from various suppliers. Some open source options include:</p> <p> </p> <p> </p>"},{"location":"domains/process_control/overview/#message-brokers","title":"Message Brokers","text":"<p>Message Brokers support publishing of data from connectors and subscribing(pub/sub) to data from consumers. Popular options used with RTDIP are:</p> <p> </p> <p> </p> <p> </p>"},{"location":"domains/process_control/overview/#real-time-data-ingestion-platform","title":"Real Time Data Ingestion Platform","text":"<p>For more information about the Real Time Data Platform and its components to connect to data sources and destinations, please refer to this link.</p>"},{"location":"domains/smart_meter/data_model/","title":"Meters Data Model","text":"<p>Base Raw To Meters Data Model: here </p> <ul> <li>ISO:<ul> <li>MISO To Meters Data Model: here</li> <li>PJM  To Meters Data Model: here</li> </ul> </li> </ul>"},{"location":"domains/smart_meter/overview/","title":"Smart Meter / Energy Domain","text":""},{"location":"domains/smart_meter/overview/#overview","title":"Overview","text":"<p>Meter data is central to accelerating the electrification and decarbonisation of the energy grid. RTDIP provides the ability to consume meter data from exemplar sources, transform it and store it in an appropriate open-source format to enable domain-specific energy services, such as:</p> <ul> <li>Energy Load Forecasting</li> <li>Energy Generation Forecasting</li> <li>Other behind-the-meter services and insights</li> </ul> <p>At a high level, the electricity system (US example) works as follows:</p> <ul> <li>Generators, of various types (coal, oil, natural gas, nuclear, wind turbines &amp; PV, etc.) produce electricity </li> <li>Utilities, distribute and transmit the electricity from the Generators through the grid to the \u00a0point of consumption i.e. buildings and homes</li> <li>Suppliers, wholesale purchase the electricity and sell it as retail contracts to Buyers</li> <li>Buyers, consume electricity, via buildings, homes, and electric vehicles, etc.</li> <li>Consultants, facilitate these transactions and/or offer data insights e.g. load forecasting to tailor purchasing, targeting reduced risk, profit, and competitive costs for Buyers</li> </ul> <p>An Independent System Operator (ISO) sometimes called the Regional Transmission Organisation (RTO) is an organisation that is in charge of the entire process. They coordinate, control, and monitor the electric grid in a specific region, typically a multi-state area.</p>"},{"location":"domains/smart_meter/overview/#meter-data-pipelines","title":"Meter Data Pipelines","text":"<p>Load forecasting is a technique used by ISO's, and energy-providing companies to predict the power/energy needed to meet the demand and supply equilibrium of the energy grid. RTDIP defines and provides example pipelines for the two primary inputs to energy services like load forecasting, namely weather and meter data.</p> <p>Specifically, with respect to meter data RTDIP defines and provides two exemplar ISO's:</p> <ul> <li>the Midcontinent Independent System Operator, MISO </li> <li>the PJM Interconnection LLC Independent System Operator, PJM</li> </ul>"},{"location":"domains/smart_meter/overview/#architecture","title":"Architecture","text":"<p>The overall ETL flow of the pipeline is outlined below:</p> <pre><code>graph LR\n  A(External Meter Source e.g. MISO, PJM) --&gt; B(RTDIP Source/Connector);\n  B --&gt; C(RTDIP Transformer);\n  C --&gt; D(RTDIP Source/Connector);\n  D --&gt; E[(RTDIP Destination)];</code></pre> <ol> <li>Source: The specific source/connector acquires data from a specific external endpoint (MISO or PJM) and persists the raw data into Deltalake </li> <li>Transformer: \u00a0An RTDIP transformer translates this raw data into a meter specific Delta schema. \u00a0</li> <li>Destination: Essentially the function of loading is abstracted from the user and is handled by Deltalake. </li> </ol> <p>Indicative schema are available here. </p>"},{"location":"domains/smart_meter/overview/#real-time-data-ingestion-platform","title":"Real Time Data Ingestion Platform","text":"<p>For more information about the Real Time Data Platform and its components to connect to data sources and destinations, please refer to this link.</p>"},{"location":"domains/weather/data_model/","title":"Weather Data Model","text":"<p>The weather data model describes available weather data schemas commonly used in weather services and available via RTDIP. </p>"},{"location":"domains/weather/data_model/#15-day-hourly-forecast","title":"15-day Hourly Forecast","text":"<p>This model returns weather forecasts starting with the current day. Forecast data is using TAF format, which is similar to METAR but contains extra forecast variables</p> Type Name Description String class Data identifier Integer clds Hourly average cloud cover expressed as a percentage. String day_ind This data field indicates whether it is daytime or nighttime based on the Local Apparent Time of the location. Integer dewpt The temperature which air must be cooled at constant pressure to reach saturation. The Dew Point is also an indirect measure of the humidity of the air. The Dew Point will never exceed the Temperature. When the Dew Point and Temperature are equal, clouds or fog will typically form. The closer the values of Temperature and Dew Point, the higher the relative humidity. String dow Day of week Float expire_time_gmt Expiration time in UNIX seconds Float fcst_valid Time forecast is valid in UNIX seconds String fcst_valid_local Time forecast is valid in local apparent time Integer feels_like Hourly feels like temperature. An apparent temperature. It represents what the air temperature \u201cfeels like\u201d on exposed human skin due to the combined effect of the wind chill or heat index. String golf_category The Golf Index Category expressed as a worded phrase the weather conditions for playing golf Integer golf_index The Golf Index expresses on a scale of 0 to 10 the weather conditions for playing golf Not applicable at night. 0-2=Very Poor, 3=Poor, 4-5=Fair, 6-7=Good, 8-9=Very Good, 10=Excellent Integer gust The maximum expected wind gust speed. Integer hi Hourly maximum heat index. An apparent temperature. It represents what the air temperature \u201cfeels like\u201d on exposed human skin due to the combined effect of warm temperatures and high humidity. When the temperature is 70\u00b0F or higher, the Feels Like value represents the computed Heat Index. For temperatures between 40\u00b0F and 70\u00b0F, the Feels Like value and Temperature are the same, regardless of wind speed and humidity, so use the Temperature value. Integer icon_code This number is the key to the weather icon lookup. The data field shows the icon number that is matched to represent the observed weather conditions. Please refer to the Forecast Icon Code, Weather Phrases and Images document. Integer icon_extd Code representing explicit full set sensible weather. Please refer to the Forecast Icon Code, Weather Phrases and Images document. Float mslp Hourly mean sea level pressure Integer num This data field is the sequential number that identifies each of the forecasted days in the API. They start on day 1, which is the forecast for the current day. Then the forecast for tomorrow uses number 2, then number 3 for the day after tomorrow, and so forth. String phrase_12char Hourly sensible weather phrase String phrase_22char Hourly sensible weather phrase String phrase_32char Hourly sensible weather phrase Integer pop Hourly maximum probability of precipitation String precip_type The short text describing the expected type accumulation associated with the Probability of Precipitation (POP) display for the hour. Float qpf The forecasted measurable precipitation (liquid or liquid equivalent) during the hour. Integer rh The relative humidity of the air, which is defined as the ratio of the amount of water vapor in the air to the amount of vapor required to bring the air to saturation at a constant temperature. Relative humidity is always expressed as a percentage. Integer severity A number denoting how impactful is the forecasted weather for this hour. Can be used to determine the graphical treatment of the weather display such as using red font on weather.com Float snow_qpf The forecasted hourly snow accumulation during the hour. String subphrase_pt1 Part 1 of 3-part hourly sensible weather phrase String subphrase_pt2 Part 2 of 3-part hourly sensible weather phrase String subphrase_pt3 Part 3 of 3-part hourly sensible weather phrase Integer temp The temperature of the air, measured by a thermometer 1.5 meters (4.5 feet) above the ground that is shaded from the other elements. You will receive this data field in Fahrenheit degrees or Celsius degrees. String uv_desc The UV Index Description which complements the UV Index value by providing an associated level of risk of skin damage due to exposure. Integer uv_index Hourly maximum UV index Decimal uv_index_raw The non-truncated UV Index which is the intensity of the solar radiation based on a number of factors. Integer uv_warning TWC-created UV warning based on UV index of 11 or greater. Decimal vis Prevailing hourly visibility Integer wc Hourly minimum wind chill. An apparent temperature. It represents what the air temperature \u201cfeels like\u201d on exposed human skin due to the combined effect of the cold temperatures and wind speed. When the temperature is 61\u00b0F or lower the Feels Like value represents the computed Wind Chill so display the Wind Chill value. For temperatures between 61\u00b0F and 75\u00b0F, the Feels Like value and Temperature are the same, regardless of wind speed and humidity, so display the Temperature value. Integer wdir Hourly average wind direction in magnetic notation. String wdir_cardinal Hourly average wind direction in cardinal notation. Integer wspd The maximum forecasted hourly wind speed. The wind is treated as a vector; hence, winds must have direction and magnitude (speed). The wind information reported in the hourly current conditions corresponds to a 10-minute average called the sustained wind speed. Sudden or brief variations in the wind speed are known as \u201cwind gusts\u201d and are reported in a separate data field. Wind directions are always expressed as \"from whence the wind blows\" meaning that a North wind blows from North to South. If you face North in a North wind the wind is at your face. Face southward and the North wind is at your back. String wxman Code combining Hourly sensible weather and temperature conditions"},{"location":"domains/weather/data_model/#references","title":"References","text":"Reference Description METAR Description to METAR TAF Description of TAF"},{"location":"domains/weather/overview/","title":"Weather Services","text":""},{"location":"domains/weather/overview/#overview","title":"Overview","text":"<p>Many organizations need weather data for day-to-day operations. RTDIP provides the ability to consume data from examplar weather sources, transform it and store the data in an appropriate open source format to enable generic functions such as:</p> <ul> <li>Data Science, ML and AI applications to consume the data</li> <li>BI and Analytics</li> <li>Reporting</li> </ul> <p>A primary aim for RTDIP in 2023 is to demonstrate how the platform can be utilised for domain specific services such as:</p> <ul> <li>Consumption Load Forecasting</li> <li>Energy Generation Forecasting</li> <li>Other behind the meter services and insights</li> </ul> <p>Weather data is a primary driver,  together with meter data, of variance in load &amp; generation forecasting in the energy domain. </p>"},{"location":"domains/weather/overview/#weather-data-in-the-energy-domain","title":"Weather Data in the Energy Domain","text":"<p>One of the most widely used weather data standards is the combined METAR (Meteorological Aerodrome Report) and ICAO (International Civil Aviation Organization) standard. This standard is used by meteorological agencies and aviation organizations around the world to report weather conditions at airports and other aviation facilities. This standard is broadly utilised beyond the aviation industry including the energy domain. </p> <p>The METAR ICAO standard includes a set of codes and abbreviations that describe weather conditions in a standardized format. These codes include information such as temperature, wind speed and direction, visibility, cloud cover, and precipitation. The standard also includes codes for reporting special weather phenomena, such as thunderstorms or volcanic ash.</p> <p>Many actors in the energy domain utilise Historical, Forecast and near real-time METAR data as part of their services. Such data can be used to calculate average weather data by date and interval spanning multiple years, eg Historical Weather Data is often used to calculate an average or typical value for each weather variable eg. temperature, humidity over a given timeframe, which can be used for long range forecasting etc. </p>"},{"location":"domains/weather/overview/#architecture","title":"Architecture","text":"<p>An exemplar pipeline is defined and provided within RTDIP. The overall approach and weather data in general is agnostic but the exemplar utilises a specific external source. The overall ETL flow of the pipeline is outlined below:</p> <pre><code>graph LR\n  A(External Weather Source) --&gt; B(RTDIP Source/Connector);\n  B --&gt; C(RTDIP Transformer);\n  C --&gt; D(RTDIP Source/Connector);\n  D --&gt; E[(RTDIP Destination)];</code></pre> <ol> <li>Source: The specific source/connector aquires data from a specific external endpoint and persists the raw data into Deltalake </li> <li>Tranformer:  An RTDIP transformer translates this raw data into a weather specific Delta schema.  </li> <li>Destination: Essentially the function of loading is abstracted from the user and is handled by Deltalake. </li> </ol> <p>An indicative schema is available here. </p>"},{"location":"domains/weather/overview/#real-time-data-ingestion-platform","title":"Real Time Data Ingestion Platform","text":"<p>For more information about the Real Time Data Platform and its components to connect to data sources and destinations, please refer to this link.</p>"},{"location":"getting-started/about-rtdip/","title":"About RTDIP","text":""},{"location":"getting-started/about-rtdip/#rtdip-and-lf-energy","title":"RTDIP and LF Energy","text":"<p>By providing frameworks and reference implementations, LF Energy minimizes pain points such as cybersecurity, interoperability, control, automation, virtualization, and the orchestration of supply and demand.</p> <p>RTDIP is an LF Energy project and forms part of an overall open source energy ecosystem. To find out more about projects in LF Energy, please click here.</p>"},{"location":"getting-started/about-rtdip/#what-is-real-time-data-ingestion-platform","title":"What is Real Time Data Ingestion Platform","text":"<p>Organizations need data for day-to-day operations and to support advanced Data Science, Statistical and Machine Learning capabilities such as Optimization, Surveillance, Forecasting, and Predictive Analytics. Real Time Data forms a major part of the total data utilized in these activities.</p> <p>Real time data enables organizations to detect and respond to changes in their systems thus improving the efficiency of their operations. This data needs to be available in scalable and secure data platforms. </p> <p>Real Time Data Ingestion Platform (RTDIP) is the solution of choice leveraging PaaS (Platform as a Service) services along with some custom components to provide Data Ingestion, Data Transformation, and Data Sharing as a platform. RTDIP can interface with several data sources to ingest many different data types which include time series, alarms, video, photos and audio being provided from sources such as Historians, OPC Servers and Rest APIs, as well as data being sent from hardware such as IoT Sensors, Robots and Drones.</p>"},{"location":"getting-started/about-rtdip/#why-real-time-data-ingestion-platform","title":"\u200bWhy Real Time Data Ingestion Platform?","text":"<p>Real Time Data Ingestion Platform (RTDIP) enables users to consume Real Time Data at scale, including historical and real time streaming data. RTDIP has proven to be capable of ingesting over 3 million sensors in a production environment across every geographical location in the world.</p> <p>The Real Time Data Ingestion Platform can be run in a customers own environment, allowing them to accelerate their cloud deployments while leveraging a proven design to scale their time series data needs. </p> <p>RTDIP also provides a number popular integration options, including:</p> <ol> <li>ODBC</li> <li>JDBC</li> <li>Rest API</li> <li>Python SDK</li> </ol> <p>These options allow users to integrate with a wide variety of applications and tools, including:</p> <ol> <li>Data Visualization Tools such as Power BI, Seeq, Tableau, and Grafana</li> <li>Data Science Tools such as Jupyter Notebooks, R Studio, and Python</li> <li>Data Engineering Tools such as Apache Spark, Apache Kafka, and Apache Airflow</li> </ol> <p>RTDIP is architected to leverage Open Source technologies Apache Spark and Delta. This allows users to leverage the power of Open Source technologies to build their own custom applications and tools in whichever environment they prefer, whether that is in the cloud or on-premise on their own managed Spark Clusters.</p>"},{"location":"getting-started/installation/","title":"Getting started with RTDIP","text":"<p>RTDIP provides functionality to process and query real time data. The RTDIP SDK is central to building pipelines and querying data, so getting started with it is key to unlocking the capability of RTDIP.</p> <p>This article provides a guide on how to install the RTDIP SDK. Get started by ensuring you have all the prerequisites before following the simple installation steps.</p> <ul> <li> <p>Prerequisites</p> </li> <li> <p>Installation</p> </li> </ul>"},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":""},{"location":"getting-started/installation/#python","title":"Python","text":"<p>There are a few things to note before using the RTDIP SDK. The following prerequisites will need to be installed on your local machine.</p> <p>Python version 3.9 &gt;= and &lt; 3.12 should be installed. Check which python version you have with the following command:</p> <pre><code>python --version\n</code></pre> <p>Find the latest python version here and ensure your python path is set up correctly on your machine.</p>"},{"location":"getting-started/installation/#python-package-installers","title":"Python Package Installers","text":"<p>Installing the RTDIP can be done using a package installer, such as Pip, Conda or Micromamba.</p> PipCondaMicromamba <p>Ensure your pip python version matches the python version on your machine. Check which version of pip you have installed with the following command:</p> <pre><code>pip --version\n</code></pre> <p>There are two ways to ensure you have the correct versions installed. Either upgrade your Python and pip install or create an environment.</p> <pre><code>python -m pip install --upgrade pip\n</code></pre> <p>Check which version of Conda is installed with the following command:</p> <pre><code>conda --version\n</code></pre> <p>If necessary, upgrade Conda as follows:</p> <pre><code>conda update conda\n</code></pre> <p>Check which version of Micromamba is installed with the following command:</p> <pre><code>micromamba --version\n</code></pre> <p>If necessary, upgrade Micromamba as follows:</p> <pre><code>micromamba self-update\n</code></pre>"},{"location":"getting-started/installation/#odbc","title":"ODBC","text":"<p>To use pyodbc or turbodbc python libraries, ensure it is installed as per the below and the ODBC driver is installed as per these instructions.</p> PyodbcTurbodbc <ol> <li>If you plan to use pyodbc, Microsoft Visual C++ 14.0 or greater is required. Get it from Microsoft C++ Build Tools.</li> <li>If you are using linux, install unixodbc as per these instructions.</li> <li>Install the <code>pyodbc</code> python package into your python environment.</li> </ol> <ol> <li>To use turbodbc python library, ensure to follow the Turbodbc Getting Started section and ensure that Boost is installed correctly. </li> <li>Install the <code>turbodbc</code> python package into your python environment.</li> </ol>"},{"location":"getting-started/installation/#spark-connect","title":"Spark Connect","text":"<p>Spark Connect was released in Apache Spark 3.4.0 and enables a decoupled client-server architecture that allows remote connectivity to Spark Clusters. RTDIP SDK supports Spark Connect and can be configured using the Spark Connector and providing the Spark Connect connection string required to connect to your Spark Cluster.</p> <p>Please ensure that you have followed the instructions to enable Spark Connect on your Spark Cluster and that you are using a <code>pyspark&gt;=3.4.0</code>. If you are connecting to a Databricks Cluster, then you may prefer to install python package <code>databricks-connect&gt;=13.0.1</code> instead of <code>pyspark</code>.</p>"},{"location":"getting-started/installation/#java","title":"Java","text":"<p>To use RTDIP Pipelines components in your own environment that leverages pyspark and you do not want to leverage Spark Connect, Java 8 or later is a prerequisite. See below for suggestions to install Java in your development environment.</p> CondaJava <p>A fairly simple option is to use the conda openjdk package to install Java into your python virtual environment. An example of a conda environment.yml file to achieve this is below.</p> <pre><code>name: rtdip-sdk\nchannels:\n- conda-forge\n- defaults\ndependencies:\n- python==3.11\n- pip==23.0.1\n- openjdk==11.0.15\n- pip:\n- rtdip-sdk\n</code></pre> <p>Pypi</p> <p>This package is not available from Pypi.</p> <p>Follow the official Java JDK installation documentation here.</p> <ul> <li>Windows</li> <li>Mac OS</li> <li>Linux</li> </ul> <p>Windows</p> <p>Windows requires an additional installation of a file called winutils.exe. Please see this repo for more information.</p>"},{"location":"getting-started/installation/#installing-the-rtdip-sdk","title":"Installing the RTDIP SDK","text":"<p>RTDIP SDK is a PyPi package that can be found here. On this page you can find the project description,  release history, statistics, project links and maintainers.</p> <p>Features of the SDK can be installed using different extras statements when installing the rtdip-sdk package:</p> QueriesPipelinesPipelines + Pyspark <p>When installing the package for only quering data, simply specify  in your preferred python package installer:</p> <pre><code>pip install rtdip-sdk\n</code></pre> <p>RTDIP SDK can be installed to include the packages required to build, execute and deploy pipelines. Specify the following extra [pipelines] when installing RTDIP SDK so that the required python packages are included during installation.</p> <pre><code>pip install \"rtdip-sdk[pipelines]\"\n</code></pre> <p>RTDIP SDK can also execute pyspark functions as a part of the pipelines functionality. Specify the following extra [pipelines,pyspark] when installing RTDIP SDK so that the required pyspark python packages are included during installation.</p> <pre><code>pip install \"rtdip-sdk[pipelines,pyspark]\"\n</code></pre> <p>Java</p> <p>Ensure that Java is installed prior to installing the rtdip-sdk with the [pipelines,pyspark]. See here for more information.</p> <p>The following provides examples of how to install the RTDIP SDK package with Pip, Conda or Micromamba. Please note the section above to update any extra packages to be installed as part of the RTDIP SDK.</p> PipCondaMicromamba <p>To install the latest released version of RTDIP SDK from PyPi use the following command:</p> <pre><code>pip install rtdip-sdk\n</code></pre> <p>If you have previously installed the RTDIP SDK and would like the latest version, see below:</p> <pre><code>pip install rtdip-sdk --upgrade\n</code></pre> <p>To create an environment, you will need to create a environment.yml file with the following:</p> <pre><code>name: rtdip-sdk\nchannels:\n- conda-forge\n- defaults\ndependencies:\n- python==3.11\n- pip==23.0.1\n- pip:\n- rtdip-sdk\n</code></pre> <p>Run the following command:</p> <pre><code>conda env create -f environment.yml\n</code></pre> <p>To update an environment previously created:</p> <pre><code>conda env update -f environment.yml\n</code></pre> <p>To create an environment, you will need to create a environment.yml file with the following:</p> <pre><code>name: rtdip-sdk\nchannels:\n- conda-forge\n- defaults\ndependencies:\n- python==3.11\n- pip==23.0.1\n- pip:\n- rtdip-sdk\n</code></pre> <p>Run the following command:</p> <pre><code>micromamba create -f environment.yml\n</code></pre> <p>To update an environment previously created:</p> <pre><code>micromamba update -f environment.yml\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next steps","text":"<p>Once the installation is complete you can learn how to use the SDK here.</p> <p>Note</p> <p>If you are having trouble installing the SDK, ensure you have installed all of the prerequisites. If problems persist please see Troubleshooting for more information. Please also reach out to the RTDIP team via Issues, we are always looking to improve the SDK and value your input.</p>"},{"location":"integration/power-bi/","title":"Integration of Power BI with RTDIP","text":""},{"location":"integration/power-bi/#integration-with-power-bi","title":"Integration with Power BI","text":"<p>Microsoft Power BI is a business analytics service that provides interactive visualizations with self-service business intelligence capabilities that enable end users to create reports and dashboards by themselves without having to depend on information technology staff or database administrators.</p> <p></p> <p>When you use Azure Databricks as a data source with Power BI, you can bring the advantages of Azure Databricks performance and technology beyond data scientists and data engineers to all business users.</p> <p>You can connect Power BI Desktop to your Azure Databricks clusters and Databricks SQL warehouses by using the built-in Azure Databricks connector. You can also publish Power BI reports to the Power BI service and enable users to access the underlying Azure Databricks data using single sign-on (SSO), passing along the same Azure Active Directory credentials they use to access the report.</p> <p>For more information on how to connect Power BI with databricks, see here.</p>"},{"location":"integration/power-bi/#power-bi-installation-instructions","title":"Power BI Installation Instructions","text":"<ol> <li> <p>Install Power BI Desktop application from Microsoft Store using your Microsoft Account to sign in. </p> </li> <li> <p>Open Power BI desktop.</p> </li> <li> <p>Click on Home, Get data and More... </p> </li> <li> <p>Search for Azure Databricks and click Connect.  </p> </li> <li> <p>Fill in the details and click OK.</p> </li> <li> <p>Connect to the RTDIP data using your Databricks SQL Warehouse connection details including Hostname and HTTP Path. For Data Connectivity mode, select DirectQuery.</p> </li> <li> <p>Click Azure Active Directory, Sign In and select Connect. In Power Query Editor, there are different tables for different data types. </p> </li> <li> <p>Once connected to the Databricks SQL Warehouse, navigate to the Business Unit in the navigator bar on the left and select the asset tables for the data you wish to use in your report. There is functionality to select multiple tables if required. Click Load to get the queried data.</p> </li> </ol>"},{"location":"releases/core/","title":"Releases","text":""},{"location":"releases/core/#v095","title":"v0.9.5","text":"<p>v0.9.5  Published 2023-12-12 15:44:23+00:00 </p>"},{"location":"releases/core/#whats-changed","title":"What's Changed","text":""},{"location":"releases/core/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Add Mirico Transformer to init by @cching95 in https://github.com/rtdip/core/pull/609</li> </ul>"},{"location":"releases/core/#pipelines","title":"Pipelines","text":"<ul> <li>Mirico Transformer, Documentation and Unit Tests by @cching95 in https://github.com/rtdip/core/pull/608</li> </ul>"},{"location":"releases/core/#full-changelog-httpsgithubcomrtdipcorecomparev094v095","title":"Full Changelog: https://github.com/rtdip/core/compare/v0.9.4...v0.9.5","text":""},{"location":"releases/core/#v094","title":"v0.9.4","text":"<p>v0.9.4  Published 2023-12-07 12:01:22+00:00 </p>"},{"location":"releases/core/#whats-changed_1","title":"What's Changed","text":""},{"location":"releases/core/#documentation","title":"Documentation","text":"<ul> <li>Blog OPC UA to Delta Lake by @srjhunjhunwalacorp in https://github.com/rtdip/core/pull/604</li> </ul>"},{"location":"releases/core/#pipelines_1","title":"Pipelines","text":"<ul> <li>Source connector for ERCOT System Level Data  by @IW-SS in https://github.com/rtdip/core/pull/583</li> </ul>"},{"location":"releases/core/#new-contributors","title":"New Contributors","text":"<ul> <li>@srjhunjhunwalacorp made their first contribution in https://github.com/rtdip/core/pull/604</li> </ul>"},{"location":"releases/core/#full-changelog-httpsgithubcomrtdipcorecomparev093v094","title":"Full Changelog: https://github.com/rtdip/core/compare/v0.9.3...v0.9.4","text":""},{"location":"releases/core/#v093","title":"v0.9.3","text":"<p>v0.9.3  Published 2023-12-01 11:45:19+00:00 </p>"},{"location":"releases/core/#whats-changed_2","title":"What's Changed","text":""},{"location":"releases/core/#bug-fixes_1","title":"Bug Fixes","text":"<ul> <li>Fix Documentation Links in Swagger by @cching95 in https://github.com/rtdip/core/pull/602</li> </ul>"},{"location":"releases/core/#full-changelog-httpsgithubcomrtdipcorecomparev092v093","title":"Full Changelog: https://github.com/rtdip/core/compare/v0.9.2...v0.9.3","text":""},{"location":"releases/core/#v092","title":"v0.9.2","text":"<p>v0.9.2  Published 2023-12-01 10:05:54+00:00 </p>"},{"location":"releases/core/#whats-changed_3","title":"What's Changed","text":""},{"location":"releases/core/#new-features","title":"New Features","text":"<ul> <li>Weather Query for Raw and Latest Table  by @Amber-Rigg in https://github.com/rtdip/core/pull/562</li> </ul>"},{"location":"releases/core/#bug-fixes_2","title":"Bug Fixes","text":"<ul> <li>Summary API Standard Deviation Bug by @cching95 in https://github.com/rtdip/core/pull/591</li> </ul>"},{"location":"releases/core/#documentation_1","title":"Documentation","text":"<ul> <li>Summary of Statistics Documentation by @cching95 in https://github.com/rtdip/core/pull/582</li> <li>Summary Statistics API Documentation by @cching95 in https://github.com/rtdip/core/pull/588</li> </ul>"},{"location":"releases/core/#pipelines_2","title":"Pipelines","text":"<ul> <li>Source connector for CASIO System Level Data by @IW-SS in https://github.com/rtdip/core/pull/579</li> </ul>"},{"location":"releases/core/#queries","title":"Queries","text":"<ul> <li>Summary of Statistics Query and Unit Tests by @cching95 in https://github.com/rtdip/core/pull/582</li> <li>Summary Statistics API by @cching95 in https://github.com/rtdip/core/pull/588</li> <li>Refactored Time series Query Unit Tests by @cching95 in https://github.com/rtdip/core/pull/598</li> </ul>"},{"location":"releases/core/#other-changes","title":"Other Changes","text":"<ul> <li>Update documentation links on APIs by @cching95 in https://github.com/rtdip/core/pull/592</li> </ul>"},{"location":"releases/core/#full-changelog-httpsgithubcomrtdipcorecomparev091v092","title":"Full Changelog: https://github.com/rtdip/core/compare/v0.9.1...v0.9.2","text":""},{"location":"releases/core/#v091","title":"v0.9.1","text":"<p>v0.9.1  Published 2023-11-08 17:03:49+00:00 </p>"},{"location":"releases/core/#whats-changed_4","title":"What's Changed","text":""},{"location":"releases/core/#bug-fixes_3","title":"Bug Fixes","text":"<ul> <li>Fix button links and landing page sizing by @cching95 in https://github.com/rtdip/core/pull/566</li> <li>Fix Font family bug and code smells by @cching95 in https://github.com/rtdip/core/pull/567</li> <li>Fix SonarCloud Font bug by @cching95 in https://github.com/rtdip/core/pull/568</li> </ul>"},{"location":"releases/core/#full-changelog-httpsgithubcomrtdipcorecomparev090v091","title":"Full Changelog: https://github.com/rtdip/core/compare/v0.9.0...v0.9.1","text":""},{"location":"releases/core/#v090","title":"v0.9.0","text":"<p>v0.9.0  Published 2023-11-08 14:43:28+00:00 </p>"},{"location":"releases/core/#whats-changed_5","title":"What's Changed","text":""},{"location":"releases/core/#new-features_1","title":"New Features","text":"<ul> <li>RTDIP New Landing Page Design and Text by @cching95 in https://github.com/rtdip/core/pull/564</li> </ul>"},{"location":"releases/core/#other-changes_1","title":"Other Changes","text":"<ul> <li>Add Examples to Each Component in Sources, Destinations, Transformers, Utilities and Secrets by @cching95 in https://github.com/rtdip/core/pull/556</li> </ul>"},{"location":"releases/core/#full-changelog-httpsgithubcomrtdipcorecomparev087v090","title":"Full Changelog: https://github.com/rtdip/core/compare/v0.8.7...v0.9.0","text":""},{"location":"releases/core/#v087","title":"v0.8.7","text":"<p>v0.8.7  Published 2023-10-24 16:06:27+00:00 </p>"},{"location":"releases/core/#whats-changed_6","title":"What's Changed","text":""},{"location":"releases/core/#other-changes_2","title":"Other Changes","text":"<ul> <li>Updates for Azure Autoloader Resource Creation by @GBBBAS in https://github.com/rtdip/core/pull/553</li> </ul>"},{"location":"releases/core/#full-changelog-httpsgithubcomrtdipcorecomparev086v087","title":"Full Changelog: https://github.com/rtdip/core/compare/v0.8.6...v0.8.7","text":""},{"location":"releases/core/#v086","title":"v0.8.6","text":"<p>v0.8.6  Published 2023-10-19 08:40:07+00:00 </p>"},{"location":"releases/core/#whats-changed_7","title":"What's Changed","text":""},{"location":"releases/core/#pipelines_3","title":"Pipelines","text":"<ul> <li>Support for Spark 3.5.0 and Delta 3.0.0 by @GBBBAS in https://github.com/rtdip/core/pull/545</li> <li>Update Delta Tests for Spark 3.5.0 by @GBBBAS in https://github.com/rtdip/core/pull/549</li> </ul>"},{"location":"releases/core/#full-changelog-httpsgithubcomrtdipcorecomparev085v086","title":"Full Changelog: https://github.com/rtdip/core/compare/v0.8.5...v0.8.6","text":""},{"location":"releases/core/#v085","title":"v0.8.5","text":"<p>v0.8.5  Published 2023-10-13 08:31:35+00:00 </p>"},{"location":"releases/core/#whats-changed_8","title":"What's Changed","text":""},{"location":"releases/core/#bug-fixes_4","title":"Bug Fixes","text":"<ul> <li>Fix for setup.py version  by @GBBBAS in https://github.com/rtdip/core/pull/540</li> </ul>"},{"location":"releases/core/#pipelines_4","title":"Pipelines","text":"<ul> <li>Azure Autoloader Resources Utility Component by @GBBBAS in https://github.com/rtdip/core/pull/539</li> <li>Updates for Autoloader Resources Utility Component by @GBBBAS in https://github.com/rtdip/core/pull/542</li> </ul>"},{"location":"releases/core/#other-changes_3","title":"Other Changes","text":"<ul> <li>Add VS Code sonarlint settings to project by @GBBBAS in https://github.com/rtdip/core/pull/536</li> <li>Git prune on fetch in VS Code  by @GBBBAS in https://github.com/rtdip/core/pull/537</li> </ul>"},{"location":"releases/core/#full-changelog-httpsgithubcomrtdipcorecomparev084v085","title":"Full Changelog: https://github.com/rtdip/core/compare/v0.8.4...v0.8.5","text":""},{"location":"releases/core/#v084","title":"v0.8.4","text":"<p>v0.8.4  Published 2023-10-10 08:49:17+00:00 </p>"},{"location":"releases/core/#whats-changed_9","title":"What's Changed","text":""},{"location":"releases/core/#bug-fixes_5","title":"Bug Fixes","text":"<ul> <li>Update pydantic version by @GBBBAS in https://github.com/rtdip/core/pull/534</li> </ul>"},{"location":"releases/core/#full-changelog-httpsgithubcomrtdipcorecomparev083v084","title":"Full Changelog: https://github.com/rtdip/core/compare/v0.8.3...v0.8.4","text":""},{"location":"releases/core/#v083","title":"v0.8.3","text":"<p>v0.8.3  Published 2023-10-09 14:40:39+00:00 </p>"},{"location":"releases/core/#whats-changed_10","title":"What's Changed","text":""},{"location":"releases/core/#new-features_2","title":"New Features","text":"<ul> <li>Circular avg &amp; circular std deviation APIs by @JamesKnBr in https://github.com/rtdip/core/pull/530</li> </ul>"},{"location":"releases/core/#bug-fixes_6","title":"Bug Fixes","text":"<ul> <li>Allow null values in latest API by @GBBBAS in https://github.com/rtdip/core/pull/532</li> </ul>"},{"location":"releases/core/#other-changes_4","title":"Other Changes","text":"<ul> <li>PCDM-APM transformer Gzip compression by @JamesKnBr in https://github.com/rtdip/core/pull/531</li> </ul>"},{"location":"releases/core/#full-changelog-httpsgithubcomrtdipcorecomparev082v083","title":"Full Changelog: https://github.com/rtdip/core/compare/v0.8.2...v0.8.3","text":""},{"location":"releases/core/#v082","title":"v0.8.2","text":"<p>v0.8.2  Published 2023-10-05 08:17:19+00:00 </p>"},{"location":"releases/core/#whats-changed_11","title":"What's Changed","text":""},{"location":"releases/core/#pipelines_5","title":"Pipelines","text":"<ul> <li>Change Databricks Auth Type from token to pat by @GBBBAS in https://github.com/rtdip/core/pull/523</li> </ul>"},{"location":"releases/core/#other-changes_5","title":"Other Changes","text":"<ul> <li>Update to release note titles by @GBBBAS in https://github.com/rtdip/core/pull/527</li> </ul>"},{"location":"releases/core/#full-changelog-httpsgithubcomrtdipcorecomparev081v082","title":"Full Changelog: https://github.com/rtdip/core/compare/v0.8.1...v0.8.2","text":""},{"location":"releases/core/#v081","title":"v0.8.1","text":"<p>v0.8.1  Published 2023-10-04 10:23:32+00:00 </p>"},{"location":"releases/core/#whats-changed_12","title":"What's Changed","text":""},{"location":"releases/core/#pipelines_6","title":"Pipelines","text":"<ul> <li>Add token to Databricks Workspace Authentication by @GBBBAS in https://github.com/rtdip/core/pull/521</li> </ul>"},{"location":"releases/core/#other-changes_6","title":"Other Changes","text":"<ul> <li>Metadata and Latest Queries - Allow optional tagname_filter by @GBBBAS in https://github.com/rtdip/core/pull/520</li> </ul>"},{"location":"releases/core/#full-changelog-httpsgithubcomrtdipcorecomparev080v081","title":"Full Changelog: https://github.com/rtdip/core/compare/v0.8.0...v0.8.1","text":""},{"location":"releases/core/#v080","title":"v0.8.0","text":"<p>v0.8.0  Published 2023-10-02 13:18:50+00:00 </p>"},{"location":"releases/core/#whats-changed_13","title":"What's Changed","text":""},{"location":"releases/core/#other-changes_7","title":"Other Changes","text":"<ul> <li>Add Latest Events Queries to SDK and APIs by @GBBBAS in https://github.com/rtdip/core/pull/516</li> </ul>"},{"location":"releases/core/#full-changelog-httpsgithubcomrtdipcorecomparev0711v080","title":"Full Changelog: https://github.com/rtdip/core/compare/v0.7.11...v0.8.0","text":""},{"location":"releases/core/#v0711","title":"v0.7.11","text":"<p>v0.7.11  Published 2023-09-29 16:46:08+00:00 </p>"},{"location":"releases/core/#whats-changed_14","title":"What's Changed","text":""},{"location":"releases/core/#bug-fixes_7","title":"Bug Fixes","text":"<ul> <li>Fix bug on MQTT Tranformer and rename to SEM by @cching95 in https://github.com/rtdip/core/pull/499</li> <li>Update SEM Transformer Column and Datatype Mappings by @cching95 in https://github.com/rtdip/core/pull/503</li> <li>Honeywell APM schema change by @JamesKnBr in https://github.com/rtdip/core/pull/505</li> <li>Update SEM Mapping DataTypes by @cching95 in https://github.com/rtdip/core/pull/507</li> </ul>"},{"location":"releases/core/#pipelines_7","title":"Pipelines","text":"<ul> <li>Updates for MQTT Transformer by @cching95 in https://github.com/rtdip/core/pull/497</li> <li>Additional parameters for delta destination by @GBBBAS in https://github.com/rtdip/core/pull/500</li> <li>Additional wait logic for streaming destinations by @GBBBAS in https://github.com/rtdip/core/pull/501</li> <li>Update SDK packages by @GBBBAS in https://github.com/rtdip/core/pull/508</li> <li>PCDM Delta Append update for Integer Type by @GBBBAS in https://github.com/rtdip/core/pull/510</li> <li>Add checkpoint logic to PCDM Delta stream writes by @GBBBAS in https://github.com/rtdip/core/pull/511</li> <li>PCDM Delta Latest Values to Delta by @GBBBAS in https://github.com/rtdip/core/pull/512</li> <li>PCDM Latest include additional Update logic by @GBBBAS in https://github.com/rtdip/core/pull/513</li> <li>Add support for Spark 3.5.0 by @GBBBAS in https://github.com/rtdip/core/pull/514</li> </ul>"},{"location":"releases/core/#full-changelog-httpsgithubcomrtdipcorecomparev0710v0711","title":"Full Changelog: https://github.com/rtdip/core/compare/v0.7.10...v0.7.11","text":""},{"location":"releases/core/#v0710","title":"v0.7.10","text":"<p>v0.7.10  Published 2023-09-19 08:50:39+00:00 </p>"},{"location":"releases/core/#whats-changed_15","title":"What's Changed","text":""},{"location":"releases/core/#bug-fixes_8","title":"Bug Fixes","text":"<ul> <li>Fix for Linear Interpolation Query by @GBBBAS in https://github.com/rtdip/core/pull/494</li> </ul>"},{"location":"releases/core/#other-changes_8","title":"Other Changes","text":"<ul> <li>Data Model Review for Weather Formats Long/Wide by @vbayon in https://github.com/rtdip/core/pull/462</li> </ul>"},{"location":"releases/core/#full-changelog-httpsgithubcomrtdipcorecomparev079v0710","title":"Full Changelog: https://github.com/rtdip/core/compare/v0.7.9...v0.7.10","text":""},{"location":"releases/core/#v079","title":"v0.7.9","text":"<p>v0.7.9  Published 2023-09-18 09:07:49+00:00 </p>"},{"location":"releases/core/#whats-changed_16","title":"What's Changed","text":""},{"location":"releases/core/#other-changes_9","title":"Other Changes","text":"<ul> <li>Add offset and limits to Query Builder by @GBBBAS in https://github.com/rtdip/core/pull/491</li> <li>Add Offsets, Limits and Databricks SQL Headers to APIs by @GBBBAS in https://github.com/rtdip/core/pull/492</li> </ul>"},{"location":"releases/core/#full-changelog-httpsgithubcomrtdipcorecomparev078v079","title":"Full Changelog: https://github.com/rtdip/core/compare/v0.7.8...v0.7.9","text":""},{"location":"releases/core/#v078","title":"v0.7.8","text":"<p>v0.7.8  Published 2023-09-14 12:53:43+00:00 </p>"},{"location":"releases/core/#whats-changed_17","title":"What's Changed","text":""},{"location":"releases/core/#documentation_2","title":"Documentation","text":"<ul> <li>Kafka Eventhub Destination fixes by @JamesKnBr in https://github.com/rtdip/core/pull/486</li> </ul>"},{"location":"releases/core/#other-changes_10","title":"Other Changes","text":"<ul> <li>Add Pivot, Offset and Limit to Queries by @GBBBAS in https://github.com/rtdip/core/pull/488</li> <li>TWA Updates for Interpolation on Step False by @GBBBAS in https://github.com/rtdip/core/pull/489</li> </ul>"},{"location":"releases/core/#full-changelog-httpsgithubcomrtdipcorecomparev077v078","title":"Full Changelog: https://github.com/rtdip/core/compare/v0.7.7...V0.7.8","text":""},{"location":"releases/core/#v077","title":"v0.7.7","text":"<p>v0.7.7  Published 2023-09-11 16:12:51+00:00 </p>"},{"location":"releases/core/#whats-changed_18","title":"What's Changed","text":""},{"location":"releases/core/#new-features_3","title":"New Features","text":"<ul> <li>Kafka Eventhub Destination by @JamesKnBr in https://github.com/rtdip/core/pull/477</li> </ul>"},{"location":"releases/core/#bug-fixes_9","title":"Bug Fixes","text":"<ul> <li>Changes to Honeywell pipeline, python delta and eventhub destination by @JamesKnBr in https://github.com/rtdip/core/pull/469</li> </ul>"},{"location":"releases/core/#documentation_3","title":"Documentation","text":"<ul> <li>Added ISO pipeline samples to docs by @rodalynbarce in https://github.com/rtdip/core/pull/475</li> <li>Added EdgeX Eventhub to delta sample to docs by @JamesKnBr in https://github.com/rtdip/core/pull/476</li> </ul>"},{"location":"releases/core/#pipelines_8","title":"Pipelines","text":"<ul> <li>MQTT Json to PCDM Transformer by @cching95 in https://github.com/rtdip/core/pull/481</li> </ul>"},{"location":"releases/core/#other-changes_11","title":"Other Changes","text":"<ul> <li>Remove pyodbc from package and update docs by @GBBBAS in https://github.com/rtdip/core/pull/471</li> <li>Add python delta sample to docs by @JamesKnBr in https://github.com/rtdip/core/pull/472</li> <li>Add unit tests to kafka eventhub source by @JamesKnBr in https://github.com/rtdip/core/pull/479</li> <li>Honeywell transformer schema fix by @JamesKnBr in https://github.com/rtdip/core/pull/484</li> </ul>"},{"location":"releases/core/#full-changelog-httpsgithubcomrtdipcorecomparev076v077","title":"Full Changelog: https://github.com/rtdip/core/compare/v0.7.6...v0.7.7","text":""},{"location":"releases/core/#v076","title":"v0.7.6","text":"<p>v0.7.6  Published 2023-08-29 16:18:21+00:00 </p>"},{"location":"releases/core/#whats-changed_19","title":"What's Changed","text":""},{"location":"releases/core/#other-changes_12","title":"Other Changes","text":"<ul> <li>Query Optimisation for Resample and Interpolate Functions by @cching95 in https://github.com/rtdip/core/pull/465</li> </ul>"},{"location":"releases/core/#full-changelog-httpsgithubcomrtdipcorecomparev075v076","title":"Full Changelog: https://github.com/rtdip/core/compare/V0.7.5...v0.7.6","text":""},{"location":"releases/core/#v075","title":"v0.7.5","text":"<p>v0.7.5  Published 2023-08-25 15:58:29+00:00 </p>"},{"location":"releases/core/#whats-changed_20","title":"What's Changed","text":""},{"location":"releases/core/#other-changes_13","title":"Other Changes","text":"<ul> <li>Eventhub Destination &amp; PCDM-Honeywell Transformer Updates  by @JamesKnBr in https://github.com/rtdip/core/pull/458</li> <li>Update for Query Builder by @GBBBAS in https://github.com/rtdip/core/pull/460</li> <li>Add Select logic to Delta Append in PCDM Destination by @GBBBAS in https://github.com/rtdip/core/pull/461</li> </ul>"},{"location":"releases/core/#full-changelog-httpsgithubcomrtdipcorecomparev073v075","title":"Full Changelog: https://github.com/rtdip/core/compare/v0.7.3...v0.7.5","text":""},{"location":"releases/core/#v074","title":"v0.7.4","text":"<p>v0.7.4  Published 2023-08-25 12:56:41+00:00 </p>"},{"location":"releases/core/#whats-changed_21","title":"What's Changed","text":""},{"location":"releases/core/#documentation_4","title":"Documentation","text":"<ul> <li>Update Mkdocs Packages by @GBBBAS in https://github.com/rtdip/core/pull/449</li> <li>Documentation Structure for Weather Data Sources by @Amber-Rigg in https://github.com/rtdip/core/pull/454</li> </ul>"},{"location":"releases/core/#pipelines_9","title":"Pipelines","text":"<ul> <li>ECMWF Source and Transformer Components by @Amber-Rigg in https://github.com/rtdip/core/pull/447</li> <li>Pandas version updates by @GBBBAS in https://github.com/rtdip/core/pull/451</li> </ul>"},{"location":"releases/core/#other-changes_14","title":"Other Changes","text":"<ul> <li>Fix for Mkdocs Build by @GBBBAS in https://github.com/rtdip/core/pull/448</li> <li>Honeywell APM Transformers by @JamesKnBr in https://github.com/rtdip/core/pull/446</li> <li>Black Code Formatter by @GBBBAS in https://github.com/rtdip/core/pull/450</li> </ul>"},{"location":"releases/core/#full-changelog-httpsgithubcomrtdipcorecomparev072v074","title":"Full Changelog: https://github.com/rtdip/core/compare/v0.7.2...v0.7.4","text":""},{"location":"releases/core/#v073","title":"v0.7.3","text":"<p>v0.7.3  Published 2023-08-22 14:23:15+00:00 </p>"},{"location":"releases/core/#whats-changed_22","title":"What's Changed","text":""},{"location":"releases/core/#documentation_5","title":"Documentation","text":"<ul> <li>Documentation Structure for Weather Data Sources by @Amber-Rigg in https://github.com/rtdip/core/pull/454</li> </ul>"},{"location":"releases/core/#full-changelog-httpsgithubcomrtdipcorecomparev072v073","title":"Full Changelog: https://github.com/rtdip/core/compare/v0.7.2...v0.7.3","text":""},{"location":"releases/core/#v072","title":"v0.7.2","text":"<p>v0.7.2  Published 2023-08-21 08:39:01+00:00 </p>"},{"location":"releases/core/#whats-changed_23","title":"What's Changed","text":""},{"location":"releases/core/#documentation_6","title":"Documentation","text":"<ul> <li>Update mkdocs packages by @GBBBAS in https://github.com/rtdip/core/pull/449</li> </ul>"},{"location":"releases/core/#ingestion-framework","title":"Ingestion Framework","text":"<ul> <li>ECMWF Source and Transformer Components by @Amber-Rigg in https://github.com/rtdip/core/pull/447</li> <li>Pandas version updates by @GBBBAS in https://github.com/rtdip/core/pull/451</li> </ul>"},{"location":"releases/core/#other-changes_15","title":"Other Changes","text":"<ul> <li>Fix for mkdocs build by @GBBBAS in https://github.com/rtdip/core/pull/448</li> <li>Honeywell APM transformers by @JamesKnBr in https://github.com/rtdip/core/pull/446</li> <li>Black Code Formatter by @GBBBAS in https://github.com/rtdip/core/pull/450</li> </ul>"},{"location":"releases/core/#full-changelog-httpsgithubcomrtdipcorecomparev071v072","title":"Full Changelog: https://github.com/rtdip/core/compare/v0.7.1...v0.7.2","text":""},{"location":"releases/core/#v071","title":"v0.7.1","text":"<p>v0.7.1  Published 2023-08-21 08:36:47+00:00 </p>"},{"location":"releases/core/#whats-changed_24","title":"What's Changed","text":""},{"location":"releases/core/#documentation_7","title":"Documentation","text":""},{"location":"releases/core/#other-changes_16","title":"Other Changes","text":"<ul> <li>Mkdocs build updates by @GBBBAS in https://github.com/rtdip/core/pull/448</li> </ul>"},{"location":"releases/core/#full-changelog-httpsgithubcomrtdipcorecomparev070v071","title":"Full Changelog: https://github.com/rtdip/core/compare/v0.7.0...v0.7.1","text":""},{"location":"releases/core/#v070","title":"v0.7.0","text":"<p>v0.7.0  Published 2023-08-17 10:41:37+00:00 </p>"},{"location":"releases/core/#whats-changed_25","title":"What's Changed","text":""},{"location":"releases/core/#documentation_8","title":"Documentation","text":"<ul> <li>Meter Forecasting Energy Domain Specific Preamble by @ellisk737 in https://github.com/rtdip/core/pull/435</li> <li>Updates for Mkdocs Blogs by @GBBBAS in https://github.com/rtdip/core/pull/436</li> </ul>"},{"location":"releases/core/#ingestion-framework_1","title":"Ingestion Framework","text":"<ul> <li>Additional fields for Eventhub Kafka component by @GBBBAS in https://github.com/rtdip/core/pull/438</li> </ul>"},{"location":"releases/core/#other-changes_17","title":"Other Changes","text":"<ul> <li>Spark Connector remove stopping Spark Session by @GBBBAS in https://github.com/rtdip/core/pull/439</li> <li>Added Dagster pipeline and query examples by @rodalynbarce in https://github.com/rtdip/core/pull/440</li> <li>Created Circular Average and Circular Standard Deviation Query Functions with Unit Tests and Docs by @cching95 in https://github.com/rtdip/core/pull/442</li> <li>Updated Circular-Average.md by @rodalynbarce in https://github.com/rtdip/core/pull/443</li> <li>Query Builder  by @GBBBAS in https://github.com/rtdip/core/pull/444</li> </ul>"},{"location":"releases/core/#full-changelog-httpsgithubcomrtdipcorecomparev066v070","title":"Full Changelog: https://github.com/rtdip/core/compare/v0.6.6...v0.7.0","text":""},{"location":"releases/core/#v066","title":"v0.6.6","text":"<p>v0.6.6  Published 2023-08-09 16:41:50+00:00 </p>"},{"location":"releases/core/#whats-changed_26","title":"What's Changed","text":""},{"location":"releases/core/#ingestion-framework_2","title":"Ingestion Framework","text":"<ul> <li>Updates for PCDM to Delta by @GBBBAS in https://github.com/rtdip/core/pull/432</li> </ul>"},{"location":"releases/core/#other-changes_18","title":"Other Changes","text":"<ul> <li>Optimise Queries for Resample and Interpolation and Update Unit Tests by @cching95 in https://github.com/rtdip/core/pull/431</li> <li>v0.6.6 by @GBBBAS in https://github.com/rtdip/core/pull/433</li> </ul>"},{"location":"releases/core/#full-changelog-httpsgithubcomrtdipcorecomparev065v066","title":"Full Changelog: https://github.com/rtdip/core/compare/v0.6.5...v0.6.6","text":""},{"location":"releases/core/#v065","title":"v0.6.5","text":"<p>v0.6.5  Published 2023-08-08 16:31:28+00:00 </p>"},{"location":"releases/core/#whats-changed_27","title":"What's Changed","text":""},{"location":"releases/core/#ingestion-framework_3","title":"Ingestion Framework","text":"<ul> <li>Pandas Package upgrade by @GBBBAS in https://github.com/rtdip/core/pull/424</li> </ul>"},{"location":"releases/core/#other-changes_19","title":"Other Changes","text":"<ul> <li>Add Linear Interpolation to Interpolate Function with updated Docs and Unit Tests by @cching95 in https://github.com/rtdip/core/pull/423</li> </ul>"},{"location":"releases/core/#full-changelog-httpsgithubcomrtdipcorecomparev064v065","title":"Full Changelog: https://github.com/rtdip/core/compare/v0.6.4...v0.6.5","text":""},{"location":"releases/core/#v064","title":"v0.6.4","text":"<p>v0.6.4  Published 2023-08-07 16:22:01+00:00 </p>"},{"location":"releases/core/#whats-changed_28","title":"What's Changed","text":""},{"location":"releases/core/#documentation_9","title":"Documentation","text":"<ul> <li>Add default pip install in getting started docs  by @JamesKnBr in https://github.com/rtdip/core/pull/420</li> </ul>"},{"location":"releases/core/#ingestion-framework_4","title":"Ingestion Framework","text":"<ul> <li>Add Trigger \"AvailableNow\" Logic for Streams by @GBBBAS in https://github.com/rtdip/core/pull/419</li> <li>Update for casting of data types on SSIP Stream by @GBBBAS in https://github.com/rtdip/core/pull/421</li> </ul>"},{"location":"releases/core/#other-changes_20","title":"Other Changes","text":"<ul> <li>Updates for Badges on README by @GBBBAS in https://github.com/rtdip/core/pull/417</li> </ul>"},{"location":"releases/core/#full-changelog-httpsgithubcomrtdipcorecomparev063v064","title":"Full Changelog: https://github.com/rtdip/core/compare/v0.6.3...v0.6.4","text":""},{"location":"releases/core/#v063","title":"v0.6.3","text":"<p>v0.6.3  Published 2023-08-04 08:35:26+00:00 </p>"},{"location":"releases/core/#whats-changed_29","title":"What's Changed","text":""},{"location":"releases/core/#ingestion-framework_5","title":"Ingestion Framework","text":"<ul> <li>Add option to remove nanoseconds from Process Control Data Model writes on timestamp by @GBBBAS in https://github.com/rtdip/core/pull/412</li> <li>Minor adjustments for Pipeline functions by @GBBBAS in https://github.com/rtdip/core/pull/415</li> </ul>"},{"location":"releases/core/#other-changes_21","title":"Other Changes","text":"<ul> <li>Add polars to pipeline packages by @JamesKnBr in https://github.com/rtdip/core/pull/414</li> </ul>"},{"location":"releases/core/#full-changelog-httpsgithubcomrtdipcorecomparev062v063","title":"Full Changelog: https://github.com/rtdip/core/compare/v0.6.2...v0.6.3","text":""},{"location":"releases/core/#v062","title":"v0.6.2","text":"<p>v0.6.2  Published 2023-08-03 08:56:33+00:00 </p>"},{"location":"releases/core/#whats-changed_30","title":"What's Changed","text":""},{"location":"releases/core/#ingestion-framework_6","title":"Ingestion Framework","text":"<ul> <li>Add Python delta source and destination by @JamesKnBr in https://github.com/rtdip/core/pull/407</li> </ul>"},{"location":"releases/core/#other-changes_22","title":"Other Changes","text":"<ul> <li>Weather Transformer by @PaveeG in https://github.com/rtdip/core/pull/387</li> <li>API Documentation Updates by @GBBBAS in https://github.com/rtdip/core/pull/404</li> <li>AMQP value decoding from Kafka Headers by @GBBBAS in https://github.com/rtdip/core/pull/403</li> <li>Return Properties initial value if Kafka Headers not populated by @GBBBAS in https://github.com/rtdip/core/pull/405</li> <li>Add metadata Step to the Time Weighted Average Function, Unit test and Documentation updates. by @cching95 in https://github.com/rtdip/core/pull/408</li> </ul>"},{"location":"releases/core/#full-changelog-httpsgithubcomrtdipcorecomparev061v062","title":"Full Changelog: https://github.com/rtdip/core/compare/v0.6.1...v0.6.2","text":""},{"location":"releases/core/#v061","title":"v0.6.1","text":"<p>v0.6.1  Published 2023-07-27 15:04:11+00:00 </p>"},{"location":"releases/core/#whats-changed_31","title":"What's Changed","text":""},{"location":"releases/core/#bug-fixes_10","title":"Bug Fixes","text":"<ul> <li>Hotfix to update the tagname parameter in the Metadata query by @cching95 in https://github.com/rtdip/core/pull/401</li> </ul>"},{"location":"releases/core/#full-changelog-httpsgithubcomrtdipcorecomparev060v061","title":"Full Changelog: https://github.com/rtdip/core/compare/v0.6.0...v0.6.1","text":""},{"location":"releases/core/#v060","title":"v0.6.0","text":"<p>v0.6.0  Published 2023-07-26 16:05:03+00:00 </p>"},{"location":"releases/core/#whats-changed_32","title":"What's Changed","text":""},{"location":"releases/core/#new-features_4","title":"New Features","text":""},{"location":"releases/core/#bug-fixes_11","title":"Bug Fixes","text":"<ul> <li>Doc updates and paramater fix by @cching95 in https://github.com/rtdip/core/pull/398</li> </ul>"},{"location":"releases/core/#ingestion-framework_7","title":"Ingestion Framework","text":"<ul> <li>Transformer component - PJM raw data to Meters Data Model by @IW-SS in https://github.com/rtdip/core/pull/388</li> <li>Update Databricks Deploy to use the Databricks SDK by @GBBBAS in https://github.com/rtdip/core/pull/394</li> <li>Kafka Eventhub Source by @GBBBAS in https://github.com/rtdip/core/pull/395</li> </ul>"},{"location":"releases/core/#other-changes_23","title":"Other Changes","text":"<ul> <li>Remove JinjaSQL and refactor query builder for raw and metadata functions by @cching95 in https://github.com/rtdip/core/pull/384</li> <li>Refactored Resample and Interpolate, removed Jinjasql and updated unit tests by @cching95 in https://github.com/rtdip/core/pull/389</li> <li>Refactor time weighted average and add unit tests  by @cching95 in https://github.com/rtdip/core/pull/392</li> <li>Refactored Interpolation at Time and updated unit tests, removed jinjasql by @cching95 in https://github.com/rtdip/core/pull/391</li> <li>Update time interval parameters and api parameter descriptions and add distinct to base query by @cching95 in https://github.com/rtdip/core/pull/393</li> </ul>"},{"location":"releases/core/#full-changelog-httpsgithubcomrtdipcorecomparev053v060","title":"Full Changelog: https://github.com/rtdip/core/compare/v0.5.3...v0.6.0","text":""},{"location":"releases/core/#v053","title":"v0.5.3","text":"<p>v0.5.3  Published 2023-07-19 09:05:49+00:00 </p>"},{"location":"releases/core/#whats-changed_33","title":"What's Changed","text":""},{"location":"releases/core/#bug-fixes_12","title":"Bug Fixes","text":"<ul> <li>Update packages for API Container by @GBBBAS in https://github.com/rtdip/core/pull/373</li> <li>Add edgeX to Transformers init by @JamesKnBr in https://github.com/rtdip/core/pull/376</li> </ul>"},{"location":"releases/core/#documentation_10","title":"Documentation","text":"<ul> <li>Docs for running APIs locally on a Macbook by @GBBBAS in https://github.com/rtdip/core/pull/379</li> </ul>"},{"location":"releases/core/#ingestion-framework_8","title":"Ingestion Framework","text":"<ul> <li>Source Component - Energy Domain PJM market by @ellisk737 in https://github.com/rtdip/core/pull/370</li> <li>S3 Copy Utilities by @vbayon in https://github.com/rtdip/core/pull/378</li> </ul>"},{"location":"releases/core/#other-changes_24","title":"Other Changes","text":"<ul> <li>Dependency Updates for July by @GBBBAS in https://github.com/rtdip/core/pull/367</li> <li>Remove appuser from Dockerfile by @rodalynbarce in https://github.com/rtdip/core/pull/372</li> <li>API Convert NaN to None in JSON Response by @GBBBAS in https://github.com/rtdip/core/pull/377</li> <li>Add good/bad filter to interpolation at time API by @cching95 in https://github.com/rtdip/core/pull/381</li> <li>Include path option for delta destinations by @JamesKnBr in https://github.com/rtdip/core/pull/369</li> </ul>"},{"location":"releases/core/#new-contributors_1","title":"New Contributors","text":"<ul> <li>@ellisk737 made their first contribution in https://github.com/rtdip/core/pull/370</li> </ul>"},{"location":"releases/core/#full-changelog-httpsgithubcomrtdipcorecomparev052v053","title":"Full Changelog: https://github.com/rtdip/core/compare/v0.5.2...v0.5.3","text":""},{"location":"releases/core/#v052","title":"v0.5.2","text":"<p>v0.5.2  Published 2023-07-11 15:51:57+00:00 </p>"},{"location":"releases/core/#whats-changed_34","title":"What's Changed","text":""},{"location":"releases/core/#bug-fixes_13","title":"Bug Fixes","text":"<ul> <li>Change Imports for SSIP UDF to execute properly by @GBBBAS in https://github.com/rtdip/core/pull/352</li> <li>Updates for Default Packages Logic by @GBBBAS in https://github.com/rtdip/core/pull/353</li> <li>Fix for unit test runtime performance by @GBBBAS in https://github.com/rtdip/core/pull/358</li> <li>Fix for Circular Imports by @GBBBAS in https://github.com/rtdip/core/pull/359</li> </ul>"},{"location":"releases/core/#documentation_11","title":"Documentation","text":"<ul> <li>Add OpenSSF Tag by @GBBBAS in https://github.com/rtdip/core/pull/361</li> </ul>"},{"location":"releases/core/#ingestion-framework_9","title":"Ingestion Framework","text":"<ul> <li>Spark Session Utility Component by @GBBBAS in https://github.com/rtdip/core/pull/349</li> <li>Weather Data Connector - Feature/00137 by @PaveeG in https://github.com/rtdip/core/pull/327</li> <li>Support for Raw data to MDM Transformer class with Implementation for MISO by @IW-SS in https://github.com/rtdip/core/pull/321 </li> <li>EVM Component Updates by @rodalynbarce in https://github.com/rtdip/core/pull/345</li> <li>Add good/bad filter to interpolation at time by @cching95 in https://github.com/rtdip/core/pull/317</li> <li>EVM destination component unit tests by @cching95 in https://github.com/rtdip/core/pull/355</li> </ul>"},{"location":"releases/core/#other-changes_25","title":"Other Changes","text":"<ul> <li>Remove Eventhub from docs by @GBBBAS in https://github.com/rtdip/core/pull/357</li> <li>Sonar Security Fixes by @GBBBAS in https://github.com/rtdip/core/pull/363</li> <li>Update Sonar Python Version by @GBBBAS in https://github.com/rtdip/core/pull/364</li> </ul>"},{"location":"releases/core/#full-changelog-httpsgithubcomrtdipcorecomparev051v052","title":"Full Changelog: https://github.com/rtdip/core/compare/v0.5.1...v0.5.2","text":""},{"location":"releases/core/#v051","title":"v0.5.1","text":"<p>v0.5.1  Published 2023-06-29 12:14:07+00:00 </p>"},{"location":"releases/core/#whats-changed_35","title":"What's Changed","text":""},{"location":"releases/core/#ingestion-framework_10","title":"Ingestion Framework","text":"<ul> <li>EVM Contract Destination by @rodalynbarce in https://github.com/rtdip/core/pull/343</li> </ul>"},{"location":"releases/core/#other-changes_26","title":"Other Changes","text":"<ul> <li>EVM Component Updates by @rodalynbarce in https://github.com/rtdip/core/pull/344</li> <li>v0.5.1 by @GBBBAS in https://github.com/rtdip/core/pull/346</li> </ul>"},{"location":"releases/core/#full-changelog-httpsgithubcomrtdipcorecomparev050v051","title":"Full Changelog: https://github.com/rtdip/core/compare/v0.5.0...v0.5.1","text":""},{"location":"releases/core/#v050","title":"v0.5.0","text":"<p>v0.5.0  Published 2023-06-23 10:36:37+00:00 </p>"},{"location":"releases/core/#whats-changed_36","title":"What's Changed","text":""},{"location":"releases/core/#bug-fixes_14","title":"Bug Fixes","text":"<ul> <li>Fix for version logic in Databricks by @GBBBAS in https://github.com/rtdip/core/pull/335</li> </ul>"},{"location":"releases/core/#ingestion-framework_11","title":"Ingestion Framework","text":"<ul> <li>Chat Open AI Databricks Connector by @GBBBAS in https://github.com/rtdip/core/pull/328</li> <li>Tagname Parameter in EdgeX transformer by @JamesKnBr in https://github.com/rtdip/core/pull/329</li> <li>PySpark to Pandas DataFrame Transformer by @rodalynbarce in https://github.com/rtdip/core/pull/336</li> </ul>"},{"location":"releases/core/#full-changelog-httpsgithubcomrtdipcorecomparev042v050","title":"Full Changelog: https://github.com/rtdip/core/compare/v0.4.2...v0.5.0","text":""},{"location":"releases/core/#v042","title":"v0.4.2","text":"<p>v0.4.2  Published 2023-06-20 11:43:37+00:00 </p>"},{"location":"releases/core/#whats-changed_37","title":"What's Changed","text":""},{"location":"releases/core/#other-changes_27","title":"Other Changes","text":"<ul> <li>Bug fix on mkdocs build by @GBBBAS in https://github.com/rtdip/core/pull/324</li> </ul>"},{"location":"releases/core/#full-changelog-httpsgithubcomrtdipcorecomparev041v042","title":"Full Changelog: https://github.com/rtdip/core/compare/v0.4.1...v0.4.2","text":""},{"location":"releases/core/#v041","title":"v0.4.1","text":"<p>v0.4.1  Published 2023-05-31 09:17:12+00:00 </p>"},{"location":"releases/core/#whats-changed_38","title":"What's Changed","text":""},{"location":"releases/core/#bug-fixes_15","title":"Bug Fixes","text":"<ul> <li>Fix for Deprecated Imports by @GBBBAS in https://github.com/rtdip/core/pull/305</li> </ul>"},{"location":"releases/core/#full-changelog-httpsgithubcomrtdipcorecomparev040v041","title":"Full Changelog: https://github.com/rtdip/core/compare/v0.4.0...v0.4.1","text":""},{"location":"releases/core/#v040","title":"v0.4.0","text":"<p>v0.4.0  Published 2023-05-30 09:48:19+00:00 </p>"},{"location":"releases/core/#whats-changed_39","title":"What's Changed","text":""},{"location":"releases/core/#bug-fixes_16","title":"Bug Fixes","text":"<ul> <li>Fix Mkdocs Colour Bug by @cching95 in https://github.com/rtdip/core/pull/291</li> </ul>"},{"location":"releases/core/#ingestion-framework_12","title":"Ingestion Framework","text":"<ul> <li>EdgeX OPC UA Transformer by @JamesKnBr in https://github.com/rtdip/core/pull/271</li> <li>MISO Market API Data Source by @IW-SS in https://github.com/rtdip/core/pull/272</li> <li>Add Partition Pruning to non broadcast join PCDM Merge by @GBBBAS in https://github.com/rtdip/core/pull/274</li> <li>Add Filter Parameter and TagName field mapping for OPC Publisher Transformer by @GBBBAS in https://github.com/rtdip/core/pull/296</li> <li>Refactor for odbc and functions folders by @GBBBAS in https://github.com/rtdip/core/pull/299</li> <li>Pyspark 3.4.0 and delta-spark 2.4.0 support by @GBBBAS in https://github.com/rtdip/core/pull/297</li> <li>Add OPC UA Transformer Parameters by @GBBBAS in https://github.com/rtdip/core/pull/301</li> </ul>"},{"location":"releases/core/#other-changes_28","title":"Other Changes","text":"<ul> <li>Weather Domain and Data Model by @PaveeG in https://github.com/rtdip/core/pull/294</li> <li>Update for Micromamba Github Actions Parameters by @GBBBAS in https://github.com/rtdip/core/pull/302</li> <li>Upgrade FastAPI Package by @GBBBAS in https://github.com/rtdip/core/pull/303</li> </ul>"},{"location":"releases/core/#new-contributors_2","title":"New Contributors","text":"<ul> <li>@IW-SS made their first contribution in https://github.com/rtdip/core/pull/272</li> <li>@PaveeG made their first contribution in https://github.com/rtdip/core/pull/294</li> </ul>"},{"location":"releases/core/#full-changelog-httpsgithubcomrtdipcorecomparev031v040","title":"Full Changelog: https://github.com/rtdip/core/compare/v0.3.1...v0.4.0","text":""},{"location":"releases/core/#v031","title":"v0.3.1","text":"<p>v0.3.1  Published 2023-05-16 07:05:05+00:00 </p>"},{"location":"releases/core/#whats-changed_40","title":"What's Changed","text":""},{"location":"releases/core/#bug-fixes_17","title":"Bug Fixes","text":"<ul> <li>Databricks System Parameter Index Change by @GBBBAS in https://github.com/rtdip/core/pull/261</li> <li>Timezone in API by @cching95 in https://github.com/rtdip/core/pull/268</li> </ul>"},{"location":"releases/core/#full-changelog-httpsgithubcomrtdipcorecomparev030v031","title":"Full Changelog: https://github.com/rtdip/core/compare/v0.3.0...v0.3.1","text":""},{"location":"releases/core/#v030","title":"v0.3.0","text":"<p>v0.3.0  Published 2023-05-12 16:00:58+00:00 </p>"},{"location":"releases/core/#whats-changed_41","title":"What's Changed","text":""},{"location":"releases/core/#new-features_5","title":"New Features","text":"<ul> <li>Time Weighted Average API by @cching95 in https://github.com/rtdip/core/pull/207</li> <li>Interpolation at Time Query Function and API by @cching95 in https://github.com/rtdip/core/pull/244</li> </ul>"},{"location":"releases/core/#documentation_12","title":"Documentation","text":"<ul> <li>Process Control Data Model by @GBBBAS in https://github.com/rtdip/core/pull/234</li> <li>Added Time Zone logic to Documentation by @rodalynbarce in https://github.com/rtdip/core/pull/236</li> </ul>"},{"location":"releases/core/#ingestion-framework_13","title":"Ingestion Framework","text":"<ul> <li>Package Setup Update for Data Files by @GBBBAS in https://github.com/rtdip/core/pull/206</li> <li>Performance Updates, Transformer and Destinations Updates by @GBBBAS in https://github.com/rtdip/core/pull/215</li> <li>Delta Merge Destination Component by @GBBBAS in https://github.com/rtdip/core/pull/220</li> <li>Fledge transformer Component by @JamesKnBr in https://github.com/rtdip/core/pull/221</li> <li>Process Control Data Model to Delta by @GBBBAS in https://github.com/rtdip/core/pull/225</li> <li>Rest API Destination Component by @GBBBAS in https://github.com/rtdip/core/pull/223</li> <li>Hashicorp Vault Secret Component by @GBBBAS in https://github.com/rtdip/core/pull/227</li> <li>Spark ADLS Gen2 Service Principal Connect Utility by @GBBBAS in https://github.com/rtdip/core/pull/232</li> <li>Azure Key Vault Secret Component by @GBBBAS in https://github.com/rtdip/core/pull/230</li> <li>SSIP PI Transformer Components by @GBBBAS in https://github.com/rtdip/core/pull/233</li> <li>Delta Batch Updates by @GBBBAS in https://github.com/rtdip/core/pull/238</li> <li>Union of Types for ADLS Gen 2 Credentials Parameter by @GBBBAS in https://github.com/rtdip/core/pull/246</li> <li>Combine ChangeTypes for Merge of PCDM Data by @GBBBAS in https://github.com/rtdip/core/pull/242</li> <li>Support 3.3.* Pyspark Versions by @GBBBAS in https://github.com/rtdip/core/pull/251</li> </ul>"},{"location":"releases/core/#other-changes_29","title":"Other Changes","text":"<ul> <li>Performance Updates for Turbodbc and Databricks SQL Connector by @GBBBAS in https://github.com/rtdip/core/pull/211</li> <li>Add Arrow support for Turbodbc by @GBBBAS in https://github.com/rtdip/core/pull/212</li> <li>Turbodbc Performance Updates by @GBBBAS in https://github.com/rtdip/core/pull/213</li> <li>Delta Table Creation using Pydantic Models for Column by @GBBBAS in https://github.com/rtdip/core/pull/240</li> <li>Remove Duplicates from PCDM to Delta by @GBBBAS in https://github.com/rtdip/core/pull/241</li> <li>Updates for AMI Meter Classes by @vbayon in https://github.com/rtdip/core/pull/243</li> <li>Fix for Unioned Types on ACLs by @GBBBAS in https://github.com/rtdip/core/pull/247</li> </ul>"},{"location":"releases/core/#full-changelog-httpsgithubcomrtdipcorecomparev022v030","title":"Full Changelog: https://github.com/rtdip/core/compare/v0.2.2...v0.3.0","text":""},{"location":"releases/core/#v022","title":"v0.2.2","text":"<p>v0.2.2  Published 2023-04-28 08:49:56+00:00 </p>"},{"location":"releases/core/#whats-changed_42","title":"What's Changed","text":""},{"location":"releases/core/#new-features_6","title":"New Features","text":"<ul> <li>Timezone feature for querying functions by @cching95 in https://github.com/rtdip/core/pull/199</li> </ul>"},{"location":"releases/core/#bug-fixes_18","title":"Bug Fixes","text":"<ul> <li>Github Actions Sonarcloud Refactor for forked branches by @GBBBAS in https://github.com/rtdip/core/pull/170</li> </ul>"},{"location":"releases/core/#documentation_13","title":"Documentation","text":"<ul> <li>Removed repo structure from readme by @JamesKnBr in https://github.com/rtdip/core/pull/180</li> <li>Documentation Updates by @GBBBAS in https://github.com/rtdip/core/pull/202</li> </ul>"},{"location":"releases/core/#ingestion-framework_14","title":"Ingestion Framework","text":"<ul> <li>Azure ADLS Gen 2 ACLs Utility by @GBBBAS in https://github.com/rtdip/core/pull/181</li> <li>S3 Bucket Policy by @GBBBAS in https://github.com/rtdip/core/pull/200</li> </ul>"},{"location":"releases/core/#full-changelog-httpsgithubcomrtdipcorecomparev021v022","title":"Full Changelog: https://github.com/rtdip/core/compare/v0.2.1...v0.2.2","text":""},{"location":"releases/core/#v021","title":"v0.2.1","text":"<p>v0.2.1  Published 2023-04-21 15:26:14+00:00 </p>"},{"location":"releases/core/#whats-changed_43","title":"What's Changed","text":""},{"location":"releases/core/#documentation_14","title":"Documentation","text":"<ul> <li>Documentation Minor Updates by @JamesKnBr in https://github.com/rtdip/core/pull/136</li> <li>Fix Mkdocs error for kafka source by @cching95 in https://github.com/rtdip/core/pull/161</li> </ul>"},{"location":"releases/core/#ingestion-framework_15","title":"Ingestion Framework","text":"<ul> <li>Delta Optimize and Vacuum by @GBBBAS in https://github.com/rtdip/core/pull/140</li> <li>Spark Pipeline Execute and Deploy unit tests by @JamesKnBr in https://github.com/rtdip/core/pull/142</li> <li>Add pipeline source IoT Hub and unit test by @rodalynbarce in https://github.com/rtdip/core/pull/144</li> <li>Spark Source and Destination for Kafka by @cching95 in https://github.com/rtdip/core/pull/158</li> <li>Added Kinesis source and destination by @rodalynbarce in https://github.com/rtdip/core/pull/159</li> <li>OPC UA And Process Control Data Model Transformers by @GBBBAS in https://github.com/rtdip/core/pull/162</li> </ul>"},{"location":"releases/core/#other-changes_30","title":"Other Changes","text":"<ul> <li>Added target to pull_request in pr workflow by @rodalynbarce in https://github.com/rtdip/core/pull/153</li> <li>AMI Meter Data Model by @vbayon in https://github.com/rtdip/core/pull/126</li> </ul>"},{"location":"releases/core/#new-contributors_3","title":"New Contributors","text":"<ul> <li>@vbayon made their first contribution in https://github.com/rtdip/core/pull/126</li> </ul>"},{"location":"releases/core/#full-changelog-httpsgithubcomrtdipcorecomparev020v021","title":"Full Changelog: https://github.com/rtdip/core/compare/v0.2.0...v0.2.1","text":""},{"location":"releases/core/#v020","title":"v0.2.0","text":"<p>v0.2.0  Published 2023-04-03 12:32:58+00:00 </p>"},{"location":"releases/core/#whats-changed_44","title":"What's Changed","text":""},{"location":"releases/core/#bug-fixes_19","title":"Bug Fixes","text":"<ul> <li>Fix for docker build error by @GBBBAS in https://github.com/rtdip/core/pull/114</li> <li>Fix for docker build by @GBBBAS in https://github.com/rtdip/core/pull/115</li> <li>Time weighted average bug fix by @cching95 in https://github.com/rtdip/core/pull/120</li> </ul>"},{"location":"releases/core/#other-changes_31","title":"Other Changes","text":"<ul> <li>Documentation: Add Releases Feed to Docs Release page by @GBBBAS in https://github.com/rtdip/core/pull/69</li> <li>Documentation: Update Packages and Doc Logos by @GBBBAS in https://github.com/rtdip/core/pull/76</li> <li>Documentation: Fix Logo Alignment by @GBBBAS in https://github.com/rtdip/core/pull/78</li> <li>Pipelines: Upgrade dbx by @GBBBAS in https://github.com/rtdip/core/pull/79</li> <li>Pipelines: add spark session by @cching95 in https://github.com/rtdip/core/pull/80</li> <li>Pipelines: Add Pipelines as Extras libraries to setup by @GBBBAS in https://github.com/rtdip/core/pull/82</li> <li>Pipelines: Spark pipeline source Event hub and destination Event hub by @cching95 in https://github.com/rtdip/core/pull/83</li> <li>Pipelines: Spark pipeline Source and Destination Delta Components by @rodalynbarce in https://github.com/rtdip/core/pull/84</li> <li>Pipelines: Added Source Delta Sharing component with docs by @JamesKnBr in https://github.com/rtdip/core/pull/85</li> <li>Pipelines: Unit Tests for sources by @cching95 in https://github.com/rtdip/core/pull/88</li> <li>Pipelines: Add Transformer Eventhub body transformer by @cching95 in https://github.com/rtdip/core/pull/89</li> <li>Pipelines: Added pipeline source autoloader and unit test by @rodalynbarce in https://github.com/rtdip/core/pull/93</li> <li>Pipelines: Pipeline Executor by @GBBBAS in https://github.com/rtdip/core/pull/95</li> <li>Pipelines: Delta Table Create Utility by @GBBBAS in https://github.com/rtdip/core/pull/97</li> <li>Pipelines: Pipeline Deployment by @GBBBAS in https://github.com/rtdip/core/pull/102</li> <li>Pipelines: Pipeline Deployment Updates by @GBBBAS in https://github.com/rtdip/core/pull/103</li> <li>Pipelines: Updates for Pipeline Deployment by @GBBBAS in https://github.com/rtdip/core/pull/104</li> <li>Pipelines: Unit test for spark pipeline eventhub destination by @cching95 in https://github.com/rtdip/core/pull/105</li> <li>Pipelines: Dependency Injector on Pipeline Execute by @GBBBAS in https://github.com/rtdip/core/pull/106</li> <li>Pipelines: Unit tests for source and destination streams by @cching95 in https://github.com/rtdip/core/pull/108</li> <li>Pipelines: Pipeline Deployments and Pipeline Secrets by @GBBBAS in https://github.com/rtdip/core/pull/111</li> <li>Pipelines: Added fails tests to pipeline sources by @JamesKnBr in https://github.com/rtdip/core/pull/112</li> <li>Containers: Dockerfile refactor by @GBBBAS in https://github.com/rtdip/core/pull/116</li> <li>Pipelines: Pipeline Secrets, Converters and Deployment Updates by @GBBBAS in https://github.com/rtdip/core/pull/119</li> </ul>"},{"location":"releases/core/#new-contributors_4","title":"New Contributors","text":"<ul> <li>@rodalynbarce made their first contribution in https://github.com/rtdip/core/pull/84</li> <li>@JamesKnBr made their first contribution in https://github.com/rtdip/core/pull/85</li> </ul>"},{"location":"releases/core/#full-changelog-httpsgithubcomrtdipcorecommitsv020","title":"Full Changelog: https://github.com/rtdip/core/commits/v0.2.0","text":""},{"location":"releases/core/#v020-beta1","title":"v0.2.0-beta.1","text":"<p>v0.2.0-beta.1  Published 2023-03-30 10:21:49+00:00  Pre-release </p>"},{"location":"releases/core/#whats-changed_45","title":"What's Changed","text":""},{"location":"releases/core/#bug-fixes_20","title":"Bug Fixes","text":"<ul> <li>Hotfix to remove print statement in twa function by @cching95 in https://github.com/rtdip/core/pull/99</li> <li>Fix for docker build error by @GBBBAS in https://github.com/rtdip/core/pull/114</li> <li>Fix for docker build by @GBBBAS in https://github.com/rtdip/core/pull/115</li> <li>time weighted average bug fix by @cching95 in https://github.com/rtdip/core/pull/120</li> </ul>"},{"location":"releases/core/#ingestion-framework_16","title":"Ingestion Framework","text":"<ul> <li>Spark pipeline source Event hub and destination Event hub by @cching95 in https://github.com/rtdip/core/pull/83</li> <li>Spark pipeline source delta and destination delta by @rodalynbarce in https://github.com/rtdip/core/pull/84</li> <li>Added delta sharing with docs by @JamesKnBr in https://github.com/rtdip/core/pull/85</li> <li>Unit Tests for sources by @cching95 in https://github.com/rtdip/core/pull/88</li> <li>Added pipeline source autoloader and unit test by @rodalynbarce in https://github.com/rtdip/core/pull/93</li> <li>Pipeline Executor by @GBBBAS in https://github.com/rtdip/core/pull/95</li> <li>Delta Table Create Utility by @GBBBAS in https://github.com/rtdip/core/pull/97</li> <li>Upgrade dbx by @GBBBAS in https://github.com/rtdip/core/pull/79</li> <li>add spark session by @cching95 in https://github.com/rtdip/core/pull/80</li> <li>Add Pipelines as Extras libraries to setup by @GBBBAS in https://github.com/rtdip/core/pull/82</li> <li>Add pipeline eventhub body transformer by @cching95 in https://github.com/rtdip/core/pull/89</li> <li>Pipeline Deployment by @GBBBAS in https://github.com/rtdip/core/pull/102</li> <li>Pipeline Deployment Updates by @GBBBAS in https://github.com/rtdip/core/pull/103</li> <li>Updates for Pipeline Deployment by @GBBBAS in https://github.com/rtdip/core/pull/104</li> <li>Unit test for spark pipeline eventhub destination by @cching95 in https://github.com/rtdip/core/pull/105</li> <li>Dependency Injector on Pipeline Execute by @GBBBAS in https://github.com/rtdip/core/pull/106</li> <li>Unit tests for source and destination streams by @cching95 in https://github.com/rtdip/core/pull/108</li> <li>Pipeline Deployments and Pipeline Secrets by @GBBBAS in https://github.com/rtdip/core/pull/111</li> <li>Added fails tests to pipeline sources by @JamesKnBr in https://github.com/rtdip/core/pull/112</li> <li>Pipeline Secrets, Converters and Deployment Updates by @GBBBAS in https://github.com/rtdip/core/pull/119</li> </ul>"},{"location":"releases/core/#documentation_15","title":"Documentation","text":"<ul> <li>Add Releases Feed to Docs Release page by @GBBBAS in https://github.com/rtdip/core/pull/69</li> <li>Update Packages and Doc Logos by @GBBBAS in https://github.com/rtdip/core/pull/76</li> <li>Fix Logo Alignment by @GBBBAS in https://github.com/rtdip/core/pull/78</li> </ul>"},{"location":"releases/core/#other-changes_32","title":"Other Changes","text":"<ul> <li>Dockerfile refactor by @GBBBAS in https://github.com/rtdip/core/pull/116</li> <li>Update python version for Release build by @GBBBAS in https://github.com/rtdip/core/pull/123</li> </ul>"},{"location":"releases/core/#new-contributors_5","title":"New Contributors","text":"<ul> <li>@rodalynbarce made their first contribution in https://github.com/rtdip/core/pull/84</li> <li>@JamesKnBr made their first contribution in https://github.com/rtdip/core/pull/85</li> </ul>"},{"location":"releases/core/#full-changelog-httpsgithubcomrtdipcorecomparev016v020","title":"Full Changelog: https://github.com/rtdip/core/compare/v0.1.6...v0.2.0","text":""},{"location":"releases/core/#v016","title":"v0.1.6","text":"<p>v0.1.6  Published 2023-03-21 11:41:08+00:00 </p>"},{"location":"releases/core/#whats-changed_46","title":"What's Changed","text":""},{"location":"releases/core/#other-changes_33","title":"Other Changes","text":"<ul> <li>Remove print statement in TWA function by @cching95 in https://github.com/rtdip/core/pull/100</li> </ul>"},{"location":"releases/core/#full-changelog-httpsgithubcomrtdipcorecomparev015v016","title":"Full Changelog: https://github.com/rtdip/core/compare/v0.1.5...v0.1.6","text":""},{"location":"releases/core/#v015","title":"v0.1.5","text":"<p>v0.1.5  Published 2023-02-24 16:55:17+00:00 </p>"},{"location":"releases/core/#whats-changed_47","title":"What's Changed","text":""},{"location":"releases/core/#other-changes_34","title":"Other Changes","text":"<ul> <li>Add OS Matrix to CI Tests by @GBBBAS in https://github.com/rtdip/core/pull/65</li> <li>Real Time Data Pipelines Blog by @GBBBAS in https://github.com/rtdip/core/pull/60</li> <li>Remove OS matrix from Actions due to Boost by @GBBBAS in https://github.com/rtdip/core/pull/66</li> <li>v0.1.5 by @GBBBAS in https://github.com/rtdip/core/pull/67</li> </ul>"},{"location":"releases/core/#full-changelog-httpsgithubcomrtdipcorecomparev014v015","title":"Full Changelog: https://github.com/rtdip/core/compare/v0.1.4...v0.1.5","text":""},{"location":"releases/core/#v014","title":"v0.1.4","text":"<p>v0.1.4  Published 2023-02-09 09:23:43+00:00 </p>"},{"location":"releases/core/#whats-changed_48","title":"What's Changed","text":""},{"location":"releases/core/#bug-fixes_21","title":"Bug Fixes","text":"<ul> <li>Removal of packaging legacy version dependency by @NooraKubati in https://github.com/rtdip/core/pull/57</li> </ul>"},{"location":"releases/core/#other-changes_35","title":"Other Changes","text":"<ul> <li>Update tests for multiple versions of Python by @GBBBAS in https://github.com/rtdip/core/pull/40</li> <li>Roadmap 2023 by @GBBBAS in https://github.com/rtdip/core/pull/45</li> <li>v0.1.4 by @GBBBAS in https://github.com/rtdip/core/pull/46</li> <li>updated devcontainer.json by @cching95 in https://github.com/rtdip/core/pull/50</li> <li>fix spelling error in releases document by @cching95 in https://github.com/rtdip/core/pull/53</li> <li>v0.1.4 #2 by @NooraKubati in https://github.com/rtdip/core/pull/58</li> </ul>"},{"location":"releases/core/#full-changelog-httpsgithubcomrtdipcorecomparev013v014","title":"Full Changelog: https://github.com/rtdip/core/compare/v0.1.3...v0.1.4","text":""},{"location":"releases/core/#v013","title":"v0.1.3","text":"<p>v0.1.3  Published 2022-12-14 11:11:41+00:00 </p>"},{"location":"releases/core/#whats-changed_49","title":"What's Changed","text":""},{"location":"releases/core/#other-changes_36","title":"Other Changes","text":"<ul> <li>Add CNAME file to docs for custom domain www.rtdip.io by @GBBBAS in https://github.com/rtdip/core/pull/34</li> <li>API for Time weighted average by @cching95 in https://github.com/rtdip/core/pull/29</li> <li>Upgrade Python to 3.11 by @GBBBAS in https://github.com/rtdip/core/pull/36</li> <li>v0.1.3 by @NooraKubati in https://github.com/rtdip/core/pull/38</li> </ul>"},{"location":"releases/core/#new-contributors_6","title":"New Contributors","text":"<ul> <li>@cching95 made their first contribution in https://github.com/rtdip/core/pull/29</li> <li>@NooraKubati made their first contribution in https://github.com/rtdip/core/pull/38</li> </ul>"},{"location":"releases/core/#full-changelog-httpsgithubcomrtdipcorecomparev012v013","title":"Full Changelog: https://github.com/rtdip/core/compare/v0.1.2...v0.1.3","text":""},{"location":"releases/core/#v012","title":"v0.1.2","text":"<p>v0.1.2  Published 2022-12-08 15:07:23+00:00 </p>"},{"location":"releases/core/#whats-changed_50","title":"What's Changed","text":""},{"location":"releases/core/#new-features_7","title":"New Features","text":"<ul> <li>Updates for Mamba in CI/CD by @GBBBAS in https://github.com/rtdip/core/pull/21</li> </ul>"},{"location":"releases/core/#other-changes_37","title":"Other Changes","text":"<ul> <li>Add Mamba for Conda Installation by @GBBBAS in https://github.com/rtdip/core/pull/15</li> <li>Update to Micromamba Config in Github Actions by @GBBBAS in https://github.com/rtdip/core/pull/22</li> <li>Change to Micromamba Config by @GBBBAS in https://github.com/rtdip/core/pull/23</li> <li>Update scopes and remove Static App Configuration by @GBBBAS in https://github.com/rtdip/core/pull/20</li> <li>Add badges to Readme by @GBBBAS in https://github.com/rtdip/core/pull/17</li> <li>Add VSCode devcontainer by @GBBBAS in https://github.com/rtdip/core/pull/25</li> <li>Update to prerelease of docker containers by @GBBBAS in https://github.com/rtdip/core/pull/28</li> <li>Add RTDIP LF Energy logos by @GBBBAS in https://github.com/rtdip/core/pull/31</li> <li>v0.1.2 by @Amber-Rigg in https://github.com/rtdip/core/pull/32</li> </ul>"},{"location":"releases/core/#new-contributors_7","title":"New Contributors","text":"<ul> <li>@Amber-Rigg made their first contribution in https://github.com/rtdip/core/pull/32</li> </ul>"},{"location":"releases/core/#full-changelog-httpsgithubcomrtdipcorecomparev011v012","title":"Full Changelog: https://github.com/rtdip/core/compare/v0.1.1...v0.1.2","text":""},{"location":"releases/core/#v011","title":"v0.1.1","text":"<p>v0.1.1  Published 2022-11-01 18:58:45+00:00 </p>"},{"location":"releases/core/#whats-changed_51","title":"What's Changed","text":""},{"location":"releases/core/#bug-fixes_22","title":"Bug Fixes","text":"<ul> <li>Fix to Mkdocs package by @GBBBAS in https://github.com/rtdip/core/pull/7</li> </ul>"},{"location":"releases/core/#other-changes_38","title":"Other Changes","text":"<ul> <li>Configure Mend Bolt for GitHub by @mend-bolt-for-github in https://github.com/rtdip/core/pull/3</li> <li>v0.1.1 by @GBBBAS in https://github.com/rtdip/core/pull/8</li> <li>Fix to Mkdocs String Package by @GBBBAS in https://github.com/rtdip/core/pull/12</li> <li>v0.1.1 by @GBBBAS in https://github.com/rtdip/core/pull/13</li> </ul>"},{"location":"releases/core/#new-contributors_8","title":"New Contributors","text":"<ul> <li>@mend-bolt-for-github made their first contribution in https://github.com/rtdip/core/pull/3</li> </ul>"},{"location":"releases/core/#full-changelog-httpsgithubcomrtdipcorecomparev010v011","title":"Full Changelog: https://github.com/rtdip/core/compare/v0.1.0...v0.1.1","text":""},{"location":"releases/core/#v010","title":"v0.1.0","text":"<p>v0.1.0  Published 2022-11-01 17:47:16+00:00 </p>"},{"location":"releases/core/#whats-changed_52","title":"What's Changed","text":""},{"location":"releases/core/#other-changes_39","title":"Other Changes","text":"<ul> <li>Real Time Data Ingestion Platform by @GBBBAS in https://github.com/rtdip/core/pull/2</li> <li>v0.1.0 by @GBBBAS in https://github.com/rtdip/core/pull/4</li> </ul>"},{"location":"releases/core/#new-contributors_9","title":"New Contributors","text":"<ul> <li>@GBBBAS made their first contribution in https://github.com/rtdip/core/pull/2</li> </ul>"},{"location":"releases/core/#full-changelog-httpsgithubcomrtdipcorecommitsv010","title":"Full Changelog: https://github.com/rtdip/core/commits/v0.1.0","text":""},{"location":"roadmap/roadmap-overview/","title":"RTDIP Roadmap","text":"<p>This section provides periodical updates of what the RTDIP team will be working on over the short and long term. The team will provide updates at important periods of the year so that users of the platform can understand new capabilities and options coming to the platform.</p> <p>We welcome and encourage projects, developers, users and applications to contribute to our roadmaps. Please reach out to the RTDIP team if you have ideas or suggestions on what we could work on next.</p>"},{"location":"roadmap/yearly-roadmaps/2022-development-roadmap/","title":"RTDIP Development Roadmap in 2022","text":"<p>Defining a list of development items for RTDIP is always difficult because so much can change within  Digital Technologies in 12 months. However, as we head towards the end of the year of 2021, we have outlined themes of what RTDIP will look at in the Development and Innovation space in 2022.</p> <p>We welcome and encourage projects, developers, users and applications to contribute to our roadmaps. Please reach out to the RTDIP Technical Steering Committee team if you have ideas or suggestions on what we could innovate on in this space. We will continue to evolve these development items all through 2022 so please come back to this article throughout the year to see any new items that may be brought into scope.</p> <p>Note</p> <p>Development and Innovation items don't always make it to Production. </p>"},{"location":"roadmap/yearly-roadmaps/2022-development-roadmap/#tldr","title":"TL;DR","text":"<p>A brief summary of development and innovation items planned for 2022.</p> Item Description Estimated Quarter for Delivery Power BI Enable querying of RTDIP data via Power BI. While some work has started on this in 2021, this item explores rolling it out further and how users can combine RTDIP data with other data sources Q1 2022 Seeq Connector Enable querying of RTDIP data via Seeq. Scope is limited to simply querying RTDIP data, we may look at what else is possible with the connector once the base capability has been achieved Q1 2022 Delta Live Tables Leverage Delta Live Tables for ingestion of RTDIP data into Delta Format. Provides better processing, merging, data cleansing and monitoring capabilities to the RTDIP Delta Ingestion Pipelines Q1-Q2 2022 Multicloud Build certain existing RTDIP Azure capabilities on AWS. Enables RTDIP in the clouds aligned with the business but also to ensure multicloud is cost effective and that products in the architecture work in Cloud Environments Q1-Q3 2022 SDK An open source python SDK is developed to assist users with a simple python library for connecting, authenticating and querying RTDIP data Q1-Q4 2022 REST API Wrap the python SDK in a REST API to allow non Python users to get similar functionality to the python SDK Q1-Q4 2022 Unity Catalog Provides a multi-region Catalog of all data in RTDIP. Enables easier navigation and exploration of what datasets are available in RTDIP Q3 2022 Delta Sharing Enables sharing of Delta data via a managed service that handles security, authentication and delivery of data. Particularly useful for sharing RTDIP data with third parties Q4 2022"},{"location":"roadmap/yearly-roadmaps/2022-development-roadmap/#power-bi","title":"Power BI","text":"<p>Power BI is a popular tool amongst RTDIP End Users for querying and plotting RTDIP data. The use of Delta and Databricks SQL Warehouses in the RTDIP Platform brings native Power BI integration using connectors already available in Power BI versions after May 2021.</p> <p>The aim is to enable Power BI connectivity to RTDIP so that users can query their data by the end of Q1 2022.</p>"},{"location":"roadmap/yearly-roadmaps/2022-development-roadmap/#seeq","title":"Seeq","text":"<p>Similar to Power BI, Seeq is a popular tool amongst real time users to query and manipulate RTDIP data. Seeq and RTDIP are currently working on a connector that allows Seeq to query RTDIP data via the same Databricks SQL Warehouse that Power BI will use for querying data by the end of Q1 2022.</p>"},{"location":"roadmap/yearly-roadmaps/2022-development-roadmap/#delta-live-tables","title":"Delta Live Tables","text":"<p>For more information about the advantages of Delta Live Tables, please see this link and if you would like to see Bryce Bartmann, RTDIP team member, talking about Delta Live Tables at the Data &amp; AI Summit 2021, please see the session here.</p> <p>RTDIP has been converting it's data to the open source format Delta using standard PySpark structured streaming jobs. Whilst this has been working well for converting RTDIP data to Delta, Delta Live Tables from Databricks provides similar capabilities as standard spark code, but with additional benefits:</p> <ul> <li>Expectations: Allows developers to specify data cleansing rules on ingested data. This can assist to provide higher quality, more reliable data to users</li> <li>Data Flows: Visually describes the flow of the data through the data pipelines from source to target, including data schemas and record counts</li> <li>Maintenance: Delta Live Tables simplifies maintenance tasks required for Delta Tables by scheduling them automatically based on the deployment of the Delta Live Tables Job</li> <li>Monitoring: Delta Live Tables are easier to monitor as their graphical schematics help non-technical people to understand the status of the ingestion pipelines</li> </ul> <p>The RTDIP Team has actively worked with Databricks to build Delta Live Tables. Whilst the product is well understood, certain features like merging data needed to be made available before RTDIP could fully migrate existing spark jobs to Delta Live Tables. Databricks intend to provide the Merge function in late Q4 2021 which will then trigger this piece of work with a target of having a decision point to move to production in Q1 2022.</p>"},{"location":"roadmap/yearly-roadmaps/2022-development-roadmap/#multicloud","title":"Multicloud","text":"<p>As clouds mature, one of the most asked questions is how customers can leverage more than one cloud to provide a better and more economical solution to their customers. Even though this is a fairly new area to explore, there are a number of cloud agnostic technologies that are trying to help customers take advantage of and manage environments in more than one cloud.</p> <p>Multicloud design can be complex and requires significant analysis of existing technologies capabilities and how they translate into benefits for RTDIP customers. Databricks will be one good example of exploring their new multicloud environment management tool and how this could benefit businesses in the long run. We expect there to be more technologies that come out with multicloud capabilities throughout 2022 and we will continue to explore, test and understand how RTDIP can leverage these throughout 2022 and beyond.</p>"},{"location":"roadmap/yearly-roadmaps/2022-development-roadmap/#sdk-and-rest-api","title":"SDK and REST API","text":"<p>A common theme we are seeing amongst applications and users of RTDIP data is a simple way to authenticate, query and manipulate RTDIP data. In an effort to also build a stronger developer community around RTDIP, we will be building a python SDK that python users can use in their code for performing common functions with RTDIP Data:</p> <ul> <li>Authenticating with RTDIP</li> <li>Connecting to RTDIP data</li> <li>Querying RTDIP raw data</li> <li>Performing sampling on raw data</li> <li>Performing interpolation on sampled data</li> </ul> <p>We plan to deliver the first version of the python SDK early in 2022 and welcome all python developers to contribute to the repository. </p> <p>For non python users, we plan to wrap the SDK in a REST API. This facilitates a language agnostic way of benefitting from all the development of the python SDK. These REST APIs will be rolled out in line with functionality built with the python SDK.</p>"},{"location":"roadmap/yearly-roadmaps/2022-development-roadmap/#unity-catalog","title":"Unity Catalog","text":"<p>Cataloging data is a common activity when building data lakes that contain data from multiple sources and from multiple geographic regions. RTDIP will explore and deploy a catalog of all data sources currently being ingested into the platform.</p>"},{"location":"roadmap/yearly-roadmaps/2022-development-roadmap/#delta-sharing","title":"Delta Sharing","text":"<p>One of the most common requests the RTDIP team receive is how to share RTDIP data with third parties. Delta Sharing is an open source capability that allows sharing of Delta data via a managed service that provides authentication, connection management and supply of Delta data to third parties. </p> <p>We aim to see what more we can do in this space to make sharing of data simpler from an architecture perspective while still meeting all the security requirements around sharing of data with third parties.</p>"},{"location":"roadmap/yearly-roadmaps/2023-development-roadmap/","title":"RTDIP Development Roadmap in 2023","text":"<p>Shell\u2019s internally developed Shell Sensor Intelligence Platform was open sourced to LF Energy in 2022 and rebranded to Real Time Data Ingestion Platform. This was a major milestone for that project and has opened a number of strategic doors and expanded how the development roadmap is determined when considering it in an open source context. Below is the roadmap for 2023</p> <p>We welcome and encourage projects, developers, users and applications to contribute to our roadmaps. Please reach out to the RTDIP Technical Steering Committee team if you have ideas or suggestions on what we could innovate on in this space. We will continue to evolve these development items all through 2022 so please come back to this article throughout the year to see any new items that may be brought into scope.</p>"},{"location":"roadmap/yearly-roadmaps/2023-development-roadmap/#tldr","title":"TL;DR","text":"Item Description Estimated Quarter for Delivery IEC CIM RTDIP will target the IEC CIM data model and focus specifically on modelling data related to scada/process time series and sensor metadata Q1 2023 Ingestion Framework Develop a Data Ingestion framework, incorporated into the RTDIP SDK that supports building, testing and deploying streaming data pipelines into an RTDIP environment. The goal of this framework is to eventually facilitate low-code/no-code setup of streaming data pipelines that can be easily deployed by RTDIP users. In 2023, the first iteration of the framework will be designed and developed Q1-Q2 2023 Meter Data Design and build ingestion pipelines for meter data that meets the IEC CIM and Data Ingestion Framework items above Q2-Q4 2023 LF Energy Integration Increase integration of RTDIP with other LF Energy components. RTDIP will reach out to LF Energy projects to understand which data and integration opportunities exist and support building the necessary components to support these integrations Q1-Q4 2023 Microgrid Support Deployment of RTDIP and related LF Energy components to a pilot microgrid site Q1-Q4 2023 Multicloud Build certain existing RTDIP Azure capabilities on AWS. Enables RTDIP in the clouds aligned with the business but also to ensure multicloud is cost effective and that products in the architecture work in Cloud Environments Q1-Q3 2023 Adoption Adapt RTDIP to cater for a wider audience, particularly to increase RTDIP adoption in the energy sector. A simpler deployment process, facilitation of more environments RTDIP can setup in and catering for more orchestration engines are all initiatives to increase adoption in 2023 Q1-Q4 2023"},{"location":"roadmap/yearly-roadmaps/2023-development-roadmap/#iec-cim","title":"IEC CIM","text":"<p>RTDIP will define IEC CIM compliant data models that will be built in the RTDIP SDK for time series data and metering data. This will ensure that RTDIP can support systems that require data modelled to the IEC CIM standard. The data models will be defined and available within the RTDIP SDK so that it can be used in the ingestion and query layers of the RTDIP SDK.</p>"},{"location":"roadmap/yearly-roadmaps/2023-development-roadmap/#ingestion-framework","title":"Ingestion Framework","text":"<p>An ingestion framework will be incorporated into the RTDIP SDK that will facilitate the following: - Simple reuse of components - Standardisation of development of ingestion source, transformation and destination components - A common deployment pattern that supports the most popular spark environment setups - Targeting of popular orchestration engines and targeting Databricks Worflows and Airflow. Dagster/Flyte to be considered.</p> <p>The ingestion framework will be designed and developed in Q1-Q3 2023 and will be used to build the ingestion pipelines for meter data in Q2-Q4 2023.</p>"},{"location":"roadmap/yearly-roadmaps/2023-development-roadmap/#meter-data","title":"Meter Data","text":"<p>RTDIP will be extended to incorporate meter data as part of the platform. The ingestion framework above will be used to build ingestion pipelines for meter data. The pipelines will be designed to ingest meter data from a variety of meter data sources and will be designed to support the IEC CIM data model.</p>"},{"location":"roadmap/yearly-roadmaps/2023-development-roadmap/#lf-energy-integration","title":"LF Energy Integration","text":"<p>Increase integration of RTDIP with other LF Energy components. Data is at the centre of every component in an energy system and a data platform can play a major role in supporting integration across different components. RTDIP will reach out to LF Energy projects to understand which data and integration opportunities exist and support building the necessary components to support this integration. The scope of this will be more clearly defined after connecting with the relevant projects of LF Energy to define opportunities for integration.</p>"},{"location":"roadmap/yearly-roadmaps/2023-development-roadmap/#microgrid-support","title":"Microgrid Support","text":"<p>Deployment of RTDIP and related LF Energy components to a Shell pilot microgrid site. This exercise will help to identify how RTDIP supports an energy system better and demonstrate the integration of RTDIP with other LF Energy components in a real world environment.</p>"},{"location":"roadmap/yearly-roadmaps/2023-development-roadmap/#adoption","title":"Adoption","text":"<p>Adapt RTDIP to cater for a wider audience, particularly to increase RTDIP adoption in the energy sector. To do this, the roadmap for 2023 will focus on:</p> <ul> <li>Making it simpler for deploying and incorporating RTDIP into existing technology stacks</li> <li>Provide options for deploying RTDIP ingestion jobs in different Spark Environments. For 2023, this will be Databricks(Jobs with stretch target of DLT) and self-managed Spark Clusters setup using Open Source Spark</li> <li>Support deployment of RTDIP data pipelines to different workflow managers. For 2023, this will include Airflow and Databricks Workflows, with a stretch target of including Dagster</li> <li>Support more clouds. For 2023, the target is AWS.</li> </ul>"},{"location":"roadmap/yearly-roadmaps/2023-development-roadmap/#multicloud","title":"Multicloud","text":"<p>The target for 2023 is to enable RTDIP components to run in AWS. This includes:</p> <ul> <li>RTDIP API support on AWS Lambda</li> <li>Ingestion support on AWS Databricks</li> <li>BI support on AWS Databricks SQL</li> </ul>"},{"location":"sdk/overview/","title":"Overview","text":""},{"location":"sdk/overview/#what-is-the-rtdip-sdk","title":"What is the RTDIP SDK?","text":"<p>\u200b\u200bReal Time Data Ingestion Platform (RTDIP) SDK is a python software development kit built to provide users, data scientists and developers with the ability to interact with components of the Real Time Data Ingestion Platform, including:</p> <ul> <li>Building, Executing and Deploying Ingestion Pipelines</li> <li>Execution of queries on RTDIP data</li> <li>Authentication to securely interact with environments and data</li> </ul>"},{"location":"sdk/overview/#installation","title":"Installation","text":"<p>To get started with the RTDIP SDK, follow these installation instructions.</p>"},{"location":"sdk/authentication/azure/","title":"Azure Active Directory","text":"<p>The RTDIP SDK includes several Azure AD authentication methods to cater to the preference of the user:</p> <ul> <li>Default Authentication - For authenticating users with Azure AD using the azure-identity package. Note the order that Default Authentication uses to sign in a user and how it does it in this documentation. From experience, the Visual Studio Code login is the easiest to setup, but the azure cli option is the most reliable option. This page is useful for troubleshooting issues with this option to authenticate.</li> </ul> <p>Visual Studio Code</p> <p>As per the guidance in the documentation - To authenticate in Visual Studio Code, ensure version 0.9.11 or earlier of the Azure Account extension is installed. To track progress toward supporting newer extension versions, see this GitHub issue. Once installed, open the Command Palette and run the Azure: Sign In command.</p> <ul> <li> <p>Certificate Authentication - Service Principal authentication using a certificate</p> </li> <li> <p>Client Secret Authentication - Service Principal authentication using a client id and client secret</p> </li> </ul>"},{"location":"sdk/authentication/azure/#authentication","title":"Authentication","text":"<p>The following section describes authentication using Azure Active Directory..</p> <p>Note<p>If you are using the SDK directly in Databricks please note that DefaultAuth will not work.</p> </p> <p>1. Import rtdip-sdk authentication methods with the following:</p> <pre><code>from rtdip_sdk.authentication import authenticate as auth\n</code></pre> <p>2. Use any of the following authentication methods. Replace tenant_id , client_id, certificate_path or client_secret with your own details.</p> Default AuthenticationCertificate AuthenticationClient Secret Authentication <pre><code>DefaultAzureCredential = auth.DefaultAuth().authenticate()\n</code></pre> <pre><code>CertificateCredential = auth.CertificateAuth(tenant_id, client_id, certificate_path).authenticate()\n</code></pre> <pre><code>ClientSecretCredential = auth.ClientSecretAuth(tenant_id, client_id, client_secret).authenticate()\n</code></pre> <p>3. The methods above will return back a Client Object. The following example will show you how to retrieve the access_token from a credential object. The access token will be used in later steps to connect to RTDIP via the three options (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect).</p>"},{"location":"sdk/authentication/azure/#tokens","title":"Tokens","text":"<p>Once authenticated, it is possible to retrieve tokens for specific Azure Resources by providing scopes when retrieving tokens. Please see below for examples of how to retrieve tokens for Azure resources regularly used in RTDIP.</p> Databricks <pre><code>access_token = DefaultAzureCredential.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\n</code></pre> <p>Note</p> <p>RTDIP are continuously adding more to this list so check back regularly!</p>"},{"location":"sdk/authentication/databricks/","title":"Databricks","text":"<p>Databricks supports authentication using Personal Access Tokens (PAT) and information about this authentication method is available here.</p>"},{"location":"sdk/authentication/databricks/#authentication","title":"Authentication","text":"<p>To generate a Databricks PAT Token, follow this guide and ensure that the token is stored securely and is never used directly in code.</p> <p>Your Databricks PAT Token can be used in the RTDIP SDK to authenticate with any Databricks Workspace or Databricks SQL Warehouse and simply provided in the <code>access_token</code> fields where tokens are required in the RTDIP SDK.</p>"},{"location":"sdk/authentication/databricks/#example","title":"Example","text":"<p>Below is an example of using a Databricks PAT Token for authenticating with a Databricks SQL Warehouse.</p> <pre><code>from rtdip_sdk.connectors import DatabricksSQLConnection\n\nserver_hostname = \"server_hostname\"\nhttp_path = \"http_path\"\naccess_token = \"dbapi.......\"\n\nconnection = DatabricksSQLConnection(server_hostname, http_path, access_token)\n</code></pre> <p>Replace server_hostname, http_path with your own information and specify your Databricks PAT token for the access_token. </p>"},{"location":"sdk/code-reference/authentication/azure/","title":"Authentication","text":""},{"location":"sdk/code-reference/authentication/azure/#src.sdk.python.rtdip_sdk.authentication.azure.CertificateAuth","title":"<code>CertificateAuth</code>","text":"<p>Enables authentication to Azure Active Directory using a certificate that was generated for an App Registration.</p> <p>The certificate must have an RSA private key, because this credential signs assertions using RS256</p> <p>Parameters:</p> Name Type Description Default <code>tenant_id</code> <code>str</code> <p>The Azure Active Directory tenant (directory) Id of the service principal.</p> required <code>client_id</code> <code>str</code> <p>The client (application) ID of the service principal</p> required <code>certificate_path</code> <code>str</code> <p>Optional path to a certificate file in PEM or PKCS12 format, including the private key. If not provided, certificate_data is required.</p> <code>None</code> Source code in <code>src/sdk/python/rtdip_sdk/authentication/azure.py</code> <pre><code>class CertificateAuth:\n\"\"\"\n    Enables authentication to Azure Active Directory using a certificate that was generated for an App Registration.\n\n    The certificate must have an RSA private key, because this credential signs assertions using RS256\n\n    Args:\n        tenant_id: The Azure Active Directory tenant (directory) Id of the service principal.\n        client_id: The client (application) ID of the service principal\n        certificate_path: Optional path to a certificate file in PEM or PKCS12 format, including the private key. If not provided, certificate_data is required.\n    \"\"\"\n\n    def __init__(\n        self, tenant_id: str, client_id: str, certificate_path: str = None\n    ) -&gt; None:\n        self.tenant_id = tenant_id\n        self.client_id = client_id\n        self.certificate_path = certificate_path\n\n    def authenticate(self) -&gt; CertificateCredential:\n\"\"\"\n        Authenticates as a service principal using a certificate.\n\n        Returns:\n            CertificateCredential: Authenticates as a service principal using a certificate.\n        \"\"\"\n        try:\n            access_token = CertificateCredential(\n                self.tenant_id, self.client_id, self.certificate_path\n            )\n            return access_token\n        except Exception as e:\n            logging.exception(\"error returning certificate credential\")\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/authentication/azure/#src.sdk.python.rtdip_sdk.authentication.azure.CertificateAuth.authenticate","title":"<code>authenticate()</code>","text":"<p>Authenticates as a service principal using a certificate.</p> <p>Returns:</p> Name Type Description <code>CertificateCredential</code> <code>CertificateCredential</code> <p>Authenticates as a service principal using a certificate.</p> Source code in <code>src/sdk/python/rtdip_sdk/authentication/azure.py</code> <pre><code>def authenticate(self) -&gt; CertificateCredential:\n\"\"\"\n    Authenticates as a service principal using a certificate.\n\n    Returns:\n        CertificateCredential: Authenticates as a service principal using a certificate.\n    \"\"\"\n    try:\n        access_token = CertificateCredential(\n            self.tenant_id, self.client_id, self.certificate_path\n        )\n        return access_token\n    except Exception as e:\n        logging.exception(\"error returning certificate credential\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/authentication/azure/#src.sdk.python.rtdip_sdk.authentication.azure.ClientSecretAuth","title":"<code>ClientSecretAuth</code>","text":"<p>Enables authentication to Azure Active Directory using a client secret that was generated for an App Registration.</p> <p>Parameters:</p> Name Type Description Default <code>tenant_id</code> <code>str</code> <p>The Azure Active Directory tenant (directory) Id of the service principal.</p> required <code>client_id</code> <code>str</code> <p>The client (application) ID of the service principal</p> required <code>client_secret</code> <code>str</code> <p>A client secret that was generated for the App Registration used to authenticate the client.</p> required Source code in <code>src/sdk/python/rtdip_sdk/authentication/azure.py</code> <pre><code>class ClientSecretAuth:\n\"\"\"\n    Enables authentication to Azure Active Directory using a client secret that was generated for an App Registration.\n\n    Args:\n        tenant_id: The Azure Active Directory tenant (directory) Id of the service principal.\n        client_id: The client (application) ID of the service principal\n        client_secret: A client secret that was generated for the App Registration used to authenticate the client.\n    \"\"\"\n\n    def __init__(self, tenant_id: str, client_id: str, client_secret: str) -&gt; None:\n        self.tenant_id = tenant_id\n        self.client_id = client_id\n        self.client_secret = client_secret\n\n    def authenticate(self) -&gt; ClientSecretCredential:\n\"\"\"\n        Authenticates as a service principal using a client secret.\n\n        Returns:\n            ClientSecretCredential: Authenticates as a service principal using a client secret.\n        \"\"\"\n        try:\n            access_token = ClientSecretCredential(\n                self.tenant_id, self.client_id, self.client_secret\n            )\n            return access_token\n        except Exception as e:\n            logging.exception(\"error returning client secret credential\")\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/authentication/azure/#src.sdk.python.rtdip_sdk.authentication.azure.ClientSecretAuth.authenticate","title":"<code>authenticate()</code>","text":"<p>Authenticates as a service principal using a client secret.</p> <p>Returns:</p> Name Type Description <code>ClientSecretCredential</code> <code>ClientSecretCredential</code> <p>Authenticates as a service principal using a client secret.</p> Source code in <code>src/sdk/python/rtdip_sdk/authentication/azure.py</code> <pre><code>def authenticate(self) -&gt; ClientSecretCredential:\n\"\"\"\n    Authenticates as a service principal using a client secret.\n\n    Returns:\n        ClientSecretCredential: Authenticates as a service principal using a client secret.\n    \"\"\"\n    try:\n        access_token = ClientSecretCredential(\n            self.tenant_id, self.client_id, self.client_secret\n        )\n        return access_token\n    except Exception as e:\n        logging.exception(\"error returning client secret credential\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/authentication/azure/#src.sdk.python.rtdip_sdk.authentication.azure.DefaultAuth","title":"<code>DefaultAuth</code>","text":"<p>A default credential capable of handling most Azure SDK authentication scenarios.</p> <p>The identity it uses depends on the environment. When an access token is needed, it requests one using these identities in turn, stopping when one provides a token:</p> <p>1) A service principal configured by environment variables.</p> <p>2) An Azure managed identity.</p> <p>3) On Windows only: a user who has signed in with a Microsoft application, such as Visual Studio. If multiple identities are in the cache, then the value of the environment variable AZURE_USERNAME is used to select which identity to use.</p> <p>4) The user currently signed in to Visual Studio Code.</p> <p>5) The identity currently logged in to the Azure CLI.</p> <p>6) The identity currently logged in to Azure PowerShell.</p> <p>Parameters:</p> Name Type Description Default <code>exclude_cli_credential</code> <code>Optional</code> <p>Whether to exclude the Azure CLI from the credential. Defaults to False.</p> <code>False</code> <code>exclude_environment_credential</code> <code>Optional</code> <p>Whether to exclude a service principal configured by environment variables from the credential. Defaults to True.</p> <code>True</code> <code>exclude_managed_identity_credential</code> <code>Optional</code> <p>Whether to exclude managed identity from the credential. Defaults to True</p> <code>True</code> <code>exclude_powershell_credential</code> <code>Optional</code> <p>Whether to exclude Azure PowerShell. Defaults to False.</p> <code>False</code> <code>exclude_visual_studio_code_credential</code> <code>Optional</code> <p>Whether to exclude stored credential from VS Code. Defaults to False</p> <code>False</code> <code>exclude_shared_token_cache_credential</code> <code>Optional</code> <p>Whether to exclude the shared token cache. Defaults to False.</p> <code>False</code> <code>exclude_interactive_browser_credential</code> <code>Optional</code> <p>Whether to exclude interactive browser authentication (see InteractiveBrowserCredential). Defaults to False</p> <code>False</code> <code>logging_enable</code> <code>Optional</code> <p>Turn on or off logging. Defaults to False.</p> <code>False</code> Source code in <code>src/sdk/python/rtdip_sdk/authentication/azure.py</code> <pre><code>class DefaultAuth:\n\"\"\"\n    A default credential capable of handling most Azure SDK authentication scenarios.\n\n    The identity it uses depends on the environment. When an access token is needed, it requests one using these identities in turn, stopping when one provides a token:\n\n    1) A service principal configured by environment variables.\n\n    2) An Azure managed identity.\n\n    3) On Windows only: a user who has signed in with a Microsoft application, such as Visual Studio. If multiple identities are in the cache, then the value of the environment variable AZURE_USERNAME is used to select which identity to use.\n\n    4) The user currently signed in to Visual Studio Code.\n\n    5) The identity currently logged in to the Azure CLI.\n\n    6) The identity currently logged in to Azure PowerShell.\n\n    Args:\n        exclude_cli_credential (Optional): Whether to exclude the Azure CLI from the credential. Defaults to False.\n        exclude_environment_credential (Optional): Whether to exclude a service principal configured by environment variables from the credential. Defaults to True.\n        exclude_managed_identity_credential (Optional): Whether to exclude managed identity from the credential. Defaults to True\n        exclude_powershell_credential (Optional): Whether to exclude Azure PowerShell. Defaults to False.\n        exclude_visual_studio_code_credential (Optional): Whether to exclude stored credential from VS Code. Defaults to False\n        exclude_shared_token_cache_credential (Optional): Whether to exclude the shared token cache. Defaults to False.\n        exclude_interactive_browser_credential (Optional): Whether to exclude interactive browser authentication (see InteractiveBrowserCredential). Defaults to False\n        logging_enable (Optional): Turn on or off logging. Defaults to False.\n    \"\"\"\n\n    def __init__(\n        self,\n        exclude_cli_credential=False,\n        exclude_environment_credential=True,\n        exclude_managed_identity_credential=True,\n        exclude_powershell_credential=False,\n        exclude_visual_studio_code_credential=False,\n        exclude_shared_token_cache_credential=False,\n        exclude_interactive_browser_credential=False,\n        logging_enable=False,\n    ) -&gt; None:\n        self.exclude_cli_credential = exclude_cli_credential\n        self.exclude_environment_credential = exclude_environment_credential\n        self.exclude_managed_identity_credential = exclude_managed_identity_credential\n        self.exclude_powershell_credential = exclude_powershell_credential\n        self.exclude_visual_studio_code_credential = (\n            exclude_visual_studio_code_credential\n        )\n        self.exclude_shared_token_cache_credential = (\n            exclude_shared_token_cache_credential\n        )\n        self.exclude_interactive_browser_credential = (\n            exclude_interactive_browser_credential\n        )\n        self.logging_enable = logging_enable\n\n    def authenticate(self) -&gt; DefaultAzureCredential:\n\"\"\"\n        A default credential capable of handling most Azure SDK authentication scenarios.\n\n        Returns:\n            DefaultAzureCredential: A default credential capable of handling most Azure SDK authentication scenarios.\n        \"\"\"\n        try:\n            access_token = DefaultAzureCredential(\n                exclude_cli_credential=self.exclude_cli_credential,\n                exclude_environment_credential=self.exclude_environment_credential,\n                exclude_managed_identity_credential=self.exclude_managed_identity_credential,\n                exclude_powershell_credential=self.exclude_powershell_credential,\n                exclude_visual_studio_code_credential=self.exclude_visual_studio_code_credential,\n                exclude_shared_token_cache_credential=self.exclude_shared_token_cache_credential,\n                exclude_interactive_browser_credential=self.exclude_interactive_browser_credential,\n                logging_enable=self.logging_enable,\n            )\n            return access_token\n        except Exception as e:\n            logging.exception(\"error returning default azure credential\")\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/authentication/azure/#src.sdk.python.rtdip_sdk.authentication.azure.DefaultAuth.authenticate","title":"<code>authenticate()</code>","text":"<p>A default credential capable of handling most Azure SDK authentication scenarios.</p> <p>Returns:</p> Name Type Description <code>DefaultAzureCredential</code> <code>DefaultAzureCredential</code> <p>A default credential capable of handling most Azure SDK authentication scenarios.</p> Source code in <code>src/sdk/python/rtdip_sdk/authentication/azure.py</code> <pre><code>def authenticate(self) -&gt; DefaultAzureCredential:\n\"\"\"\n    A default credential capable of handling most Azure SDK authentication scenarios.\n\n    Returns:\n        DefaultAzureCredential: A default credential capable of handling most Azure SDK authentication scenarios.\n    \"\"\"\n    try:\n        access_token = DefaultAzureCredential(\n            exclude_cli_credential=self.exclude_cli_credential,\n            exclude_environment_credential=self.exclude_environment_credential,\n            exclude_managed_identity_credential=self.exclude_managed_identity_credential,\n            exclude_powershell_credential=self.exclude_powershell_credential,\n            exclude_visual_studio_code_credential=self.exclude_visual_studio_code_credential,\n            exclude_shared_token_cache_credential=self.exclude_shared_token_cache_credential,\n            exclude_interactive_browser_credential=self.exclude_interactive_browser_credential,\n            logging_enable=self.logging_enable,\n        )\n        return access_token\n    except Exception as e:\n        logging.exception(\"error returning default azure credential\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/integrations/openstef/database/","title":"OpenSTEF Integration with RTDIP","text":""},{"location":"sdk/code-reference/integrations/openstef/database/#src.sdk.python.rtdip_sdk.integrations.openstef.database.DataBase","title":"<code>DataBase</code>","text":"<p>Provides a high-level interface to various data sources.</p> <p>All user/client code should use this class to get or write data. Under the hood this class uses various services to interfact with its datasource.</p> <p>Warning</p> <p>This is a singleton class. When calling multiple times with a config argument no new configuration will be applied.</p>"},{"location":"sdk/code-reference/integrations/openstef/database/#src.sdk.python.rtdip_sdk.integrations.openstef.database.DataBase--example","title":"Example","text":"<pre><code>from typing import Union\nfrom pydantic.v1 import BaseSettings\nfrom src.sdk.python.rtdip_sdk.authentication.azure import DefaultAuth\nfrom src.sdk.python.rtdip_sdk.integrations.openstef.database import DataBase\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\n\nclass ConfigSettings(BaseSettings):\n    api_username: str = \"None\"\n    api_password: str = \"None\"\n    api_admin_username: str = \"None\"\n    api_admin_password: str = \"None\"\n    api_url: str = \"None\"\n    pcdm_host: str = \"{DATABRICKS-SERVER-HOSTNAME}\"\n    pcdm_token: str = token\n    pcdm_port: int = 443\n    pcdm_http_path: str = \"{SQL-WAREHOUSE-HTTP-PATH}\"\n    pcdm_catalog: str = \"{YOUR-CATALOG-NAME}\"\n    pcdm_schema: str = \"{YOUR-SCHEMA-NAME}\"\n    db_host: str = \"{DATABRICKS-SERVER-HOSTNAME}\"\n    db_token: str = token\n    db_port: int = 443\n    db_http_path: str = \"{SQL-WAREHOUSE-HTTP-PATH}\"\n    db_catalog: str = \"{YOUR-CATALOG-NAME}\"\n    db_schema: str = \"{YOUR-SCHEMA-NAME}\"\n    proxies: Union[dict[str, str], None] = None\n\n\nconfig = ConfigSettings()\n\ndb = DataBase(config)\n\nweather_data = db.get_weather_data(\n                location=\"Deelen\",\n                weatherparams=[\"pressure\", \"temp\"],\n                datetime_start=datetime(2023, 8, 29),\n                datetime_end=datetime(2023, 8, 30),\n                source=\"harm_arome\",\n                )\n\nprint(weather_data)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>BaseSettings</code> <p>Configuration object. See Attributes table below.</p> required <p>Attributes:</p> Name Type Description <code>api_username</code> <code>str</code> <p>API username</p> <code>api_password</code> <code>str</code> <p>API password</p> <code>api_admin_username</code> <code>str</code> <p>API admin username</p> <code>api_admin_password</code> <code>str</code> <p>API admin password</p> <code>api_url</code> <code>str</code> <p>API url</p> <code>pcdm_host</code> <code>str</code> <p>Databricks hostname for Time Series data</p> <code>pcdm_token</code> <code>str</code> <p>Databricks token</p> <code>pcdm_port</code> <code>int</code> <p>Databricks port</p> <code>pcdm_catalog</code> <code>str</code> <p>Databricks catalog</p> <code>pcdm_schema</code> <code>str</code> <p>Databricks schema</p> <code>pcdm_http_path</code> <code>str</code> <p>SQL warehouse http path</p> <code>db_host</code> <code>str</code> <p>Databricks hostname for Prediction Job information and measurements data</p> <code>db_token</code> <code>str</code> <p>Databricks token</p> <code>db_port</code> <code>int</code> <p>Databricks port</p> <code>db_catalog</code> <code>str</code> <p>Databricks catalog</p> <code>db_schema</code> <code>str</code> <p>Databricks schema</p> <code>db_http_path</code> <code>str</code> <p>SQL warehouse http path</p> <code>proxies</code> <code>Union[dict[str, str], None]</code> <p>Proxies</p> Source code in <code>src/sdk/python/rtdip_sdk/integrations/openstef/database.py</code> <pre><code>class DataBase(metaclass=Singleton):\n\"\"\"\n    Provides a high-level interface to various data sources.\n\n    All user/client code should use this class to get or write data. Under the hood this class uses various services to interfact with its datasource.\n\n    !!! note \"Warning\"\n        This is a singleton class. When calling multiple times with a config argument no new configuration will be applied.\n\n     Example\n    --------\n    ```python\n    from typing import Union\n    from pydantic.v1 import BaseSettings\n    from src.sdk.python.rtdip_sdk.authentication.azure import DefaultAuth\n    from src.sdk.python.rtdip_sdk.integrations.openstef.database import DataBase\n\n    auth = DefaultAuth().authenticate()\n    token = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\n\n    class ConfigSettings(BaseSettings):\n        api_username: str = \"None\"\n        api_password: str = \"None\"\n        api_admin_username: str = \"None\"\n        api_admin_password: str = \"None\"\n        api_url: str = \"None\"\n        pcdm_host: str = \"{DATABRICKS-SERVER-HOSTNAME}\"\n        pcdm_token: str = token\n        pcdm_port: int = 443\n        pcdm_http_path: str = \"{SQL-WAREHOUSE-HTTP-PATH}\"\n        pcdm_catalog: str = \"{YOUR-CATALOG-NAME}\"\n        pcdm_schema: str = \"{YOUR-SCHEMA-NAME}\"\n        db_host: str = \"{DATABRICKS-SERVER-HOSTNAME}\"\n        db_token: str = token\n        db_port: int = 443\n        db_http_path: str = \"{SQL-WAREHOUSE-HTTP-PATH}\"\n        db_catalog: str = \"{YOUR-CATALOG-NAME}\"\n        db_schema: str = \"{YOUR-SCHEMA-NAME}\"\n        proxies: Union[dict[str, str], None] = None\n\n\n    config = ConfigSettings()\n\n    db = DataBase(config)\n\n    weather_data = db.get_weather_data(\n                    location=\"Deelen\",\n                    weatherparams=[\"pressure\", \"temp\"],\n                    datetime_start=datetime(2023, 8, 29),\n                    datetime_end=datetime(2023, 8, 30),\n                    source=\"harm_arome\",\n                    )\n\n    print(weather_data)\n    ```\n\n    Args:\n        config: Configuration object. See Attributes table below.\n\n    Attributes:\n        api_username (str): API username\n        api_password (str): API password\n        api_admin_username (str): API admin username\n        api_admin_password (str): API admin password\n        api_url (str): API url\n        pcdm_host (str): Databricks hostname for Time Series data\n        pcdm_token (str): Databricks token\n        pcdm_port (int): Databricks port\n        pcdm_catalog (str): Databricks catalog\n        pcdm_schema (str): Databricks schema\n        pcdm_http_path (str): SQL warehouse http path\n        db_host (str): Databricks hostname for Prediction Job information and measurements data\n        db_token (str): Databricks token\n        db_port (int): Databricks port\n        db_catalog (str): Databricks catalog\n        db_schema (str): Databricks schema\n        db_http_path (str): SQL warehouse http path\n        proxies Union[dict[str, str], None]: Proxies\n    \"\"\"\n\n    _instance = None\n\n    # services\n    _write = Write()\n    _prediction_job = PredictionJobRetriever()\n    _weather = Weather()\n    _historic_cdb_data_service = Ems()\n    _predictor = Predictor()\n    _splitting = Splitting()\n    _predictions = Predictions()\n    _model_input = ModelInput()\n    _systems = Systems()\n\n    # write methods\n    write_weather_data = _write.write_weather_data\n    write_realised = _write.write_realised\n    write_realised_pvdata = _write.write_realised_pvdata\n    write_kpi = _write.write_kpi\n    write_forecast = _write.write_forecast\n    write_apx_market_data = _write.write_apx_market_data\n    write_sjv_load_profiles = _write.write_sjv_load_profiles\n    write_windturbine_powercurves = _write.write_windturbine_powercurves\n    write_energy_splitting_coefficients = _write.write_energy_splitting_coefficients\n\n    # prediction job methods\n    get_prediction_jobs_solar = _prediction_job.get_prediction_jobs_solar\n    get_prediction_jobs_wind = _prediction_job.get_prediction_jobs_wind\n    get_prediction_jobs = _prediction_job.get_prediction_jobs\n    get_prediction_job = _prediction_job.get_prediction_job\n    get_pids_for_api_key = _prediction_job.get_pids_for_api_key\n    get_pids_for_api_keys = _prediction_job.get_pids_for_api_keys\n    get_ean_for_pid = _prediction_job.get_ean_for_pid\n    get_eans_for_pids = _prediction_job.get_eans_for_pids\n\n    # weather methods\n    get_weather_forecast_locations = _weather.get_weather_forecast_locations\n    get_weather_data = _weather.get_weather_data\n    get_datetime_last_stored_knmi_weatherdata = (\n        _weather.get_datetime_last_stored_knmi_weatherdata\n    )\n    # predictor methods\n    get_predictors = _predictor.get_predictors\n    get_electricity_price = _predictor.get_electricity_price\n    get_load_profiles = _predictor.get_load_profiles\n    # historic cdb data service\n    get_load_sid = _historic_cdb_data_service.get_load_sid\n    get_load_pid = _historic_cdb_data_service.get_load_pid\n\n    # splitting methods\n    get_wind_ref = _splitting.get_wind_ref\n    get_energy_split_coefs = _splitting.get_energy_split_coefs\n    get_input_energy_splitting = _splitting.get_input_energy_splitting\n    # predictions methods\n    get_predicted_load = _predictions.get_predicted_load\n    get_predicted_load_tahead = _predictions.get_predicted_load_tahead\n    get_prediction_including_components = (\n        _predictions.get_prediction_including_components\n    )\n    get_forecast_quality = _predictions.get_forecast_quality\n    # model input methods\n    get_model_input = _model_input.get_model_input\n    get_wind_input = _model_input.get_wind_input\n    get_power_curve = _model_input.get_power_curve\n    get_solar_input = _model_input.get_solar_input\n    # systems methods\n    get_systems_near_location = _systems.get_systems_near_location\n    get_systems_by_pid = _systems.get_systems_by_pid\n    get_pv_systems_with_incorrect_location = (\n        _systems.get_pv_systems_with_incorrect_location\n    )\n    get_random_pv_systems = _systems.get_random_pv_systems\n    get_api_key_for_system = _systems.get_api_key_for_system\n    get_api_keys_for_systems = _systems.get_api_keys_for_systems\n\n    def __init__(self, config: BaseSettings):\n        self._datainterface = _DataInterface(config)\n        # Ktp api\n        self.ktp_api = self._datainterface.ktp_api\n\n        DataBase._instance = self\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/","title":"Json","text":""},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.PipelineJobFromJsonConverter","title":"<code>PipelineJobFromJsonConverter</code>","text":"<p>             Bases: <code>ConverterInterface</code></p> <p>Converts a json string into a Pipeline Job.</p>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.PipelineJobFromJsonConverter--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.secrets import PipelineJobFromJsonConverter\n\nconvert_json_string_to_pipline_job = PipelineJobFromJsonConverter(\n    pipeline_json = \"{JSON-STRING}\"\n)\n\nconvert_json_string_to_pipline_job.convert()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>pipeline_json</code> <code>str</code> <p>Json representing PipelineJob information, including tasks and related steps</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/converters/pipeline_job_json.py</code> <pre><code>class PipelineJobFromJsonConverter(ConverterInterface):\n\"\"\"\n    Converts a json string into a Pipeline Job.\n\n    Example\n    -------\n    ```python\n    from rtdip_sdk.pipelines.secrets import PipelineJobFromJsonConverter\n\n    convert_json_string_to_pipline_job = PipelineJobFromJsonConverter(\n        pipeline_json = \"{JSON-STRING}\"\n    )\n\n    convert_json_string_to_pipline_job.convert()\n    ```\n\n    Parameters:\n        pipeline_json (str): Json representing PipelineJob information, including tasks and related steps\n    \"\"\"\n\n    pipeline_json: str\n\n    def __init__(self, pipeline_json: str):\n        self.pipeline_json = pipeline_json\n\n    def _try_convert_to_pipeline_secret(self, value):\n        try:\n            if \"pipeline_secret\" in value:\n                value[\"pipeline_secret\"][\"type\"] = getattr(\n                    sys.modules[__name__], value[\"pipeline_secret\"][\"type\"]\n                )\n            return PipelineSecret.parse_obj(value[\"pipeline_secret\"])\n        except:  # NOSONAR\n            return value\n\n    def convert(self) -&gt; PipelineJob:\n\"\"\"\n        Converts a json string to a Pipeline Job\n        \"\"\"\n        pipeline_job_dict = json.loads(self.pipeline_json)\n\n        # convert string component to class\n        for task in pipeline_job_dict[\"task_list\"]:\n            for step in task[\"step_list\"]:\n                step[\"component\"] = getattr(sys.modules[__name__], step[\"component\"])\n                for param_key, param_value in step[\"component_parameters\"].items():\n                    step[\"component_parameters\"][\n                        param_key\n                    ] = self._try_convert_to_pipeline_secret(param_value)\n                    if not isinstance(\n                        step[\"component_parameters\"][param_key], PipelineSecret\n                    ) and isinstance(param_value, dict):\n                        for key, value in param_value.items():\n                            step[\"component_parameters\"][param_key][\n                                key\n                            ] = self._try_convert_to_pipeline_secret(value)\n\n        return PipelineJob(**pipeline_job_dict)\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.PipelineJobFromJsonConverter.convert","title":"<code>convert()</code>","text":"<p>Converts a json string to a Pipeline Job</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/converters/pipeline_job_json.py</code> <pre><code>def convert(self) -&gt; PipelineJob:\n\"\"\"\n    Converts a json string to a Pipeline Job\n    \"\"\"\n    pipeline_job_dict = json.loads(self.pipeline_json)\n\n    # convert string component to class\n    for task in pipeline_job_dict[\"task_list\"]:\n        for step in task[\"step_list\"]:\n            step[\"component\"] = getattr(sys.modules[__name__], step[\"component\"])\n            for param_key, param_value in step[\"component_parameters\"].items():\n                step[\"component_parameters\"][\n                    param_key\n                ] = self._try_convert_to_pipeline_secret(param_value)\n                if not isinstance(\n                    step[\"component_parameters\"][param_key], PipelineSecret\n                ) and isinstance(param_value, dict):\n                    for key, value in param_value.items():\n                        step[\"component_parameters\"][param_key][\n                            key\n                        ] = self._try_convert_to_pipeline_secret(value)\n\n    return PipelineJob(**pipeline_job_dict)\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.PipelineJobToJsonConverter","title":"<code>PipelineJobToJsonConverter</code>","text":"<p>             Bases: <code>ConverterInterface</code></p> <p>Converts a Pipeline Job into a json string.</p>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.PipelineJobToJsonConverter--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.secrets import PipelineJobToJsonConverter\n\nconvert_pipeline_job_to_json_string = PipelineJobFromJsonConverter(\n    pipeline_json = PipelineJob\n)\n\nconvert_pipeline_job_to_json_string.convert()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>pipeline_job</code> <code>PipelineJob</code> <p>A Pipeline Job consisting of tasks and steps</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/converters/pipeline_job_json.py</code> <pre><code>class PipelineJobToJsonConverter(ConverterInterface):\n\"\"\"\n    Converts a Pipeline Job into a json string.\n\n    Example\n    -------\n    ```python\n    from rtdip_sdk.pipelines.secrets import PipelineJobToJsonConverter\n\n    convert_pipeline_job_to_json_string = PipelineJobFromJsonConverter(\n        pipeline_json = PipelineJob\n    )\n\n    convert_pipeline_job_to_json_string.convert()\n    ```\n\n    Parameters:\n        pipeline_job (PipelineJob): A Pipeline Job consisting of tasks and steps\n    \"\"\"\n\n    pipeline_job: PipelineJob\n\n    def __init__(self, pipeline_job: PipelineJob):\n        self.pipeline_job = pipeline_job\n\n    def convert(self):\n\"\"\"\n        Converts a Pipeline Job to a json string\n        \"\"\"\n        # required because pydantic does not use encoders in subclasses\n        for task in self.pipeline_job.task_list:\n            step_dict_list = []\n            for step in task.step_list:\n                step_dict_list.append(\n                    json.loads(step.json(models_as_dict=False, exclude_none=True))\n                )\n            task.step_list = step_dict_list\n\n        pipeline_job_json = self.pipeline_job.json(exclude_none=True)\n        return pipeline_job_json\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.PipelineJobToJsonConverter.convert","title":"<code>convert()</code>","text":"<p>Converts a Pipeline Job to a json string</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/converters/pipeline_job_json.py</code> <pre><code>def convert(self):\n\"\"\"\n    Converts a Pipeline Job to a json string\n    \"\"\"\n    # required because pydantic does not use encoders in subclasses\n    for task in self.pipeline_job.task_list:\n        step_dict_list = []\n        for step in task.step_list:\n            step_dict_list.append(\n                json.loads(step.json(models_as_dict=False, exclude_none=True))\n            )\n        task.step_list = step_dict_list\n\n    pipeline_job_json = self.pipeline_job.json(exclude_none=True)\n    return pipeline_job_json\n</code></pre>"},{"location":"sdk/code-reference/pipelines/deploy/databricks/","title":"Databricks","text":""},{"location":"sdk/code-reference/pipelines/deploy/databricks/#src.sdk.python.rtdip_sdk.pipelines.deploy.databricks.DatabricksSDKDeploy","title":"<code>DatabricksSDKDeploy</code>","text":"<p>             Bases: <code>DeployInterface</code></p> <p>Deploys an RTDIP Pipeline to Databricks Workflows leveraging the Databricks SDK.</p> <p>Deploying an RTDIP Pipeline to Databricks requires only a few additional pieces of information to ensure the RTDIP Pipeline Job can be run in Databricks. This information includes:</p> <ul> <li>Cluster: This can be defined a the Job or Task level and includes the size of the cluster to be used for the job</li> <li>Task: The cluster to be used to execute the task, as well as any task scheduling information, if required.</li> </ul> <p>All options available in the Databricks Jobs REST API v2.1 can be configured in the Databricks classes that have been defined in <code>rtdip_sdk.pipelines.deploy.models.databricks</code>, enabling full control of the configuration of the Databricks Workflow :</p> <ul> <li><code>CreateJob</code></li> <li><code>Task</code></li> </ul> <p>RTDIP Pipeline Components provide Databricks with all the required Python packages and JARs to execute each component and these will be setup on the Workflow automatically during the Databricks Workflow creation.</p> Example <p>This example assumes that a PipelineJob has already been defined by a variable called <code>pipeline_job</code></p> <pre><code>from rtdip_sdk.pipelines.deploy import DatabricksSDKDeploy, CreateJob, JobCluster, ClusterSpec, Task, NotebookTask, ComputeSpecKind, AutoScale, RuntimeEngine, DataSecurityMode\n\ncluster_list = []\ncluster_list.append(JobCluster(\n    job_cluster_key=\"test_cluster\",\n    new_cluster=ClusterSpec(\n        node_type_id=\"Standard_E4ds_v5\",\n        autoscale=AutoScale(min_workers=1, max_workers=3),\n        spark_version=\"13.2.x-scala2.12\",\n        data_security_mode=DataSecurityMode.SINGLE_USER,\n        runtime_engine=RuntimeEngine.PHOTON\n    )\n))\n\ntask_list = []\ntask_list.append(Task(\n    task_key=\"test_task\",\n    job_cluster_key=\"test_cluster\",\n    notebook_task=NotebookTask(\n        notebook_path=\"/path/to/pipeline/rtdip_pipeline.py\"\n    )\n))\n\njob = CreateJob(\n    name=\"test_job_rtdip\",\n    job_clusters=cluster_list,\n    tasks=task_list\n)\n\ndatabricks_job = DatabricksSDKDeploy(databricks_job=job, host=\"https://test.databricks.net\", token=\"test_token\")\n\n# Execute the deploy method to create a Workflow in the specified Databricks Environment\ndeploy_result = databricks_job.deploy()\n\n# If the job should be executed immediately, execute the `launch` method\nlaunch_result = databricks_job.launch()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>databricks_job</code> <code>DatabricksJob</code> <p>Contains Databricks specific information required for deploying the RTDIP Pipeline Job to Databricks, such as cluster and workflow scheduling information. This can be any field in the Databricks Jobs REST API v2.1</p> required <code>host</code> <code>str</code> <p>Databricks URL</p> required <code>token</code> <code>str</code> <p>Token for authenticating with Databricks such as a Databricks PAT Token or Azure AD Token</p> required <code>workspace_directory</code> <code>str</code> <p>Determines the folder location in the Databricks Workspace. Defaults to /rtdip</p> <code>'/rtdip'</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/deploy/databricks.py</code> <pre><code>class DatabricksSDKDeploy(DeployInterface):\n\"\"\"\n    Deploys an RTDIP Pipeline to Databricks Workflows leveraging the Databricks [SDK.](https://docs.databricks.com/dev-tools/sdk-python.html)\n\n    Deploying an RTDIP Pipeline to Databricks requires only a few additional pieces of information to ensure the RTDIP Pipeline Job can be run in Databricks. This information includes:\n\n    - **Cluster**: This can be defined a the Job or Task level and includes the size of the cluster to be used for the job\n    - **Task**: The cluster to be used to execute the task, as well as any task scheduling information, if required.\n\n    All options available in the [Databricks Jobs REST API v2.1](https://docs.databricks.com/dev-tools/api/latest/jobs.html) can be configured in the Databricks classes that have been defined in `rtdip_sdk.pipelines.deploy.models.databricks`, enabling full control of the configuration of the Databricks Workflow :\n\n    - `CreateJob`\n    - `Task`\n\n    RTDIP Pipeline Components provide Databricks with all the required Python packages and JARs to execute each component and these will be setup on the Workflow automatically during the Databricks Workflow creation.\n\n    Example:\n        This example assumes that a PipelineJob has already been defined by a variable called `pipeline_job`\n\n        ```python\n        from rtdip_sdk.pipelines.deploy import DatabricksSDKDeploy, CreateJob, JobCluster, ClusterSpec, Task, NotebookTask, ComputeSpecKind, AutoScale, RuntimeEngine, DataSecurityMode\n\n        cluster_list = []\n        cluster_list.append(JobCluster(\n            job_cluster_key=\"test_cluster\",\n            new_cluster=ClusterSpec(\n                node_type_id=\"Standard_E4ds_v5\",\n                autoscale=AutoScale(min_workers=1, max_workers=3),\n                spark_version=\"13.2.x-scala2.12\",\n                data_security_mode=DataSecurityMode.SINGLE_USER,\n                runtime_engine=RuntimeEngine.PHOTON\n            )\n        ))\n\n        task_list = []\n        task_list.append(Task(\n            task_key=\"test_task\",\n            job_cluster_key=\"test_cluster\",\n            notebook_task=NotebookTask(\n                notebook_path=\"/path/to/pipeline/rtdip_pipeline.py\"\n            )\n        ))\n\n        job = CreateJob(\n            name=\"test_job_rtdip\",\n            job_clusters=cluster_list,\n            tasks=task_list\n        )\n\n        databricks_job = DatabricksSDKDeploy(databricks_job=job, host=\"https://test.databricks.net\", token=\"test_token\")\n\n        # Execute the deploy method to create a Workflow in the specified Databricks Environment\n        deploy_result = databricks_job.deploy()\n\n        # If the job should be executed immediately, execute the `launch` method\n        launch_result = databricks_job.launch()\n        ```\n\n    Parameters:\n        databricks_job (DatabricksJob): Contains Databricks specific information required for deploying the RTDIP Pipeline Job to Databricks, such as cluster and workflow scheduling information. This can be any field in the [Databricks Jobs REST API v2.1](https://docs.databricks.com/dev-tools/api/latest/jobs.html)\n        host (str): Databricks URL\n        token (str): Token for authenticating with Databricks such as a Databricks PAT Token or Azure AD Token\n        workspace_directory (str, optional): Determines the folder location in the Databricks Workspace. Defaults to /rtdip\n    \"\"\"\n\n    def __init__(\n        self,\n        databricks_job: CreateJob,\n        host: str,\n        token: str,\n        workspace_directory: str = \"/rtdip\",\n    ) -&gt; None:\n        if databricks_job.name is None or databricks_job.name == \"\":\n            raise ValueError(\"databricks_job.name cannot be empty\")\n        self.databricks_job = databricks_job\n        self.host = host\n        self.token = token\n        self.workspace_directory = workspace_directory\n\n    def _convert_file_to_binary(self, path) -&gt; BytesIO:\n        with open(path, \"rb\") as f:\n            return BytesIO(f.read())\n\n    def _load_module(self, module_name, path):\n        spec = spec_from_file_location(module_name, path)\n        module = module_from_spec(spec)\n        spec.loader.exec_module(module)\n        sys.modules[module.__name__] = module\n        return module\n\n    def deploy(self) -&gt; Union[bool, ValueError]:\n\"\"\"\n        Deploys an RTDIP Pipeline Job to Databricks Workflows. The deployment is managed by the Job Name and therefore will overwrite any existing workflow in Databricks with the same name.\n        \"\"\"\n        # Add libraries to Databricks Job\n        workspace_client = WorkspaceClient(\n            host=self.host, token=self.token, auth_type=\"pat\"\n        )\n        for task in self.databricks_job.tasks:\n            if task.notebook_task is None and task.spark_python_task is None:\n                return ValueError(\n                    \"A Notebook or Spark Python Task must be populated for each task in the Databricks Job\"\n                )  # NOSONAR\n            if task.notebook_task is not None:\n                module = self._load_module(\n                    task.task_key + \"file_upload\", task.notebook_task.notebook_path\n                )\n                (task_libraries, spark_configuration) = PipelineComponentsGetUtility(\n                    module.__name__\n                ).execute()\n                workspace_client.workspace.mkdirs(path=self.workspace_directory)\n                path = \"{}/{}\".format(\n                    self.workspace_directory,\n                    Path(task.notebook_task.notebook_path).name,\n                )\n                workspace_client.workspace.upload(\n                    path=path,\n                    overwrite=True,\n                    content=self._convert_file_to_binary(\n                        task.notebook_task.notebook_path\n                    ),\n                )\n                task.notebook_task.notebook_path = path\n            else:\n                module = self._load_module(\n                    task.task_key + \"file_upload\", task.spark_python_task.python_file\n                )\n                (task_libraries, spark_configuration) = PipelineComponentsGetUtility(\n                    module\n                ).execute()\n                workspace_client.workspace.mkdirs(path=self.workspace_directory)\n                path = \"{}/{}\".format(\n                    self.workspace_directory,\n                    Path(task.spark_python_task.python_file).name,\n                )\n                workspace_client.workspace.upload(\n                    path=path,\n                    overwrite=True,\n                    content=self._convert_file_to_binary(\n                        task.spark_python_task.python_file\n                    ),\n                )\n                task.spark_python_task.python_file = path\n\n            task.libraries = []\n            for pypi_library in task_libraries.pypi_libraries:\n                task.libraries.append(\n                    Library(\n                        pypi=PythonPyPiLibrary(\n                            package=pypi_library.to_string(), repo=pypi_library.repo\n                        )\n                    )\n                )\n            for maven_library in task_libraries.maven_libraries:\n                if not maven_library.group_id in [\"io.delta\", \"org.apache.spark\"]:\n                    task.libraries.append(\n                        Library(\n                            maven=MavenLibrary(\n                                coordinates=maven_library.to_string(),\n                                repo=maven_library.repo,\n                            )\n                        )\n                    )\n            for wheel_library in task_libraries.pythonwheel_libraries:\n                task.libraries.append(Library(whl=wheel_library))\n\n            try:\n                rtdip_version = version(\"rtdip-sdk\")\n                task.libraries.append(\n                    Library(\n                        pypi=PythonPyPiLibrary(\n                            package=\"rtdip-sdk[pipelines]=={}\".format(rtdip_version)\n                        )\n                    )\n                )\n            except PackageNotFoundError as e:\n                task.libraries.append(\n                    Library(pypi=PythonPyPiLibrary(package=\"rtdip-sdk[pipelines]\"))\n                )\n\n            # Add Spark Configuration to Databricks Job\n            if (\n                task.new_cluster is None\n                and task.job_cluster_key is None\n                and task.compute_key is None\n            ):\n                return ValueError(\n                    \"A Cluster or Compute must be specified for each task in the Databricks Job\"\n                )\n            if task.new_cluster is not None:\n                if spark_configuration is not None:\n                    if task.new_cluster.spark_conf is None:\n                        task.new_cluster.spark_conf = {}\n                    task.new_cluster.spark_conf.update(spark_configuration)\n            elif task.job_cluster_key is not None:\n                for job_cluster in self.databricks_job.job_clusters:\n                    if job_cluster.job_cluster_key == task.job_cluster_key:\n                        if spark_configuration is not None:\n                            if job_cluster.new_cluster.spark_conf is None:\n                                job_cluster.new_cluster.spark_conf = {}\n                            job_cluster.new_cluster.spark_conf.update(\n                                spark_configuration\n                            )\n                        break\n            elif task.compute_key is not None:\n                for compute in self.databricks_job.compute:\n                    if compute.compute_key == task.compute_key:\n                        # TODO : Add spark config for compute. Does not seem to be currently available in the Databricks SDK # NOSONAR\n                        # compute.spark_conf.update(spark_configuration)\n                        break\n\n        # Create Databricks Job\n        job_found = False\n        for existing_job in workspace_client.jobs.list(name=self.databricks_job.name):\n            new_settings = JobSettings()\n            for key, value in self.databricks_job.__dict__.items():\n                if key in new_settings.__dict__:\n                    setattr(new_settings, key, value)\n            workspace_client.jobs.reset(\n                job_id=existing_job.job_id, new_settings=new_settings\n            )\n            job_found = True\n            break\n\n        if job_found == False:\n            workspace_client.jobs.create(**self.databricks_job.__dict__)\n\n        return True\n\n    def launch(self):\n\"\"\"\n        Launches an RTDIP Pipeline Job in Databricks Workflows. This will perform the equivalent of a `Run Now` in Databricks Workflows\n        \"\"\"\n        workspace_client = WorkspaceClient(\n            host=self.host, token=self.token, auth_type=\"pat\"\n        )\n        job_found = False\n        for existing_job in workspace_client.jobs.list(name=self.databricks_job.name):\n            workspace_client.jobs.run_now(job_id=existing_job.job_id)\n            job_found = True\n            break\n\n        if job_found == False:\n            raise ValueError(\"Job not found in Databricks Workflows\")\n\n        return True\n</code></pre>"},{"location":"sdk/code-reference/pipelines/deploy/databricks/#src.sdk.python.rtdip_sdk.pipelines.deploy.databricks.DatabricksSDKDeploy.deploy","title":"<code>deploy()</code>","text":"<p>Deploys an RTDIP Pipeline Job to Databricks Workflows. The deployment is managed by the Job Name and therefore will overwrite any existing workflow in Databricks with the same name.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/deploy/databricks.py</code> <pre><code>def deploy(self) -&gt; Union[bool, ValueError]:\n\"\"\"\n    Deploys an RTDIP Pipeline Job to Databricks Workflows. The deployment is managed by the Job Name and therefore will overwrite any existing workflow in Databricks with the same name.\n    \"\"\"\n    # Add libraries to Databricks Job\n    workspace_client = WorkspaceClient(\n        host=self.host, token=self.token, auth_type=\"pat\"\n    )\n    for task in self.databricks_job.tasks:\n        if task.notebook_task is None and task.spark_python_task is None:\n            return ValueError(\n                \"A Notebook or Spark Python Task must be populated for each task in the Databricks Job\"\n            )  # NOSONAR\n        if task.notebook_task is not None:\n            module = self._load_module(\n                task.task_key + \"file_upload\", task.notebook_task.notebook_path\n            )\n            (task_libraries, spark_configuration) = PipelineComponentsGetUtility(\n                module.__name__\n            ).execute()\n            workspace_client.workspace.mkdirs(path=self.workspace_directory)\n            path = \"{}/{}\".format(\n                self.workspace_directory,\n                Path(task.notebook_task.notebook_path).name,\n            )\n            workspace_client.workspace.upload(\n                path=path,\n                overwrite=True,\n                content=self._convert_file_to_binary(\n                    task.notebook_task.notebook_path\n                ),\n            )\n            task.notebook_task.notebook_path = path\n        else:\n            module = self._load_module(\n                task.task_key + \"file_upload\", task.spark_python_task.python_file\n            )\n            (task_libraries, spark_configuration) = PipelineComponentsGetUtility(\n                module\n            ).execute()\n            workspace_client.workspace.mkdirs(path=self.workspace_directory)\n            path = \"{}/{}\".format(\n                self.workspace_directory,\n                Path(task.spark_python_task.python_file).name,\n            )\n            workspace_client.workspace.upload(\n                path=path,\n                overwrite=True,\n                content=self._convert_file_to_binary(\n                    task.spark_python_task.python_file\n                ),\n            )\n            task.spark_python_task.python_file = path\n\n        task.libraries = []\n        for pypi_library in task_libraries.pypi_libraries:\n            task.libraries.append(\n                Library(\n                    pypi=PythonPyPiLibrary(\n                        package=pypi_library.to_string(), repo=pypi_library.repo\n                    )\n                )\n            )\n        for maven_library in task_libraries.maven_libraries:\n            if not maven_library.group_id in [\"io.delta\", \"org.apache.spark\"]:\n                task.libraries.append(\n                    Library(\n                        maven=MavenLibrary(\n                            coordinates=maven_library.to_string(),\n                            repo=maven_library.repo,\n                        )\n                    )\n                )\n        for wheel_library in task_libraries.pythonwheel_libraries:\n            task.libraries.append(Library(whl=wheel_library))\n\n        try:\n            rtdip_version = version(\"rtdip-sdk\")\n            task.libraries.append(\n                Library(\n                    pypi=PythonPyPiLibrary(\n                        package=\"rtdip-sdk[pipelines]=={}\".format(rtdip_version)\n                    )\n                )\n            )\n        except PackageNotFoundError as e:\n            task.libraries.append(\n                Library(pypi=PythonPyPiLibrary(package=\"rtdip-sdk[pipelines]\"))\n            )\n\n        # Add Spark Configuration to Databricks Job\n        if (\n            task.new_cluster is None\n            and task.job_cluster_key is None\n            and task.compute_key is None\n        ):\n            return ValueError(\n                \"A Cluster or Compute must be specified for each task in the Databricks Job\"\n            )\n        if task.new_cluster is not None:\n            if spark_configuration is not None:\n                if task.new_cluster.spark_conf is None:\n                    task.new_cluster.spark_conf = {}\n                task.new_cluster.spark_conf.update(spark_configuration)\n        elif task.job_cluster_key is not None:\n            for job_cluster in self.databricks_job.job_clusters:\n                if job_cluster.job_cluster_key == task.job_cluster_key:\n                    if spark_configuration is not None:\n                        if job_cluster.new_cluster.spark_conf is None:\n                            job_cluster.new_cluster.spark_conf = {}\n                        job_cluster.new_cluster.spark_conf.update(\n                            spark_configuration\n                        )\n                    break\n        elif task.compute_key is not None:\n            for compute in self.databricks_job.compute:\n                if compute.compute_key == task.compute_key:\n                    # TODO : Add spark config for compute. Does not seem to be currently available in the Databricks SDK # NOSONAR\n                    # compute.spark_conf.update(spark_configuration)\n                    break\n\n    # Create Databricks Job\n    job_found = False\n    for existing_job in workspace_client.jobs.list(name=self.databricks_job.name):\n        new_settings = JobSettings()\n        for key, value in self.databricks_job.__dict__.items():\n            if key in new_settings.__dict__:\n                setattr(new_settings, key, value)\n        workspace_client.jobs.reset(\n            job_id=existing_job.job_id, new_settings=new_settings\n        )\n        job_found = True\n        break\n\n    if job_found == False:\n        workspace_client.jobs.create(**self.databricks_job.__dict__)\n\n    return True\n</code></pre>"},{"location":"sdk/code-reference/pipelines/deploy/databricks/#src.sdk.python.rtdip_sdk.pipelines.deploy.databricks.DatabricksSDKDeploy.launch","title":"<code>launch()</code>","text":"<p>Launches an RTDIP Pipeline Job in Databricks Workflows. This will perform the equivalent of a <code>Run Now</code> in Databricks Workflows</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/deploy/databricks.py</code> <pre><code>def launch(self):\n\"\"\"\n    Launches an RTDIP Pipeline Job in Databricks Workflows. This will perform the equivalent of a `Run Now` in Databricks Workflows\n    \"\"\"\n    workspace_client = WorkspaceClient(\n        host=self.host, token=self.token, auth_type=\"pat\"\n    )\n    job_found = False\n    for existing_job in workspace_client.jobs.list(name=self.databricks_job.name):\n        workspace_client.jobs.run_now(job_id=existing_job.job_id)\n        job_found = True\n        break\n\n    if job_found == False:\n        raise ValueError(\"Job not found in Databricks Workflows\")\n\n    return True\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/blockchain/evm/","title":"Write to EVM Blockchain","text":""},{"location":"sdk/code-reference/pipelines/destinations/blockchain/evm/#src.sdk.python.rtdip_sdk.pipelines.destinations.blockchain.evm.EVMContractDestination","title":"<code>EVMContractDestination</code>","text":"<p>             Bases: <code>DestinationInterface</code></p> <p>The EVM Contract Destination is used to write to a smart contract blockchain.</p>"},{"location":"sdk/code-reference/pipelines/destinations/blockchain/evm/#src.sdk.python.rtdip_sdk.pipelines.destinations.blockchain.evm.EVMContractDestination--examples","title":"Examples","text":"<pre><code>from rtdip_sdk.pipelines.destinations import EVMContractDestination\n\nevm_contract_destination = EVMContractDestination(\n    url=\"https://polygon-mumbai.g.alchemy.com/v2/\u27e8API_KEY\u27e9\",\n    account=\"{ACCOUNT-ADDRESS}\",\n    private_key=\"{PRIVATE-KEY}\",\n    abi=\"{SMART-CONTRACT'S-ABI}\",\n    contract=\"{SMART-CONTRACT-ADDRESS}\",\n    function_name=\"{SMART-CONTRACT-FUNCTION}\",\n    function_params=({PARAMETER_1}, {PARAMETER_2}, {PARAMETER_3}),\n    transaction={'gas': {GAS}, 'gasPrice': {GAS-PRICE}},\n)\n\nevm_contract_destination.write_batch()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>Blockchain network URL e.g. 'https://polygon-mumbai.g.alchemy.com/v2/\u27e8API_KEY\u27e9'</p> required <code>account</code> <code>str</code> <p>Address of the sender that will be signing the transaction.</p> required <code>private_key</code> <code>str</code> <p>Private key for your blockchain account.</p> required <code>abi</code> <code>json str</code> <p>Smart contract's ABI.</p> required <code>contract</code> <code>str</code> <p>Address of the smart contract.</p> <code>None</code> <code>function_name</code> <code>str</code> <p>Smart contract method to call on.</p> <code>None</code> <code>function_params</code> <code>tuple</code> <p>Parameters of given function.</p> <code>None</code> <code>transaction</code> <code>dict</code> <p>A dictionary containing a set of instructions to interact with a smart contract deployed on the blockchain (See common parameters in Attributes table below).</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>data</code> <code>hexadecimal str</code> <p>Additional information store in the transaction.</p> <code>from</code> <code>hexadecimal str</code> <p>Address of sender for a transaction.</p> <code>gas</code> <code>int</code> <p>Amount of gas units to perform a transaction.</p> <code>gasPrice</code> <code>int Wei</code> <p>Price to pay for each unit of gas. Integers are specified in Wei, web3's to_wei function can be used to specify the amount in a different currency.</p> <code>nonce</code> <code>int</code> <p>The number of transactions sent from a given address.</p> <code>to</code> <code>hexadecimal str</code> <p>Address of recipient for a transaction.</p> <code>value</code> <code>int Wei</code> <p>Value being transferred in a transaction. Integers are specified in Wei, web3's to_wei function can be used to specify the amount in a different currency.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/blockchain/evm.py</code> <pre><code>class EVMContractDestination(DestinationInterface):\n\"\"\"\n    The EVM Contract Destination is used to write to a smart contract blockchain.\n\n    Examples\n    --------\n    ```python\n    from rtdip_sdk.pipelines.destinations import EVMContractDestination\n\n    evm_contract_destination = EVMContractDestination(\n        url=\"https://polygon-mumbai.g.alchemy.com/v2/\u27e8API_KEY\u27e9\",\n        account=\"{ACCOUNT-ADDRESS}\",\n        private_key=\"{PRIVATE-KEY}\",\n        abi=\"{SMART-CONTRACT'S-ABI}\",\n        contract=\"{SMART-CONTRACT-ADDRESS}\",\n        function_name=\"{SMART-CONTRACT-FUNCTION}\",\n        function_params=({PARAMETER_1}, {PARAMETER_2}, {PARAMETER_3}),\n        transaction={'gas': {GAS}, 'gasPrice': {GAS-PRICE}},\n    )\n\n    evm_contract_destination.write_batch()\n    ```\n\n    Parameters:\n        url (str): Blockchain network URL e.g. 'https://polygon-mumbai.g.alchemy.com/v2/\u27e8API_KEY\u27e9'\n        account (str): Address of the sender that will be signing the transaction.\n        private_key (str): Private key for your blockchain account.\n        abi (json str): Smart contract's ABI.\n        contract (str): Address of the smart contract.\n        function_name (str): Smart contract method to call on.\n        function_params (tuple): Parameters of given function.\n        transaction (dict): A dictionary containing a set of instructions to interact with a smart contract deployed on the blockchain (See common parameters in Attributes table below).\n\n    Attributes:\n        data (hexadecimal str): Additional information store in the transaction.\n        from (hexadecimal str): Address of sender for a transaction.\n        gas (int): Amount of gas units to perform a transaction.\n        gasPrice (int Wei): Price to pay for each unit of gas. Integers are specified in Wei, web3's to_wei function can be used to specify the amount in a different currency.\n        nonce (int): The number of transactions sent from a given address.\n        to (hexadecimal str): Address of recipient for a transaction.\n        value (int Wei): Value being transferred in a transaction. Integers are specified in Wei, web3's to_wei function can be used to specify the amount in a different currency.\n    \"\"\"\n\n    url: str\n    account: str\n    private_key: str\n    abi: str\n    contract: str\n    function_name: str\n    function_params: tuple\n    transaction: dict\n\n    def __init__(\n        self,\n        url: str,\n        account: str,\n        private_key: str,\n        abi: str,\n        contract: str = None,\n        function_name: str = None,\n        function_params: tuple = None,\n        transaction: dict = None,\n    ) -&gt; None:\n        self.url = url\n        self.account = account\n        self.private_key = private_key\n        self.abi = json.loads(abi)\n        self.contract = contract\n        self.function_name = function_name\n        self.function_params = function_params\n        self.transaction = transaction\n        self.web3 = Web3(Web3.HTTPProvider(self.url))\n\n    @staticmethod\n    def system_type():\n        return SystemType.PYTHON\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_write_validation(self) -&gt; bool:\n        return True\n\n    def post_write_validation(self) -&gt; bool:\n        return True\n\n    def _process_transaction(self):\n        if \"nonce\" not in self.transaction.keys():\n            nonce = self.web3.eth.get_transaction_count(self.account)\n            self.transaction[\"nonce\"] = nonce\n        if \"from\" not in self.transaction.keys():\n            self.transaction[\"from\"] = self.account\n\n    def write_batch(self) -&gt; str:\n\"\"\"\n        Writes to a smart contract deployed in a blockchain and returns the transaction hash.\n\n        Example:\n        ```\n        from web3 import Web3\n\n        web3 = Web3(Web3.HTTPProvider(\"https://polygon-mumbai.g.alchemy.com/v2/&lt;API_KEY&gt;\"))\n\n        x = EVMContractDestination(\n                            url=\"https://polygon-mumbai.g.alchemy.com/v2/&lt;API_KEY&gt;\",\n                            account='&lt;ACCOUNT&gt;',\n                            private_key='&lt;PRIVATE_KEY&gt;',\n                            contract='&lt;CONTRACT&gt;',\n                            function_name='transferFrom',\n                            function_params=('&lt;FROM_ACCOUNT&gt;', '&lt;TO_ACCOUNT&gt;', 0),\n                            abi = 'ABI',\n                            transaction={\n                                'gas': 100000,\n                                'gasPrice': 1000000000 # or web3.to_wei('1', 'gwei')\n                                },\n                            )\n\n        print(x.write_batch())\n        ```\n        \"\"\"\n        contract = self.web3.eth.contract(address=self.contract, abi=self.abi)\n\n        self._process_transaction()\n        tx = contract.functions[self.function_name](\n            *self.function_params\n        ).build_transaction(self.transaction)\n\n        signed_tx = self.web3.eth.account.sign_transaction(tx, self.private_key)\n        tx_hash = self.web3.eth.send_raw_transaction(signed_tx.rawTransaction)\n        self.web3.eth.wait_for_transaction_receipt(tx_hash)\n\n        return str(self.web3.to_hex(tx_hash))\n\n    def write_stream(self):\n\"\"\"\n        Raises:\n            NotImplementedError: Write stream is not supported.\n        \"\"\"\n        raise NotImplementedError(\"EVMContractDestination only supports batch writes.\")\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/blockchain/evm/#src.sdk.python.rtdip_sdk.pipelines.destinations.blockchain.evm.EVMContractDestination.write_batch","title":"<code>write_batch()</code>","text":"<p>Writes to a smart contract deployed in a blockchain and returns the transaction hash.</p> <p>Example: <pre><code>from web3 import Web3\n\nweb3 = Web3(Web3.HTTPProvider(\"https://polygon-mumbai.g.alchemy.com/v2/&lt;API_KEY&gt;\"))\n\nx = EVMContractDestination(\n                    url=\"https://polygon-mumbai.g.alchemy.com/v2/&lt;API_KEY&gt;\",\n                    account='&lt;ACCOUNT&gt;',\n                    private_key='&lt;PRIVATE_KEY&gt;',\n                    contract='&lt;CONTRACT&gt;',\n                    function_name='transferFrom',\n                    function_params=('&lt;FROM_ACCOUNT&gt;', '&lt;TO_ACCOUNT&gt;', 0),\n                    abi = 'ABI',\n                    transaction={\n                        'gas': 100000,\n                        'gasPrice': 1000000000 # or web3.to_wei('1', 'gwei')\n                        },\n                    )\n\nprint(x.write_batch())\n</code></pre></p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/blockchain/evm.py</code> <pre><code>def write_batch(self) -&gt; str:\n\"\"\"\n    Writes to a smart contract deployed in a blockchain and returns the transaction hash.\n\n    Example:\n    ```\n    from web3 import Web3\n\n    web3 = Web3(Web3.HTTPProvider(\"https://polygon-mumbai.g.alchemy.com/v2/&lt;API_KEY&gt;\"))\n\n    x = EVMContractDestination(\n                        url=\"https://polygon-mumbai.g.alchemy.com/v2/&lt;API_KEY&gt;\",\n                        account='&lt;ACCOUNT&gt;',\n                        private_key='&lt;PRIVATE_KEY&gt;',\n                        contract='&lt;CONTRACT&gt;',\n                        function_name='transferFrom',\n                        function_params=('&lt;FROM_ACCOUNT&gt;', '&lt;TO_ACCOUNT&gt;', 0),\n                        abi = 'ABI',\n                        transaction={\n                            'gas': 100000,\n                            'gasPrice': 1000000000 # or web3.to_wei('1', 'gwei')\n                            },\n                        )\n\n    print(x.write_batch())\n    ```\n    \"\"\"\n    contract = self.web3.eth.contract(address=self.contract, abi=self.abi)\n\n    self._process_transaction()\n    tx = contract.functions[self.function_name](\n        *self.function_params\n    ).build_transaction(self.transaction)\n\n    signed_tx = self.web3.eth.account.sign_transaction(tx, self.private_key)\n    tx_hash = self.web3.eth.send_raw_transaction(signed_tx.rawTransaction)\n    self.web3.eth.wait_for_transaction_receipt(tx_hash)\n\n    return str(self.web3.to_hex(tx_hash))\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/blockchain/evm/#src.sdk.python.rtdip_sdk.pipelines.destinations.blockchain.evm.EVMContractDestination.write_stream","title":"<code>write_stream()</code>","text":"<p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Write stream is not supported.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/blockchain/evm.py</code> <pre><code>def write_stream(self):\n\"\"\"\n    Raises:\n        NotImplementedError: Write stream is not supported.\n    \"\"\"\n    raise NotImplementedError(\"EVMContractDestination only supports batch writes.\")\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/python/delta/","title":"Write to Delta","text":""},{"location":"sdk/code-reference/pipelines/destinations/python/delta/#src.sdk.python.rtdip_sdk.pipelines.destinations.python.delta.PythonDeltaDestination","title":"<code>PythonDeltaDestination</code>","text":"<p>             Bases: <code>DestinationInterface</code></p> <p>The Python Delta Destination is used to write data to a Delta table from a Polars LazyFrame.</p>"},{"location":"sdk/code-reference/pipelines/destinations/python/delta/#src.sdk.python.rtdip_sdk.pipelines.destinations.python.delta.PythonDeltaDestination--example","title":"Example","text":"AzureAWS <pre><code>from rtdip_sdk.pipelines.destinations import PythonDeltaDestination\n\npath = \"abfss://{FILE-SYSTEM}@{ACCOUNT-NAME}.dfs.core.windows.net/{PATH}/{FILE-NAME}\n\npython_delta_destination = PythonDeltaDestination(\n    data=LazyFrame\n    path=path,\n    storage_options={\n        \"azure_storage_account_name\": \"{AZURE-STORAGE-ACCOUNT-NAME}\",\n        \"azure_storage_account_key\": \"{AZURE-STORAGE-ACCOUNT-KEY}\"\n    },\n    mode=:error\",\n    overwrite_schema=False,\n    delta_write_options=None\n)\n\npython_delta_destination.read_batch()\n</code></pre> <pre><code>from rtdip_sdk.pipelines.destinations import PythonDeltaDestination\n\npath = \"https://s3.{REGION-CODE}.amazonaws.com/{BUCKET-NAME}/{KEY-NAME}\"\n\npython_delta_destination = PythonDeltaDestination(\n    data=LazyFrame\n    path=path,\n    options={\n        \"aws_access_key_id\": \"{AWS-ACCESS-KEY-ID}\",\n        \"aws_secret_access_key\": \"{AWS-SECRET-ACCESS-KEY}\"\n    },\n    mode=:error\",\n    overwrite_schema=False,\n    delta_write_options=None\n)\n\npython_delta_destination.read_batch()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>LazyFrame</code> <p>Polars LazyFrame to be written to Delta</p> required <code>path</code> <code>str</code> <p>Path to Delta table to be written to; either local or remote. Locally if the Table does't exist one will be created, but to write to AWS or Azure, you must have an existing Delta Table</p> required <code>options</code> <code>Optional dict</code> <p>Used if writing to a remote location. For AWS use format {\"aws_access_key_id\": \"&lt;&gt;\", \"aws_secret_access_key\": \"&lt;&gt;\"}. For Azure use format {\"azure_storage_account_name\": \"storageaccountname\", \"azure_storage_access_key\": \"&lt;&gt;\"}</p> <code>None</code> <code>mode</code> <code>Literal['error', 'append', 'overwrite', 'ignore']</code> <p>Defaults to error if table exists, 'ignore' won't write anything if table exists</p> <code>'error'</code> <code>overwrite_schema</code> <code>bool</code> <p>If True will allow for the table schema to be overwritten</p> <code>False</code> <code>delta_write_options</code> <code>dict</code> <p>Options when writing to a Delta table. See here for all options</p> <code>None</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/python/delta.py</code> <pre><code>class PythonDeltaDestination(DestinationInterface):\n\"\"\"\n    The Python Delta Destination is used to write data to a Delta table from a Polars LazyFrame.\n\n     Example\n    --------\n    === \"Azure\"\n\n        ```python\n        from rtdip_sdk.pipelines.destinations import PythonDeltaDestination\n\n        path = \"abfss://{FILE-SYSTEM}@{ACCOUNT-NAME}.dfs.core.windows.net/{PATH}/{FILE-NAME}\n\n        python_delta_destination = PythonDeltaDestination(\n            data=LazyFrame\n            path=path,\n            storage_options={\n                \"azure_storage_account_name\": \"{AZURE-STORAGE-ACCOUNT-NAME}\",\n                \"azure_storage_account_key\": \"{AZURE-STORAGE-ACCOUNT-KEY}\"\n            },\n            mode=:error\",\n            overwrite_schema=False,\n            delta_write_options=None\n        )\n\n        python_delta_destination.read_batch()\n\n        ```\n    === \"AWS\"\n\n        ```python\n        from rtdip_sdk.pipelines.destinations import PythonDeltaDestination\n\n        path = \"https://s3.{REGION-CODE}.amazonaws.com/{BUCKET-NAME}/{KEY-NAME}\"\n\n        python_delta_destination = PythonDeltaDestination(\n            data=LazyFrame\n            path=path,\n            options={\n                \"aws_access_key_id\": \"{AWS-ACCESS-KEY-ID}\",\n                \"aws_secret_access_key\": \"{AWS-SECRET-ACCESS-KEY}\"\n            },\n            mode=:error\",\n            overwrite_schema=False,\n            delta_write_options=None\n        )\n\n        python_delta_destination.read_batch()\n        ```\n\n    Parameters:\n        data (LazyFrame): Polars LazyFrame to be written to Delta\n        path (str): Path to Delta table to be written to; either local or [remote](https://delta-io.github.io/delta-rs/python/usage.html#loading-a-delta-table){ target=\"_blank\" }. **Locally** if the Table does't exist one will be created, but to write to AWS or Azure, you must have an existing Delta Table\n        options (Optional dict): Used if writing to a remote location. For AWS use format {\"aws_access_key_id\": \"&lt;&gt;\", \"aws_secret_access_key\": \"&lt;&gt;\"}. For Azure use format {\"azure_storage_account_name\": \"storageaccountname\", \"azure_storage_access_key\": \"&lt;&gt;\"}\n        mode (Literal['error', 'append', 'overwrite', 'ignore']): Defaults to error if table exists, 'ignore' won't write anything if table exists\n        overwrite_schema (bool): If True will allow for the table schema to be overwritten\n        delta_write_options (dict): Options when writing to a Delta table. See [here](https://delta-io.github.io/delta-rs/python/api_reference.html#writing-deltatables){ target=\"_blank\" } for all options\n    \"\"\"\n\n    data: LazyFrame\n    path: str\n    options: dict\n    mode: Literal[\"error\", \"append\", \"overwrite\", \"ignore\"]\n    overwrite_schema: bool\n    delta_write_options: dict\n\n    def __init__(\n        self,\n        data: LazyFrame,\n        path: str,\n        options: dict = None,\n        mode: Literal[\"error\", \"append\", \"overwrite\", \"ignore\"] = \"error\",\n        overwrite_schema: bool = False,\n        delta_write_options: dict = None,\n    ) -&gt; None:\n        self.data = data\n        self.path = path\n        self.options = options\n        self.mode = mode\n        self.overwrite_schema = overwrite_schema\n        self.delta_write_options = delta_write_options\n\n    @staticmethod\n    def system_type():\n\"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYTHON\n        \"\"\"\n        return SystemType.PYTHON\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_write_validation(self):\n        return True\n\n    def post_write_validation(self):\n        return True\n\n    def write_batch(self):\n\"\"\"\n        Writes batch data to Delta without using Spark.\n        \"\"\"\n        if isinstance(self.data, pl.LazyFrame):\n            df = self.data.collect()\n            df.write_delta(\n                self.path,\n                mode=self.mode,\n                overwrite_schema=self.overwrite_schema,\n                storage_options=self.options,\n                delta_write_options=self.delta_write_options,\n            )\n        else:\n            raise ValueError(\n                \"Data must be a Polars LazyFrame. See https://pola-rs.github.io/polars/py-polars/html/reference/lazyframe/index.html\"\n            )\n\n    def write_stream(self):\n\"\"\"\n        Raises:\n            NotImplementedError: Writing to a Delta table using Python is only possible for batch writes. To perform a streaming read, use the write_stream method of the SparkDeltaDestination component.\n        \"\"\"\n        raise NotImplementedError(\n            \"Writing to a Delta table using Python is only possible for batch writes. To perform a streaming read, use the write_stream method of the SparkDeltaDestination component\"\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/python/delta/#src.sdk.python.rtdip_sdk.pipelines.destinations.python.delta.PythonDeltaDestination.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYTHON</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/python/delta.py</code> <pre><code>@staticmethod\ndef system_type():\n\"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYTHON\n    \"\"\"\n    return SystemType.PYTHON\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/python/delta/#src.sdk.python.rtdip_sdk.pipelines.destinations.python.delta.PythonDeltaDestination.write_batch","title":"<code>write_batch()</code>","text":"<p>Writes batch data to Delta without using Spark.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/python/delta.py</code> <pre><code>def write_batch(self):\n\"\"\"\n    Writes batch data to Delta without using Spark.\n    \"\"\"\n    if isinstance(self.data, pl.LazyFrame):\n        df = self.data.collect()\n        df.write_delta(\n            self.path,\n            mode=self.mode,\n            overwrite_schema=self.overwrite_schema,\n            storage_options=self.options,\n            delta_write_options=self.delta_write_options,\n        )\n    else:\n        raise ValueError(\n            \"Data must be a Polars LazyFrame. See https://pola-rs.github.io/polars/py-polars/html/reference/lazyframe/index.html\"\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/python/delta/#src.sdk.python.rtdip_sdk.pipelines.destinations.python.delta.PythonDeltaDestination.write_stream","title":"<code>write_stream()</code>","text":"<p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Writing to a Delta table using Python is only possible for batch writes. To perform a streaming read, use the write_stream method of the SparkDeltaDestination component.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/python/delta.py</code> <pre><code>def write_stream(self):\n\"\"\"\n    Raises:\n        NotImplementedError: Writing to a Delta table using Python is only possible for batch writes. To perform a streaming read, use the write_stream method of the SparkDeltaDestination component.\n    \"\"\"\n    raise NotImplementedError(\n        \"Writing to a Delta table using Python is only possible for batch writes. To perform a streaming read, use the write_stream method of the SparkDeltaDestination component\"\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/delta/","title":"Write to Delta","text":""},{"location":"sdk/code-reference/pipelines/destinations/spark/delta/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.delta.SparkDeltaDestination","title":"<code>SparkDeltaDestination</code>","text":"<p>             Bases: <code>DestinationInterface</code></p> <p>The Spark Delta Destination is used to write data to a Delta table.</p>"},{"location":"sdk/code-reference/pipelines/destinations/spark/delta/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.delta.SparkDeltaDestination--examples","title":"Examples","text":"<p><pre><code>#Delta Destination for Streaming Queries\n\nfrom rtdip_sdk.pipelines.destinations import SparkDeltaDestination\n\ndelta_destination = SparkDeltaDestination(\n    data=df,\n    options={\n        \"checkpointLocation\": \"/{CHECKPOINT-LOCATION}/\"\n    },\n    destination=\"DELTA-TABLE-PATH\",\n    mode=\"append\",\n    trigger=\"10 seconds\",\n    query_name=\"DeltaDestination\",\n    query_wait_interval=None\n)\n\ndelta_destination.write_stream()\n</code></pre> <pre><code>#Delta Destination for Batch Queries\n\nfrom rtdip_sdk.pipelines.destinations import SparkDeltaDestination\n\ndelta_destination = SparkDeltaDestination(\n    data=df,\n    options={\n        \"overwriteSchema\": True\n    },\n    destination=\"DELTA-TABLE-PATH\",\n    mode=\"append\",\n    trigger=\"10 seconds\",\n    query_name=\"DeltaDestination\",\n    query_wait_interval=None\n)\n\ndelta_destination.write_batch()\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Dataframe to be written to Delta</p> required <code>options</code> <code>dict</code> <p>Options that can be specified for a Delta Table write operation (See Attributes table below). Further information on the options is available for batch and streaming.</p> required <code>destination</code> <code>str</code> <p>Either the name of the Hive Metastore or Unity Catalog Delta Table or the path to the Delta table</p> required <code>mode</code> <code>optional str</code> <p>Method of writing to Delta Table - append/overwrite (batch), append/update/complete (stream). Default is append</p> <code>'append'</code> <code>trigger</code> <code>optional str</code> <p>Frequency of the write operation. Specify \"availableNow\" to execute a trigger once, otherwise specify a time period such as \"30 seconds\", \"5 minutes\". Set to \"0 seconds\" if you do not want to use a trigger. (stream) Default is 10 seconds</p> <code>'10 seconds'</code> <code>query_name</code> <code>optional str</code> <p>Unique name for the query in associated SparkSession. (stream) Default is DeltaDestination</p> <code>'DeltaDestination'</code> <code>query_wait_interval</code> <code>optional int</code> <p>If set, waits for the streaming query to complete before returning. (stream) Default is None</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>checkpointLocation</code> <code>str</code> <p>Path to checkpoint files. (Streaming)</p> <code>txnAppId</code> <code>str</code> <p>A unique string that you can pass on each DataFrame write. (Batch &amp; Streaming)</p> <code>txnVersion</code> <code>str</code> <p>A monotonically increasing number that acts as transaction version. (Batch &amp; Streaming)</p> <code>maxRecordsPerFile</code> <code>int str</code> <p>Specify the maximum number of records to write to a single file for a Delta Lake table. (Batch)</p> <code>replaceWhere</code> <code>str</code> <p>Condition(s) for overwriting. (Batch)</p> <code>partitionOverwriteMode</code> <code>str</code> <p>When set to dynamic, overwrites all existing data in each logical partition for which the write will commit new data. Default is static. (Batch)</p> <code>overwriteSchema</code> <code>bool str</code> <p>If True, overwrites the schema as well as the table data. (Batch)</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/delta.py</code> <pre><code>class SparkDeltaDestination(DestinationInterface):\n\"\"\"\n    The Spark Delta Destination is used to write data to a Delta table.\n\n    Examples\n    --------\n    ```python\n    #Delta Destination for Streaming Queries\n\n    from rtdip_sdk.pipelines.destinations import SparkDeltaDestination\n\n    delta_destination = SparkDeltaDestination(\n        data=df,\n        options={\n            \"checkpointLocation\": \"/{CHECKPOINT-LOCATION}/\"\n        },\n        destination=\"DELTA-TABLE-PATH\",\n        mode=\"append\",\n        trigger=\"10 seconds\",\n        query_name=\"DeltaDestination\",\n        query_wait_interval=None\n    )\n\n    delta_destination.write_stream()\n    ```\n    ```python\n    #Delta Destination for Batch Queries\n\n    from rtdip_sdk.pipelines.destinations import SparkDeltaDestination\n\n    delta_destination = SparkDeltaDestination(\n        data=df,\n        options={\n            \"overwriteSchema\": True\n        },\n        destination=\"DELTA-TABLE-PATH\",\n        mode=\"append\",\n        trigger=\"10 seconds\",\n        query_name=\"DeltaDestination\",\n        query_wait_interval=None\n    )\n\n    delta_destination.write_batch()\n    ```\n\n    Parameters:\n        data (DataFrame): Dataframe to be written to Delta\n        options (dict): Options that can be specified for a Delta Table write operation (See Attributes table below). Further information on the options is available for [batch](https://docs.delta.io/latest/delta-batch.html#write-to-a-table){ target=\"_blank\" } and [streaming](https://docs.delta.io/latest/delta-streaming.html#delta-table-as-a-sink){ target=\"_blank\" }.\n        destination (str): Either the name of the Hive Metastore or Unity Catalog Delta Table **or** the path to the Delta table\n        mode (optional str): Method of writing to Delta Table - append/overwrite (batch), append/update/complete (stream). Default is append\n        trigger (optional str): Frequency of the write operation. Specify \"availableNow\" to execute a trigger once, otherwise specify a time period such as \"30 seconds\", \"5 minutes\". Set to \"0 seconds\" if you do not want to use a trigger. (stream) Default is 10 seconds\n        query_name (optional str): Unique name for the query in associated SparkSession. (stream) Default is DeltaDestination\n        query_wait_interval (optional int): If set, waits for the streaming query to complete before returning. (stream) Default is None\n\n    Attributes:\n        checkpointLocation (str): Path to checkpoint files. (Streaming)\n        txnAppId (str): A unique string that you can pass on each DataFrame write. (Batch &amp; Streaming)\n        txnVersion (str): A monotonically increasing number that acts as transaction version. (Batch &amp; Streaming)\n        maxRecordsPerFile (int str): Specify the maximum number of records to write to a single file for a Delta Lake table. (Batch)\n        replaceWhere (str): Condition(s) for overwriting. (Batch)\n        partitionOverwriteMode (str): When set to dynamic, overwrites all existing data in each logical partition for which the write will commit new data. Default is static. (Batch)\n        overwriteSchema (bool str): If True, overwrites the schema as well as the table data. (Batch)\n    \"\"\"\n\n    data: DataFrame\n    options: dict\n    destination: str\n    mode: str\n    trigger: str\n    query_name: str\n    query_wait_interval: int\n\n    def __init__(\n        self,\n        data: DataFrame,\n        options: dict,\n        destination: str,\n        mode: str = \"append\",\n        trigger: str = \"10 seconds\",\n        query_name: str = \"DeltaDestination\",\n        query_wait_interval: int = None,\n    ) -&gt; None:\n        self.data = data\n        self.options = options\n        self.destination = destination\n        self.mode = mode\n        self.trigger = trigger\n        self.query_name = query_name\n        self.query_wait_interval = query_wait_interval\n\n    @staticmethod\n    def system_type():\n\"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        libraries.add_maven_library(get_default_package(\"spark_delta_core\"))\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {\n            \"spark.sql.extensions\": \"io.delta.sql.DeltaSparkSessionExtension\",\n            \"spark.sql.catalog.spark_catalog\": \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n        }\n\n    def pre_write_validation(self):\n        return True\n\n    def post_write_validation(self):\n        return True\n\n    def write_batch(self):\n\"\"\"\n        Writes batch data to Delta. Most of the options provided by the Apache Spark DataFrame write API are supported for performing batch writes on tables.\n        \"\"\"\n        try:\n            if \"/\" in self.destination:\n                return (\n                    self.data.write.format(\"delta\")\n                    .mode(self.mode)\n                    .options(**self.options)\n                    .save(self.destination)\n                )\n            else:\n                return (\n                    self.data.write.format(\"delta\")\n                    .mode(self.mode)\n                    .options(**self.options)\n                    .saveAsTable(self.destination)\n                )\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def write_stream(self):\n\"\"\"\n        Writes streaming data to Delta. Exactly-once processing is guaranteed\n        \"\"\"\n        TRIGGER_OPTION = (\n            {\"availableNow\": True}\n            if self.trigger == \"availableNow\"\n            else {\"processingTime\": self.trigger}\n        )\n        try:\n            if \"/\" in self.destination:\n                query = (\n                    self.data.writeStream.trigger(**TRIGGER_OPTION)\n                    .format(\"delta\")\n                    .queryName(self.query_name)\n                    .outputMode(self.mode)\n                    .options(**self.options)\n                    .start(self.destination)\n                )\n            else:\n                query = (\n                    self.data.writeStream.trigger(**TRIGGER_OPTION)\n                    .format(\"delta\")\n                    .queryName(self.query_name)\n                    .outputMode(self.mode)\n                    .options(**self.options)\n                    .toTable(self.destination)\n                )\n\n            if self.query_wait_interval:\n                while query.isActive:\n                    if query.lastProgress:\n                        logging.info(query.lastProgress)\n                    time.sleep(self.query_wait_interval)\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/delta/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.delta.SparkDeltaDestination.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/delta.py</code> <pre><code>@staticmethod\ndef system_type():\n\"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/delta/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.delta.SparkDeltaDestination.write_batch","title":"<code>write_batch()</code>","text":"<p>Writes batch data to Delta. Most of the options provided by the Apache Spark DataFrame write API are supported for performing batch writes on tables.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/delta.py</code> <pre><code>def write_batch(self):\n\"\"\"\n    Writes batch data to Delta. Most of the options provided by the Apache Spark DataFrame write API are supported for performing batch writes on tables.\n    \"\"\"\n    try:\n        if \"/\" in self.destination:\n            return (\n                self.data.write.format(\"delta\")\n                .mode(self.mode)\n                .options(**self.options)\n                .save(self.destination)\n            )\n        else:\n            return (\n                self.data.write.format(\"delta\")\n                .mode(self.mode)\n                .options(**self.options)\n                .saveAsTable(self.destination)\n            )\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/delta/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.delta.SparkDeltaDestination.write_stream","title":"<code>write_stream()</code>","text":"<p>Writes streaming data to Delta. Exactly-once processing is guaranteed</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/delta.py</code> <pre><code>def write_stream(self):\n\"\"\"\n    Writes streaming data to Delta. Exactly-once processing is guaranteed\n    \"\"\"\n    TRIGGER_OPTION = (\n        {\"availableNow\": True}\n        if self.trigger == \"availableNow\"\n        else {\"processingTime\": self.trigger}\n    )\n    try:\n        if \"/\" in self.destination:\n            query = (\n                self.data.writeStream.trigger(**TRIGGER_OPTION)\n                .format(\"delta\")\n                .queryName(self.query_name)\n                .outputMode(self.mode)\n                .options(**self.options)\n                .start(self.destination)\n            )\n        else:\n            query = (\n                self.data.writeStream.trigger(**TRIGGER_OPTION)\n                .format(\"delta\")\n                .queryName(self.query_name)\n                .outputMode(self.mode)\n                .options(**self.options)\n                .toTable(self.destination)\n            )\n\n        if self.query_wait_interval:\n            while query.isActive:\n                if query.lastProgress:\n                    logging.info(query.lastProgress)\n                time.sleep(self.query_wait_interval)\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/delta_merge/","title":"Write to Delta using Merge","text":""},{"location":"sdk/code-reference/pipelines/destinations/spark/delta_merge/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.delta_merge.SparkDeltaMergeDestination","title":"<code>SparkDeltaMergeDestination</code>","text":"<p>             Bases: <code>DestinationInterface</code></p> <p>The Spark Delta Merge Destination is used to merge data into a Delta table. Refer to this documentation for more information about Delta Merge.</p>"},{"location":"sdk/code-reference/pipelines/destinations/spark/delta_merge/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.delta_merge.SparkDeltaMergeDestination--examples","title":"Examples","text":"<p><pre><code>#Delta Merge Destination for Streaming Queries\n\nfrom rtdip_sdk.pipelines.destinations import SparkDeltaMergeDestination\n\ndelta_merge_destination = SparkDeltaMergeDestination(\n    data=df,\n    destination=\"DELTA-TABLE-PATH\",\n    options={\n        \"checkpointLocation\": \"/{CHECKPOINT-LOCATION}/\"\n    },\n    merge_condition=\"`source.id = target.id`\"\n    when_matched_update_list=None\n    when_matched_delete_list=None\n    when_not_matched_insert_list=None\n    when_not_matched_by_source_update_list=None\n    when_not_matched_by_source_delete_list=None\n    try_broadcast_join=False\n    trigger=\"10 seconds\",\n    query_name=\"DeltaDestination\"\n    query_wait_interval=None\n)\n\ndelta_merge_destination.write_stream()\n</code></pre> <pre><code>#Delta Merge Destination for Batch Queries\n\nfrom rtdip_sdk.pipelines.destinations import SparkDeltaMergeDestination\n\ndelta_merge_destination = SparkDeltaMergeDestination(\n    data=df,\n    destination=\"DELTA-TABLE-PATH\",\n    options={},\n    merge_condition=\"`source.id = target.id`\",\n    when_matched_update_list=None,\n    when_matched_delete_list=None,\n    when_not_matched_insert_list=None,\n    when_not_matched_by_source_update_list=None,\n    when_not_matched_by_source_delete_list=None,\n    try_broadcast_join=False,\n    trigger=\"10 seconds\",\n    query_name=\"DeltaDestination\"\n    query_wait_interval=None\n)\n\ndelta_merge_destination.write_batch()\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Dataframe to be merged into a Delta Table</p> required <code>destination</code> <code>str</code> <p>Either the name of the Hive Metastore or Unity Catalog Delta Table or the path to the Delta table</p> required <code>options</code> <code>dict</code> <p>Options that can be specified for a Delta Table read operation (See Attributes table below). Further information on the options is available for batch and streaming.</p> required <code>merge_condition</code> <code>str</code> <p>Condition for matching records between dataframe and delta table. Reference Dataframe columns as <code>source</code> and Delta Table columns as <code>target</code>. For example <code>source.id = target.id</code>.</p> required <code>when_matched_update_list</code> <code>optional list[DeltaMergeConditionValues]</code> <p>Conditions(optional) and values to be used when updating rows that match the <code>merge_condition</code>. Specify <code>*</code> for Values if all columns from Dataframe should be inserted.</p> <code>None</code> <code>when_matched_delete_list</code> <code>optional list[DeltaMergeCondition]</code> <p>Conditions(optional) to be used when deleting rows that match the <code>merge_condition</code>.</p> <code>None</code> <code>when_not_matched_insert_list</code> <code>optional list[DeltaMergeConditionValues]</code> <p>Conditions(optional) and values to be used when inserting rows that do not match the <code>merge_condition</code>. Specify <code>*</code> for Values if all columns from Dataframe should be inserted.</p> <code>None</code> <code>when_not_matched_by_source_update_list</code> <code>optional list[DeltaMergeConditionValues]</code> <p>Conditions(optional) and values to be used when updating rows that do not match the <code>merge_condition</code>.</p> <code>None</code> <code>when_not_matched_by_source_delete_list</code> <code>optional list[DeltaMergeCondition]</code> <p>Conditions(optional) to be used when deleting rows that do not match the <code>merge_condition</code>.</p> <code>None</code> <code>try_broadcast_join</code> <code>optional bool</code> <p>Attempts to perform a broadcast join in the merge which can leverage data skipping using partition pruning and file pruning automatically. Can fail if dataframe being merged is large and therefore more suitable for streaming merges than batch merges</p> <code>False</code> <code>trigger</code> <code>optional str</code> <p>Frequency of the write operation. Specify \"availableNow\" to execute a trigger once, otherwise specify a time period such as \"30 seconds\", \"5 minutes\". Set to \"0 seconds\" if you do not want to use a trigger. (stream) Default is 10 seconds</p> <code>'10 seconds'</code> <code>query_name</code> <code>optional str</code> <p>Unique name for the query in associated SparkSession</p> <code>'DeltaMergeDestination'</code> <code>query_wait_interval</code> <code>optional int</code> <p>If set, waits for the streaming query to complete before returning. (stream) Default is None</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>checkpointLocation</code> <code>str</code> <p>Path to checkpoint files. (Streaming)</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/delta_merge.py</code> <pre><code>class SparkDeltaMergeDestination(DestinationInterface):\n\"\"\"\n    The Spark Delta Merge Destination is used to merge data into a Delta table. Refer to this [documentation](https://docs.delta.io/latest/delta-update.html#upsert-into-a-table-using-merge&amp;language-python) for more information about Delta Merge.\n\n    Examples\n    --------\n    ```python\n    #Delta Merge Destination for Streaming Queries\n\n    from rtdip_sdk.pipelines.destinations import SparkDeltaMergeDestination\n\n    delta_merge_destination = SparkDeltaMergeDestination(\n        data=df,\n        destination=\"DELTA-TABLE-PATH\",\n        options={\n            \"checkpointLocation\": \"/{CHECKPOINT-LOCATION}/\"\n        },\n        merge_condition=\"`source.id = target.id`\"\n        when_matched_update_list=None\n        when_matched_delete_list=None\n        when_not_matched_insert_list=None\n        when_not_matched_by_source_update_list=None\n        when_not_matched_by_source_delete_list=None\n        try_broadcast_join=False\n        trigger=\"10 seconds\",\n        query_name=\"DeltaDestination\"\n        query_wait_interval=None\n    )\n\n    delta_merge_destination.write_stream()\n    ```\n    ```python\n    #Delta Merge Destination for Batch Queries\n\n    from rtdip_sdk.pipelines.destinations import SparkDeltaMergeDestination\n\n    delta_merge_destination = SparkDeltaMergeDestination(\n        data=df,\n        destination=\"DELTA-TABLE-PATH\",\n        options={},\n        merge_condition=\"`source.id = target.id`\",\n        when_matched_update_list=None,\n        when_matched_delete_list=None,\n        when_not_matched_insert_list=None,\n        when_not_matched_by_source_update_list=None,\n        when_not_matched_by_source_delete_list=None,\n        try_broadcast_join=False,\n        trigger=\"10 seconds\",\n        query_name=\"DeltaDestination\"\n        query_wait_interval=None\n    )\n\n    delta_merge_destination.write_batch()\n    ```\n\n    Parameters:\n        data (DataFrame): Dataframe to be merged into a Delta Table\n        destination (str): Either the name of the Hive Metastore or Unity Catalog Delta Table **or** the path to the Delta table\n        options (dict): Options that can be specified for a Delta Table read operation (See Attributes table below). Further information on the options is available for [batch](https://docs.delta.io/latest/delta-batch.html#write-to-a-table){ target=\"_blank\" } and [streaming](https://docs.delta.io/latest/delta-streaming.html#delta-table-as-a-sink){ target=\"_blank\" }.\n        merge_condition (str): Condition for matching records between dataframe and delta table. Reference Dataframe columns as `source` and Delta Table columns as `target`. For example `source.id = target.id`.\n        when_matched_update_list (optional list[DeltaMergeConditionValues]): Conditions(optional) and values to be used when updating rows that match the `merge_condition`. Specify `*` for Values if all columns from Dataframe should be inserted.\n        when_matched_delete_list (optional list[DeltaMergeCondition]): Conditions(optional) to be used when deleting rows that match the `merge_condition`.\n        when_not_matched_insert_list (optional list[DeltaMergeConditionValues]): Conditions(optional) and values to be used when inserting rows that do not match the `merge_condition`. Specify `*` for Values if all columns from Dataframe should be inserted.\n        when_not_matched_by_source_update_list (optional list[DeltaMergeConditionValues]): Conditions(optional) and values to be used when updating rows that do not match the `merge_condition`.\n        when_not_matched_by_source_delete_list (optional list[DeltaMergeCondition]): Conditions(optional) to be used when deleting rows that do not match the `merge_condition`.\n        try_broadcast_join (optional bool): Attempts to perform a broadcast join in the merge which can leverage data skipping using partition pruning and file pruning automatically. Can fail if dataframe being merged is large and therefore more suitable for streaming merges than batch merges\n        trigger (optional str): Frequency of the write operation. Specify \"availableNow\" to execute a trigger once, otherwise specify a time period such as \"30 seconds\", \"5 minutes\". Set to \"0 seconds\" if you do not want to use a trigger. (stream) Default is 10 seconds\n        query_name (optional str): Unique name for the query in associated SparkSession\n        query_wait_interval (optional int): If set, waits for the streaming query to complete before returning. (stream) Default is None\n\n    Attributes:\n        checkpointLocation (str): Path to checkpoint files. (Streaming)\n    \"\"\"\n\n    spark: SparkSession\n    data: DataFrame\n    destination: str\n    options: dict\n    merge_condition: str\n    when_matched_update_list: List[DeltaMergeConditionValues]\n    when_matched_delete_list: List[DeltaMergeCondition]\n    when_not_matched_insert_list: List[DeltaMergeConditionValues]\n    when_not_matched_by_source_update_list: List[DeltaMergeConditionValues]\n    when_not_matched_by_source_delete_list: List[DeltaMergeCondition]\n    try_broadcast_join: bool\n    trigger: str\n    query_name: str\n    query_wait_interval: int\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        data: DataFrame,\n        destination: str,\n        options: dict,\n        merge_condition: str,\n        when_matched_update_list: List[DeltaMergeConditionValues] = None,\n        when_matched_delete_list: List[DeltaMergeCondition] = None,\n        when_not_matched_insert_list: List[DeltaMergeConditionValues] = None,\n        when_not_matched_by_source_update_list: List[DeltaMergeConditionValues] = None,\n        when_not_matched_by_source_delete_list: List[DeltaMergeCondition] = None,\n        try_broadcast_join: bool = False,\n        trigger=\"10 seconds\",\n        query_name: str = \"DeltaMergeDestination\",\n        query_wait_interval: int = None,\n    ) -&gt; None:\n        self.spark = spark\n        self.data = data\n        self.destination = destination\n        self.options = options\n        self.merge_condition = merge_condition\n        self.when_matched_update_list = (\n            [] if when_matched_update_list is None else when_matched_update_list\n        )\n        self.when_matched_delete_list = (\n            [] if when_matched_delete_list is None else when_matched_delete_list\n        )\n        self.when_not_matched_insert_list = (\n            [] if when_not_matched_insert_list is None else when_not_matched_insert_list\n        )\n        if (\n            isinstance(when_not_matched_by_source_update_list, list)\n            and len(when_not_matched_by_source_update_list) &gt; 0\n        ):\n            _package_version_meets_minimum(\"delta-spark\", \"2.3.0\")\n        self.when_not_matched_by_source_update_list = (\n            []\n            if when_not_matched_by_source_update_list is None\n            else when_not_matched_by_source_update_list\n        )\n        if (\n            isinstance(when_not_matched_by_source_delete_list, list)\n            and len(when_not_matched_by_source_delete_list) &gt; 0\n        ):\n            _package_version_meets_minimum(\"delta-spark\", \"2.3.0\")\n        self.when_not_matched_by_source_delete_list = (\n            []\n            if when_not_matched_by_source_delete_list is None\n            else when_not_matched_by_source_delete_list\n        )\n        self.try_broadcast_join = try_broadcast_join\n        self.trigger = trigger\n        self.query_name = query_name\n        self.query_wait_interval = query_wait_interval\n\n    @staticmethod\n    def system_type():\n\"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        libraries.add_maven_library(get_default_package(\"spark_delta_core\"))\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {\n            \"spark.sql.extensions\": \"io.delta.sql.DeltaSparkSessionExtension\",\n            \"spark.sql.catalog.spark_catalog\": \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n            \"spark.databricks.delta.schema.autoMerge.enabled\": \"true\",\n        }\n\n    def pre_write_validation(self):\n        return True\n\n    def post_write_validation(self):\n        return True\n\n    def _delta_merge_builder(\n        self, df: DataFrame, try_broadcast_join: bool\n    ) -&gt; DeltaMergeBuilder:\n        if \"/\" in self.destination:\n            delta_table = DeltaTable.forPath(self.spark, self.destination)\n        else:\n            delta_table = DeltaTable.forName(self.spark, self.destination)\n\n        if try_broadcast_join == True:\n            delta_merge_builder = delta_table.alias(\"target\").merge(\n                source=broadcast(df).alias(\"source\"), condition=self.merge_condition\n            )\n        else:\n            delta_merge_builder = delta_table.alias(\"target\").merge(\n                source=df.alias(\"source\"), condition=self.merge_condition\n            )\n\n        for when_matched_update in self.when_matched_update_list:\n            if when_matched_update.values == \"*\":\n                delta_merge_builder = delta_merge_builder.whenMatchedUpdateAll(\n                    condition=when_matched_update.condition,\n                )\n            else:\n                delta_merge_builder = delta_merge_builder.whenMatchedUpdate(\n                    condition=when_matched_update.condition,\n                    set=when_matched_update.values,\n                )\n\n        for when_matched_delete in self.when_matched_delete_list:\n            delta_merge_builder = delta_merge_builder.whenMatchedDelete(\n                condition=when_matched_delete.condition,\n            )\n\n        for when_not_matched_insert in self.when_not_matched_insert_list:\n            if when_not_matched_insert.values == \"*\":\n                delta_merge_builder = delta_merge_builder.whenNotMatchedInsertAll(\n                    condition=when_not_matched_insert.condition,\n                )\n            else:\n                delta_merge_builder = delta_merge_builder.whenNotMatchedInsert(\n                    condition=when_not_matched_insert.condition,\n                    values=when_not_matched_insert.values,\n                )\n\n        for (\n            when_not_matched_by_source_update\n        ) in self.when_not_matched_by_source_update_list:\n            delta_merge_builder = delta_merge_builder.whenNotMatchedBySourceUpdate(\n                condition=when_not_matched_by_source_update.condition,\n                set=when_not_matched_by_source_update.values,\n            )\n\n        for (\n            when_not_matched_by_source_delete\n        ) in self.when_not_matched_by_source_delete_list:\n            delta_merge_builder = delta_merge_builder.whenNotMatchedBySourceDelete(\n                condition=when_not_matched_by_source_delete.condition,\n            )\n\n        return delta_merge_builder\n\n    def _stream_merge_micro_batch(\n        self, micro_batch_df: DataFrame, epoch_id=None\n    ):  # NOSONAR\n        micro_batch_df.persist()\n\n        retry_delta_merge = False\n\n        if self.try_broadcast_join == True:\n            try:\n                delta_merge = self._delta_merge_builder(\n                    micro_batch_df, self.try_broadcast_join\n                )\n                delta_merge.execute()\n            except Exception as e:\n                if \"SparkOutOfMemoryError\" in str(e):\n                    retry_delta_merge = True\n                else:\n                    raise e\n\n        if self.try_broadcast_join == False or retry_delta_merge == True:\n            delta_merge = self._delta_merge_builder(micro_batch_df, False)\n            delta_merge.execute()\n\n        micro_batch_df.unpersist()\n\n    def write_batch(self):\n\"\"\"\n        Merges batch data into a Delta Table.\n        \"\"\"\n        try:\n            delta_merge = self._delta_merge_builder(self.data, self.try_broadcast_join)\n            return delta_merge.execute()\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def write_stream(self):\n\"\"\"\n        Merges streaming data to Delta using foreachBatch\n        \"\"\"\n        TRIGGER_OPTION = (\n            {\"availableNow\": True}\n            if self.trigger == \"availableNow\"\n            else {\"processingTime\": self.trigger}\n        )\n        try:\n            query = (\n                self.data.writeStream.trigger(**TRIGGER_OPTION)\n                .format(\"delta\")\n                .foreachBatch(self._stream_merge_micro_batch)\n                .queryName(self.query_name)\n                .outputMode(\"update\")\n                .options(**self.options)\n                .start()\n            )\n\n            if self.query_wait_interval:\n                while query.isActive:\n                    if query.lastProgress:\n                        logging.info(query.lastProgress)\n                    time.sleep(self.query_wait_interval)\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/delta_merge/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.delta_merge.SparkDeltaMergeDestination.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/delta_merge.py</code> <pre><code>@staticmethod\ndef system_type():\n\"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/delta_merge/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.delta_merge.SparkDeltaMergeDestination.write_batch","title":"<code>write_batch()</code>","text":"<p>Merges batch data into a Delta Table.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/delta_merge.py</code> <pre><code>def write_batch(self):\n\"\"\"\n    Merges batch data into a Delta Table.\n    \"\"\"\n    try:\n        delta_merge = self._delta_merge_builder(self.data, self.try_broadcast_join)\n        return delta_merge.execute()\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/delta_merge/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.delta_merge.SparkDeltaMergeDestination.write_stream","title":"<code>write_stream()</code>","text":"<p>Merges streaming data to Delta using foreachBatch</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/delta_merge.py</code> <pre><code>def write_stream(self):\n\"\"\"\n    Merges streaming data to Delta using foreachBatch\n    \"\"\"\n    TRIGGER_OPTION = (\n        {\"availableNow\": True}\n        if self.trigger == \"availableNow\"\n        else {\"processingTime\": self.trigger}\n    )\n    try:\n        query = (\n            self.data.writeStream.trigger(**TRIGGER_OPTION)\n            .format(\"delta\")\n            .foreachBatch(self._stream_merge_micro_batch)\n            .queryName(self.query_name)\n            .outputMode(\"update\")\n            .options(**self.options)\n            .start()\n        )\n\n        if self.query_wait_interval:\n            while query.isActive:\n                if query.lastProgress:\n                    logging.info(query.lastProgress)\n                time.sleep(self.query_wait_interval)\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/eventhub/","title":"Write to Eventhub","text":""},{"location":"sdk/code-reference/pipelines/destinations/spark/eventhub/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.eventhub.SparkEventhubDestination","title":"<code>SparkEventhubDestination</code>","text":"<p>             Bases: <code>DestinationInterface</code></p> <p>This Spark destination class is used to write batch or streaming data to Eventhubs. Eventhub configurations need to be specified as options in a dictionary. Additionally, there are more optional configurations which can be found here. If using startingPosition or endingPosition make sure to check out Event Position section for more details and examples.</p>"},{"location":"sdk/code-reference/pipelines/destinations/spark/eventhub/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.eventhub.SparkEventhubDestination--examples","title":"Examples","text":"<p><pre><code>#Eventhub Destination for Streaming Queries\n\nfrom rtdip_sdk.pipelines.destinations import SparkEventhubDestination\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\nconnectionString = Endpoint=sb://{NAMESPACE}.servicebus.windows.net/;SharedAccessKeyName={ACCESS_KEY_NAME};SharedAccessKey={ACCESS_KEY}=;EntityPath={EVENT_HUB_NAME}\n\neventhub_destination = SparkEventhubDestination(\n    spark=spark,\n    data=df,\n    options={\n        \"eventhubs.connectionString\": connectionString,\n        \"eventhubs.consumerGroup\": \"{YOUR-EVENTHUB-CONSUMER-GROUP}\",\n        \"checkpointLocation\": \"/{CHECKPOINT-LOCATION}/\"\n    },\n    trigger=\"10 seconds\",\n    query_name=\"EventhubDestination\",\n    query_wait_interval=None\n)\n\neventhub_destination.write_stream()\n</code></pre> <pre><code>#Eventhub Destination for Batch Queries\n\nfrom rtdip_sdk.pipelines.destinations import SparkEventhubDestination\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\nconnectionString = Endpoint=sb://{NAMESPACE}.servicebus.windows.net/;SharedAccessKeyName={ACCESS_KEY_NAME};SharedAccessKey={ACCESS_KEY}=;EntityPath={EVENT_HUB_NAME}\n\n\neventhub_destination = SparkEventhubDestination(\n    spark=spark,\n    data=df,\n    options={\n        \"eventhubs.connectionString\": connectionString,\n        \"eventhubs.consumerGroup\": \"{YOUR-EVENTHUB-CONSUMER-GROUP}\"\n    },\n    trigger=\"10 seconds\",\n    query_name=\"EventhubDestination\",\n    query_wait_interval=None\n)\n\neventhub_destination.write_batch()\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session</p> required <code>data</code> <code>DataFrame</code> <p>Dataframe to be written to Eventhub</p> required <code>options</code> <code>dict</code> <p>A dictionary of Eventhub configurations (See Attributes table below). All Configuration options for Eventhubs can be found here.</p> required <code>trigger</code> <code>optional str</code> <p>Frequency of the write operation. Specify \"availableNow\" to execute a trigger once, otherwise specify a time period such as \"30 seconds\", \"5 minutes\". Set to \"0 seconds\" if you do not want to use a trigger. (stream) Default is 10 seconds</p> <code>'10 seconds'</code> <code>query_name</code> <code>str</code> <p>Unique name for the query in associated SparkSession</p> <code>'EventhubDestination'</code> <code>query_wait_interval</code> <code>optional int</code> <p>If set, waits for the streaming query to complete before returning. (stream) Default is None</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>checkpointLocation</code> <code>str</code> <p>Path to checkpoint files. (Streaming)</p> <code>eventhubs.connectionString</code> <code>str</code> <p>Eventhubs connection string is required to connect to the Eventhubs service. (Streaming and Batch)</p> <code>eventhubs.consumerGroup</code> <code>str</code> <p>A consumer group is a view of an entire eventhub. Consumer groups enable multiple consuming applications to each have a separate view of the event stream, and to read the stream independently at their own pace and with their own offsets. (Streaming and Batch)</p> <code>eventhubs.startingPosition</code> <code>JSON str</code> <p>The starting position for your Structured Streaming job. If a specific EventPosition is not set for a partition using startingPositions, then we use the EventPosition set in startingPosition. If nothing is set in either option, we will begin consuming from the end of the partition. (Streaming and Batch)</p> <code>eventhubs.endingPosition</code> <code>JSON str</code> <p>(JSON str): The ending position of a batch query. This works the same as startingPosition. (Batch)</p> <code>maxEventsPerTrigger</code> <code>long</code> <p>Rate limit on maximum number of events processed per trigger interval. The specified total number of events will be proportionally split across partitions of different volume. (Stream)</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/eventhub.py</code> <pre><code>class SparkEventhubDestination(DestinationInterface):\n\"\"\"\n    This Spark destination class is used to write batch or streaming data to Eventhubs. Eventhub configurations need to be specified as options in a dictionary.\n    Additionally, there are more optional configurations which can be found [here.](https://github.com/Azure/azure-event-hubs-spark/blob/master/docs/PySpark/structured-streaming-pyspark.md#event-hubs-configuration){ target=\"_blank\" }\n    If using startingPosition or endingPosition make sure to check out **Event Position** section for more details and examples.\n\n    Examples\n    --------\n    ```python\n    #Eventhub Destination for Streaming Queries\n\n    from rtdip_sdk.pipelines.destinations import SparkEventhubDestination\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    connectionString = Endpoint=sb://{NAMESPACE}.servicebus.windows.net/;SharedAccessKeyName={ACCESS_KEY_NAME};SharedAccessKey={ACCESS_KEY}=;EntityPath={EVENT_HUB_NAME}\n\n    eventhub_destination = SparkEventhubDestination(\n        spark=spark,\n        data=df,\n        options={\n            \"eventhubs.connectionString\": connectionString,\n            \"eventhubs.consumerGroup\": \"{YOUR-EVENTHUB-CONSUMER-GROUP}\",\n            \"checkpointLocation\": \"/{CHECKPOINT-LOCATION}/\"\n        },\n        trigger=\"10 seconds\",\n        query_name=\"EventhubDestination\",\n        query_wait_interval=None\n    )\n\n    eventhub_destination.write_stream()\n    ```\n    ```python\n    #Eventhub Destination for Batch Queries\n\n    from rtdip_sdk.pipelines.destinations import SparkEventhubDestination\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    connectionString = Endpoint=sb://{NAMESPACE}.servicebus.windows.net/;SharedAccessKeyName={ACCESS_KEY_NAME};SharedAccessKey={ACCESS_KEY}=;EntityPath={EVENT_HUB_NAME}\n\n\n    eventhub_destination = SparkEventhubDestination(\n        spark=spark,\n        data=df,\n        options={\n            \"eventhubs.connectionString\": connectionString,\n            \"eventhubs.consumerGroup\": \"{YOUR-EVENTHUB-CONSUMER-GROUP}\"\n        },\n        trigger=\"10 seconds\",\n        query_name=\"EventhubDestination\",\n        query_wait_interval=None\n    )\n\n    eventhub_destination.write_batch()\n    ```\n\n    Parameters:\n        spark (SparkSession): Spark Session\n        data (DataFrame): Dataframe to be written to Eventhub\n        options (dict): A dictionary of Eventhub configurations (See Attributes table below). All Configuration options for Eventhubs can be found [here.](https://github.com/Azure/azure-event-hubs-spark/blob/master/docs/PySpark/structured-streaming-pyspark.md#event-hubs-configuration){ target=\"_blank\" }\n        trigger (optional str): Frequency of the write operation. Specify \"availableNow\" to execute a trigger once, otherwise specify a time period such as \"30 seconds\", \"5 minutes\". Set to \"0 seconds\" if you do not want to use a trigger. (stream) Default is 10 seconds\n        query_name (str): Unique name for the query in associated SparkSession\n        query_wait_interval (optional int): If set, waits for the streaming query to complete before returning. (stream) Default is None\n\n    Attributes:\n        checkpointLocation (str): Path to checkpoint files. (Streaming)\n        eventhubs.connectionString (str):  Eventhubs connection string is required to connect to the Eventhubs service. (Streaming and Batch)\n        eventhubs.consumerGroup (str): A consumer group is a view of an entire eventhub. Consumer groups enable multiple consuming applications to each have a separate view of the event stream, and to read the stream independently at their own pace and with their own offsets. (Streaming and Batch)\n        eventhubs.startingPosition (JSON str): The starting position for your Structured Streaming job. If a specific EventPosition is not set for a partition using startingPositions, then we use the EventPosition set in startingPosition. If nothing is set in either option, we will begin consuming from the end of the partition. (Streaming and Batch)\n        eventhubs.endingPosition: (JSON str): The ending position of a batch query. This works the same as startingPosition. (Batch)\n        maxEventsPerTrigger (long): Rate limit on maximum number of events processed per trigger interval. The specified total number of events will be proportionally split across partitions of different volume. (Stream)\n    \"\"\"\n\n    spark: SparkSession\n    data: DataFrame\n    options: dict\n    trigger: str\n    query_name: str\n    query_wait_interval: int\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        data: DataFrame,\n        options: dict,\n        trigger=\"10 seconds\",\n        query_name=\"EventhubDestination\",\n        query_wait_interval: int = None,\n    ) -&gt; None:\n        self.spark = spark\n        self.data = data\n        self.options = options\n        self.trigger = trigger\n        self.query_name = query_name\n        self.query_wait_interval = query_wait_interval\n\n    @staticmethod\n    def system_type():\n\"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        spark_libraries = Libraries()\n        spark_libraries.add_maven_library(get_default_package(\"spark_azure_eventhub\"))\n        return spark_libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_write_validation(self):\n        return True\n\n    def post_write_validation(self):\n        return True\n\n    def prepare_columns(self):\n        if \"body\" in self.data.columns:\n            if self.data.schema[\"body\"].dataType not in [StringType(), BinaryType()]:\n                try:\n                    self.data.withColumn(\"body\", col(\"body\").cast(StringType()))\n                except Exception as e:\n                    raise ValueError(\n                        \"'body' column must be of string or binary type\", e\n                    )\n        else:\n            self.data = self.data.withColumn(\n                \"body\",\n                to_json(\n                    struct(\n                        [\n                            col(column).alias(column)\n                            for column in self.data.columns\n                            if column not in [\"partitionId\", \"partitionKey\"]\n                        ]\n                    )\n                ),\n            )\n        for column in self.data.schema:\n            if (\n                column.name in [\"partitionId\", \"partitionKey\"]\n                and column.dataType != StringType()\n            ):\n                try:\n                    self.data = self.data.withColumn(\n                        column.name, col(column.name).cast(StringType())\n                    )\n                except Exception as e:\n                    raise ValueError(f\"Column {column.name} must be of string type\", e)\n        return self.data.select(\n            [\n                column\n                for column in self.data.columns\n                if column in [\"partitionId\", \"partitionKey\", \"body\"]\n            ]\n        )\n\n    def write_batch(self):\n\"\"\"\n        Writes batch data to Eventhubs.\n        \"\"\"\n        eventhub_connection_string = \"eventhubs.connectionString\"\n        try:\n            if eventhub_connection_string in self.options:\n                sc = self.spark.sparkContext\n                self.options[\n                    eventhub_connection_string\n                ] = sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(\n                    self.options[eventhub_connection_string]\n                )\n            df = self.prepare_columns()\n            return df.write.format(\"eventhubs\").options(**self.options).save()\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def write_stream(self):\n\"\"\"\n        Writes steaming data to Eventhubs.\n        \"\"\"\n        eventhub_connection_string = \"eventhubs.connectionString\"\n        try:\n            TRIGGER_OPTION = (\n                {\"availableNow\": True}\n                if self.trigger == \"availableNow\"\n                else {\"processingTime\": self.trigger}\n            )\n            if eventhub_connection_string in self.options:\n                sc = self.spark.sparkContext\n                self.options[\n                    eventhub_connection_string\n                ] = sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(\n                    self.options[eventhub_connection_string]\n                )\n            df = self.prepare_columns()\n            df = self.data.select(\n                [\n                    column\n                    for column in self.data.columns\n                    if column in [\"partitionId\", \"partitionKey\", \"body\"]\n                ]\n            )\n            query = (\n                df.writeStream.trigger(**TRIGGER_OPTION)\n                .format(\"eventhubs\")\n                .options(**self.options)\n                .queryName(self.query_name)\n                .start()\n            )\n\n            if self.query_wait_interval:\n                while query.isActive:\n                    if query.lastProgress:\n                        logging.info(query.lastProgress)\n                    time.sleep(self.query_wait_interval)\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/eventhub/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.eventhub.SparkEventhubDestination.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/eventhub.py</code> <pre><code>@staticmethod\ndef system_type():\n\"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/eventhub/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.eventhub.SparkEventhubDestination.write_batch","title":"<code>write_batch()</code>","text":"<p>Writes batch data to Eventhubs.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/eventhub.py</code> <pre><code>def write_batch(self):\n\"\"\"\n    Writes batch data to Eventhubs.\n    \"\"\"\n    eventhub_connection_string = \"eventhubs.connectionString\"\n    try:\n        if eventhub_connection_string in self.options:\n            sc = self.spark.sparkContext\n            self.options[\n                eventhub_connection_string\n            ] = sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(\n                self.options[eventhub_connection_string]\n            )\n        df = self.prepare_columns()\n        return df.write.format(\"eventhubs\").options(**self.options).save()\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/eventhub/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.eventhub.SparkEventhubDestination.write_stream","title":"<code>write_stream()</code>","text":"<p>Writes steaming data to Eventhubs.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/eventhub.py</code> <pre><code>def write_stream(self):\n\"\"\"\n    Writes steaming data to Eventhubs.\n    \"\"\"\n    eventhub_connection_string = \"eventhubs.connectionString\"\n    try:\n        TRIGGER_OPTION = (\n            {\"availableNow\": True}\n            if self.trigger == \"availableNow\"\n            else {\"processingTime\": self.trigger}\n        )\n        if eventhub_connection_string in self.options:\n            sc = self.spark.sparkContext\n            self.options[\n                eventhub_connection_string\n            ] = sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(\n                self.options[eventhub_connection_string]\n            )\n        df = self.prepare_columns()\n        df = self.data.select(\n            [\n                column\n                for column in self.data.columns\n                if column in [\"partitionId\", \"partitionKey\", \"body\"]\n            ]\n        )\n        query = (\n            df.writeStream.trigger(**TRIGGER_OPTION)\n            .format(\"eventhubs\")\n            .options(**self.options)\n            .queryName(self.query_name)\n            .start()\n        )\n\n        if self.query_wait_interval:\n            while query.isActive:\n                if query.lastProgress:\n                    logging.info(query.lastProgress)\n                time.sleep(self.query_wait_interval)\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/kafka/","title":"Write to Kafka","text":""},{"location":"sdk/code-reference/pipelines/destinations/spark/kafka/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.kafka.SparkKafkaDestination","title":"<code>SparkKafkaDestination</code>","text":"<p>             Bases: <code>DestinationInterface</code></p> <p>This Spark destination class is used to write batch or streaming data from Kafka. Required and optional configurations can be found in the Attributes tables below.</p> <p>Additionally, there are more optional configurations which can be found here.</p> <p>For compatibility between Spark and Kafka, the columns in the input dataframe are concatenated into one 'value' column of JSON string.</p>"},{"location":"sdk/code-reference/pipelines/destinations/spark/kafka/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.kafka.SparkKafkaDestination--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.destinations import SparkKafkaDestination\n\nkafka_destination = SparkKafkaDestination(\n    data=df,\n    options={\n        \"kafka.bootstrap.servers\": \"host1:port1,host2:port2\"\n    },\n    trigger=\"10 seconds\",\n    query_name=\"KafkaDestination\",\n    query_wait_interval=None\n)\n\nkafka_destination.write_stream()\n\nOR\n\nkafka_destination.write_batch()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Dataframe to be written to Kafka</p> required <code>options</code> <code>dict</code> <p>A dictionary of Kafka configurations (See Attributes tables below). For more information on configuration options see here</p> required <code>trigger</code> <code>optional str</code> <p>Frequency of the write operation. Specify \"availableNow\" to execute a trigger once, otherwise specify a time period such as \"30 seconds\", \"5 minutes\". Set to \"0 seconds\" if you do not want to use a trigger. (stream) Default is 10 seconds</p> <code>'10 seconds'</code> <code>query_name</code> <code>str</code> <p>Unique name for the query in associated SparkSession</p> <code>'KafkaDestination'</code> <code>query_wait_interval</code> <code>optional int</code> <p>If set, waits for the streaming query to complete before returning. (stream) Default is None</p> <code>None</code> <p>The following options must be set for the Kafka destination for both batch and streaming queries.</p> <p>Attributes:</p> Name Type Description <code>kafka.bootstrap.servers</code> <code>A comma-separated list of host\ufe30port</code> <p>The Kafka \"bootstrap.servers\" configuration. (Streaming and Batch)</p> <p>The following configurations are optional:</p> <p>Attributes:</p> Name Type Description <code>topic</code> <code>str</code> <p>Sets the topic that all rows will be written to in Kafka. This option overrides any topic column that may exist in the data. (Streaming and Batch)</p> <code>includeHeaders</code> <code>bool</code> <p>Whether to include the Kafka headers in the row. (Streaming and Batch)</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/kafka.py</code> <pre><code>class SparkKafkaDestination(DestinationInterface):\n\"\"\"\n    This Spark destination class is used to write batch or streaming data from Kafka. Required and optional configurations can be found in the Attributes tables below.\n\n    Additionally, there are more optional configurations which can be found [here.](https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html){ target=\"_blank\" }\n\n    For compatibility between Spark and Kafka, the columns in the input dataframe are concatenated into one 'value' column of JSON string.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.destinations import SparkKafkaDestination\n\n    kafka_destination = SparkKafkaDestination(\n        data=df,\n        options={\n            \"kafka.bootstrap.servers\": \"host1:port1,host2:port2\"\n        },\n        trigger=\"10 seconds\",\n        query_name=\"KafkaDestination\",\n        query_wait_interval=None\n    )\n\n    kafka_destination.write_stream()\n\n    OR\n\n    kafka_destination.write_batch()\n    ```\n\n    Parameters:\n        data (DataFrame): Dataframe to be written to Kafka\n        options (dict): A dictionary of Kafka configurations (See Attributes tables below). For more information on configuration options see [here](https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html){ target=\"_blank\" }\n        trigger (optional str): Frequency of the write operation. Specify \"availableNow\" to execute a trigger once, otherwise specify a time period such as \"30 seconds\", \"5 minutes\". Set to \"0 seconds\" if you do not want to use a trigger. (stream) Default is 10 seconds\n        query_name (str): Unique name for the query in associated SparkSession\n        query_wait_interval (optional int): If set, waits for the streaming query to complete before returning. (stream) Default is None\n\n    The following options must be set for the Kafka destination for both batch and streaming queries.\n\n    Attributes:\n        kafka.bootstrap.servers (A comma-separated list of host\ufe30port): The Kafka \"bootstrap.servers\" configuration. (Streaming and Batch)\n\n    The following configurations are optional:\n\n    Attributes:\n        topic (str):Sets the topic that all rows will be written to in Kafka. This option overrides any topic column that may exist in the data. (Streaming and Batch)\n        includeHeaders (bool): Whether to include the Kafka headers in the row. (Streaming and Batch)\n\n    \"\"\"\n\n    data: DataFrame\n    options: dict\n    trigger: str\n    query_name: str\n    query_wait_interval: int\n\n    def __init__(\n        self,\n        data: DataFrame,\n        options: dict,\n        trigger=\"10 seconds\",\n        query_name=\"KafkaDestination\",\n        query_wait_interval: int = None,\n    ) -&gt; None:\n        self.data = data\n        self.options = options\n        self.trigger = trigger\n        self.query_name = query_name\n        self.query_wait_interval = query_wait_interval\n\n    @staticmethod\n    def system_type():\n\"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        spark_libraries = Libraries()\n        spark_libraries.add_maven_library(get_default_package(\"spark_sql_kafka\"))\n        return spark_libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_write_validation(self):\n        return True\n\n    def post_write_validation(self):\n        return True\n\n    def write_batch(self):\n\"\"\"\n        Writes batch data to Kafka.\n        \"\"\"\n        try:\n            return (\n                self.data.select(to_json(struct(\"*\")).alias(\"value\"))\n                .write.format(\"kafka\")\n                .options(**self.options)\n                .save()\n            )\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def write_stream(self):\n\"\"\"\n        Writes steaming data to Kafka.\n        \"\"\"\n        try:\n            TRIGGER_OPTION = (\n                {\"availableNow\": True}\n                if self.trigger == \"availableNow\"\n                else {\"processingTime\": self.trigger}\n            )\n            query = (\n                self.data.select(to_json(struct(\"*\")).alias(\"value\"))\n                .writeStream.trigger(**TRIGGER_OPTION)\n                .format(\"kafka\")\n                .options(**self.options)\n                .queryName(self.query_name)\n                .start()\n            )\n\n            if self.query_wait_interval:\n                while query.isActive:\n                    if query.lastProgress:\n                        logging.info(query.lastProgress)\n                    time.sleep(self.query_wait_interval)\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/kafka/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.kafka.SparkKafkaDestination.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/kafka.py</code> <pre><code>@staticmethod\ndef system_type():\n\"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/kafka/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.kafka.SparkKafkaDestination.write_batch","title":"<code>write_batch()</code>","text":"<p>Writes batch data to Kafka.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/kafka.py</code> <pre><code>def write_batch(self):\n\"\"\"\n    Writes batch data to Kafka.\n    \"\"\"\n    try:\n        return (\n            self.data.select(to_json(struct(\"*\")).alias(\"value\"))\n            .write.format(\"kafka\")\n            .options(**self.options)\n            .save()\n        )\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/kafka/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.kafka.SparkKafkaDestination.write_stream","title":"<code>write_stream()</code>","text":"<p>Writes steaming data to Kafka.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/kafka.py</code> <pre><code>def write_stream(self):\n\"\"\"\n    Writes steaming data to Kafka.\n    \"\"\"\n    try:\n        TRIGGER_OPTION = (\n            {\"availableNow\": True}\n            if self.trigger == \"availableNow\"\n            else {\"processingTime\": self.trigger}\n        )\n        query = (\n            self.data.select(to_json(struct(\"*\")).alias(\"value\"))\n            .writeStream.trigger(**TRIGGER_OPTION)\n            .format(\"kafka\")\n            .options(**self.options)\n            .queryName(self.query_name)\n            .start()\n        )\n\n        if self.query_wait_interval:\n            while query.isActive:\n                if query.lastProgress:\n                    logging.info(query.lastProgress)\n                time.sleep(self.query_wait_interval)\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/kafka_eventhub/","title":"Write to Eventhub using Kafka","text":""},{"location":"sdk/code-reference/pipelines/destinations/spark/kafka_eventhub/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.kafka_eventhub.SparkKafkaEventhubDestination","title":"<code>SparkKafkaEventhubDestination</code>","text":"<p>             Bases: <code>DestinationInterface</code></p> <p>This Spark Destination class is used to write batch or streaming data to an Eventhub using the Kafka protocol. This enables Eventhubs to be used as a destination in applications like Delta Live Tables or Databricks Serverless Jobs as the Spark Eventhubs JAR is not supported in these scenarios.</p> <p>Default settings will be specified if not provided in the <code>options</code> parameter:</p> <ul> <li><code>kafka.sasl.mechanism</code> will be set to <code>PLAIN</code></li> <li><code>kafka.security.protocol</code> will be set to <code>SASL_SSL</code></li> <li><code>kafka.request.timeout.ms</code> will be set to <code>60000</code></li> <li><code>kafka.session.timeout.ms</code> will be set to <code>60000</code></li> </ul>"},{"location":"sdk/code-reference/pipelines/destinations/spark/kafka_eventhub/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.kafka_eventhub.SparkKafkaEventhubDestination--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.destinations import SparkKafkaEventhubDestination\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\nconnectionString = Endpoint=sb://{NAMESPACE}.servicebus.windows.net/;SharedAccessKeyName={ACCESS_KEY_NAME};SharedAccessKey={ACCESS_KEY}=;EntityPath={EVENT_HUB_NAME}\n\neventhub_destination = SparkKafkaEventhubDestination(\n    spark=spark,\n    data=df,\n    options={\n        \"kafka.bootstrap.servers\": \"host1:port1,host2:port2\"\n    },\n    consumer_group=\"{YOUR-EVENTHUB-CONSUMER-GROUP}\",\n    trigger=\"10 seconds\",\n    query_name=\"KafkaEventhubDestination\",\n    query_wait_interval=None\n)\n\neventhub_destination.write_stream()\n\nOR\n\neventhub_destination.write_batch()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session</p> required <code>data</code> <code>DataFrame</code> <p>Any columns not listed in the required schema here will be merged into a single column named \"value\", or ignored if \"value\" is an existing column</p> required <code>connection_string</code> <code>str</code> <p>Eventhubs connection string is required to connect to the Eventhubs service. This must include the Eventhub name as the <code>EntityPath</code> parameter. Example <code>\"Endpoint=sb://test.servicebus.windows.net/;SharedAccessKeyName=test;SharedAccessKey=test_key;EntityPath=test_eventhub\"</code></p> required <code>options</code> <code>dict</code> <p>A dictionary of Kafka configurations (See Attributes tables below)</p> required <code>consumer_group</code> <code>str</code> <p>The Eventhub consumer group to use for the connection</p> required <code>trigger</code> <code>optional str</code> <p>Frequency of the write operation. Specify \"availableNow\" to execute a trigger once, otherwise specify a time period such as \"30 seconds\", \"5 minutes\". Set to \"0 seconds\" if you do not want to use a trigger. (stream) Default is 10 seconds</p> <code>'10 seconds'</code> <code>query_name</code> <code>optional str</code> <p>Unique name for the query in associated SparkSession</p> <code>'KafkaEventhubDestination'</code> <code>query_wait_interval</code> <code>optional int</code> <p>If set, waits for the streaming query to complete before returning. (stream) Default is None</p> <code>None</code> <p>The following are commonly used parameters that may be included in the options dict. kafka.bootstrap.servers is the only required config. A full list of configs can be found here</p> <p>Attributes:</p> Name Type Description <code>kafka.bootstrap.servers</code> <code>A comma-separated list of host\ufe30port</code> <p>The Kafka \"bootstrap.servers\" configuration. (Streaming and Batch)</p> <code>topic</code> <code>string</code> <p>Required if there is no existing topic column in your DataFrame. Sets the topic that all rows will be written to in Kafka. (Streaming and Batch)</p> <code>includeHeaders</code> <code>bool</code> <p>Determines whether to include the Kafka headers in the row; defaults to False. (Streaming and Batch)</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/kafka_eventhub.py</code> <pre><code>class SparkKafkaEventhubDestination(DestinationInterface):\n\"\"\"\n    This Spark Destination class is used to write batch or streaming data to an Eventhub using the Kafka protocol. This enables Eventhubs to be used as a destination in applications like Delta Live Tables or Databricks Serverless Jobs as the Spark Eventhubs JAR is not supported in these scenarios.\n\n    Default settings will be specified if not provided in the `options` parameter:\n\n    - `kafka.sasl.mechanism` will be set to `PLAIN`\n    - `kafka.security.protocol` will be set to `SASL_SSL`\n    - `kafka.request.timeout.ms` will be set to `60000`\n    - `kafka.session.timeout.ms` will be set to `60000`\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.destinations import SparkKafkaEventhubDestination\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    connectionString = Endpoint=sb://{NAMESPACE}.servicebus.windows.net/;SharedAccessKeyName={ACCESS_KEY_NAME};SharedAccessKey={ACCESS_KEY}=;EntityPath={EVENT_HUB_NAME}\n\n    eventhub_destination = SparkKafkaEventhubDestination(\n        spark=spark,\n        data=df,\n        options={\n            \"kafka.bootstrap.servers\": \"host1:port1,host2:port2\"\n        },\n        consumer_group=\"{YOUR-EVENTHUB-CONSUMER-GROUP}\",\n        trigger=\"10 seconds\",\n        query_name=\"KafkaEventhubDestination\",\n        query_wait_interval=None\n    )\n\n    eventhub_destination.write_stream()\n\n    OR\n\n    eventhub_destination.write_batch()\n    ```\n\n    Parameters:\n        spark (SparkSession): Spark Session\n        data (DataFrame): Any columns not listed in the required schema [here](https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html#writing-data-to-kafka){ target=\"_blank\" } will be merged into a single column named \"value\", or ignored if \"value\" is an existing column\n        connection_string (str): Eventhubs connection string is required to connect to the Eventhubs service. This must include the Eventhub name as the `EntityPath` parameter. Example `\"Endpoint=sb://test.servicebus.windows.net/;SharedAccessKeyName=test;SharedAccessKey=test_key;EntityPath=test_eventhub\"`\n        options (dict): A dictionary of Kafka configurations (See Attributes tables below)\n        consumer_group (str): The Eventhub consumer group to use for the connection\n        trigger (optional str): Frequency of the write operation. Specify \"availableNow\" to execute a trigger once, otherwise specify a time period such as \"30 seconds\", \"5 minutes\". Set to \"0 seconds\" if you do not want to use a trigger. (stream) Default is 10 seconds\n        query_name (optional str): Unique name for the query in associated SparkSession\n        query_wait_interval (optional int): If set, waits for the streaming query to complete before returning. (stream) Default is None\n\n    The following are commonly used parameters that may be included in the options dict. kafka.bootstrap.servers is the only required config. A full list of configs can be found [here](https://kafka.apache.org/documentation/#producerconfigs){ target=\"_blank\" }\n\n    Attributes:\n        kafka.bootstrap.servers (A comma-separated list of host\ufe30port):  The Kafka \"bootstrap.servers\" configuration. (Streaming and Batch)\n        topic (string): Required if there is no existing topic column in your DataFrame. Sets the topic that all rows will be written to in Kafka. (Streaming and Batch)\n        includeHeaders (bool): Determines whether to include the Kafka headers in the row; defaults to False. (Streaming and Batch)\n    \"\"\"\n\n    spark: SparkSession\n    data: DataFrame\n    connection_string: str\n    options: dict\n    consumer_group: str\n    trigger: str\n    query_name: str\n    connection_string_properties: dict\n    query_wait_interval: int\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        data: DataFrame,\n        connection_string: str,\n        options: dict,\n        consumer_group: str,\n        trigger: str = \"10 seconds\",\n        query_name: str = \"KafkaEventhubDestination\",\n        query_wait_interval: int = None,\n    ) -&gt; None:\n        self.spark = spark\n        self.data = data\n        self.connection_string = connection_string\n        self.options = options\n        self.consumer_group = consumer_group\n        self.trigger = trigger\n        self.query_name = query_name\n        self.connection_string_properties = self._parse_connection_string(\n            connection_string\n        )\n        self.options = self._configure_options(options)\n        self.query_wait_interval = query_wait_interval\n\n    @staticmethod\n    def system_type():\n\"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        spark_libraries = Libraries()\n        spark_libraries.add_maven_library(get_default_package(\"spark_sql_kafka\"))\n        return spark_libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_write_validation(self) -&gt; bool:\n        return True\n\n    def post_write_validation(self) -&gt; bool:\n        return True\n\n    # Code is from Azure Eventhub Python SDK. Will import the package if possible with Conda in the  conda-forge channel in the future\n    def _parse_connection_string(self, connection_string: str):\n        conn_settings = [s.split(\"=\", 1) for s in connection_string.split(\";\")]\n        if any(len(tup) != 2 for tup in conn_settings):\n            raise ValueError(\"Connection string is either blank or malformed.\")\n        conn_settings = dict(conn_settings)\n        shared_access_signature = None\n        for key, value in conn_settings.items():\n            if key.lower() == \"sharedaccesssignature\":\n                shared_access_signature = value\n        shared_access_key = conn_settings.get(\"SharedAccessKey\")\n        shared_access_key_name = conn_settings.get(\"SharedAccessKeyName\")\n        if any([shared_access_key, shared_access_key_name]) and not all(\n            [shared_access_key, shared_access_key_name]\n        ):\n            raise ValueError(\n                \"Connection string must have both SharedAccessKeyName and SharedAccessKey.\"\n            )\n        if shared_access_signature is not None and shared_access_key is not None:\n            raise ValueError(\n                \"Only one of the SharedAccessKey or SharedAccessSignature must be present.\"\n            )\n        endpoint = conn_settings.get(\"Endpoint\")\n        if not endpoint:\n            raise ValueError(\"Connection string is either blank or malformed.\")\n        parsed = urlparse(endpoint.rstrip(\"/\"))\n        if not parsed.netloc:\n            raise ValueError(\"Invalid Endpoint on the Connection String.\")\n        namespace = parsed.netloc.strip()\n        properties = {\n            \"fully_qualified_namespace\": namespace,\n            \"endpoint\": endpoint,\n            \"eventhub_name\": conn_settings.get(\"EntityPath\"),\n            \"shared_access_signature\": shared_access_signature,\n            \"shared_access_key_name\": shared_access_key_name,\n            \"shared_access_key\": shared_access_key,\n        }\n        return properties\n\n    def _connection_string_builder(self, properties: dict) -&gt; str:\n        connection_string = \"Endpoint=\" + properties.get(\"endpoint\") + \";\"\n\n        if properties.get(\"shared_access_key\"):\n            connection_string += (\n                \"SharedAccessKey=\" + properties.get(\"shared_access_key\") + \";\"\n            )\n\n        if properties.get(\"shared_access_key_name\"):\n            connection_string += (\n                \"SharedAccessKeyName=\" + properties.get(\"shared_access_key_name\") + \";\"\n            )\n\n        if properties.get(\"shared_access_signature\"):\n            connection_string += (\n                \"SharedAccessSignature=\"\n                + properties.get(\"shared_access_signature\")\n                + \";\"\n            )\n        return connection_string\n\n    def _configure_options(self, options: dict) -&gt; dict:\n        if \"topic\" not in options:\n            options[\"topic\"] = self.connection_string_properties.get(\"eventhub_name\")\n\n        if \"kafka.bootstrap.servers\" not in options:\n            options[\"kafka.bootstrap.servers\"] = (\n                self.connection_string_properties.get(\"fully_qualified_namespace\")\n                + \":9093\"\n            )\n\n        if \"kafka.sasl.mechanism\" not in options:\n            options[\"kafka.sasl.mechanism\"] = \"PLAIN\"\n\n        if \"kafka.security.protocol\" not in options:\n            options[\"kafka.security.protocol\"] = \"SASL_SSL\"\n\n        if \"kafka.sasl.jaas.config\" not in options:\n            kafka_package = \"org.apache.kafka.common.security.plain.PlainLoginModule\"\n            if \"DATABRICKS_RUNTIME_VERSION\" in os.environ:\n                kafka_package = \"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule\"\n            connection_string = self._connection_string_builder(\n                self.connection_string_properties\n            )\n            options[\n                \"kafka.sasl.jaas.config\"\n            ] = '{} required username=\"$ConnectionString\" password=\"{}\";'.format(\n                kafka_package, connection_string\n            )  # NOSONAR\n\n        if \"kafka.request.timeout.ms\" not in options:\n            options[\"kafka.request.timeout.ms\"] = \"60000\"\n\n        if \"kafka.session.timeout.ms\" not in options:\n            options[\"kafka.session.timeout.ms\"] = \"60000\"\n\n        if \"kafka.group.id\" not in options:\n            options[\"kafka.group.id\"] = self.consumer_group\n\n        options[\"includeHeaders\"] = \"true\"\n\n        return options\n\n    def _transform_to_eventhub_schema(self, df: DataFrame) -&gt; DataFrame:\n        column_list = [\"key\", \"headers\", \"topic\", \"partition\"]\n        if \"value\" not in df.columns:\n            df = df.withColumn(\n                \"value\",\n                to_json(\n                    struct(\n                        [\n                            col(column).alias(column)\n                            for column in df.columns\n                            if column not in column_list\n                        ]\n                    )\n                ),\n            )\n        if \"headers\" in df.columns and (\n            df.schema[\"headers\"].dataType.elementType[\"key\"].nullable == True\n            or df.schema[\"headers\"].dataType.elementType[\"value\"].nullable == True\n        ):\n            raise ValueError(\"key and value in the headers column cannot be nullable\")\n\n        return df.select(\n            [\n                column\n                for column in df.columns\n                if column in [\"value\", \"key\", \"headers\", \"topic\", \"partition\"]\n            ]\n        )\n\n    def write_batch(self) -&gt; DataFrame:\n\"\"\"\n        Reads batch data from Kafka.\n        \"\"\"\n        try:\n            df = self._transform_to_eventhub_schema(self.data)\n            df.write.format(\"kafka\").options(**self.options).save()\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def write_stream(self) -&gt; DataFrame:\n\"\"\"\n        Reads streaming data from Kafka.\n        \"\"\"\n        try:\n            df = self._transform_to_eventhub_schema(self.data)\n            TRIGGER_OPTION = (\n                {\"availableNow\": True}\n                if self.trigger == \"availableNow\"\n                else {\"processingTime\": self.trigger}\n            )\n            query = (\n                df.writeStream.trigger(**TRIGGER_OPTION)\n                .format(\"kafka\")\n                .options(**self.options)\n                .queryName(self.query_name)\n                .start()\n            )\n\n            if self.query_wait_interval:\n                while query.isActive:\n                    if query.lastProgress:\n                        logging.info(query.lastProgress)\n                    time.sleep(self.query_wait_interval)\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/kafka_eventhub/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.kafka_eventhub.SparkKafkaEventhubDestination.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/kafka_eventhub.py</code> <pre><code>@staticmethod\ndef system_type():\n\"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/kafka_eventhub/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.kafka_eventhub.SparkKafkaEventhubDestination.write_batch","title":"<code>write_batch()</code>","text":"<p>Reads batch data from Kafka.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/kafka_eventhub.py</code> <pre><code>def write_batch(self) -&gt; DataFrame:\n\"\"\"\n    Reads batch data from Kafka.\n    \"\"\"\n    try:\n        df = self._transform_to_eventhub_schema(self.data)\n        df.write.format(\"kafka\").options(**self.options).save()\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/kafka_eventhub/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.kafka_eventhub.SparkKafkaEventhubDestination.write_stream","title":"<code>write_stream()</code>","text":"<p>Reads streaming data from Kafka.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/kafka_eventhub.py</code> <pre><code>def write_stream(self) -&gt; DataFrame:\n\"\"\"\n    Reads streaming data from Kafka.\n    \"\"\"\n    try:\n        df = self._transform_to_eventhub_schema(self.data)\n        TRIGGER_OPTION = (\n            {\"availableNow\": True}\n            if self.trigger == \"availableNow\"\n            else {\"processingTime\": self.trigger}\n        )\n        query = (\n            df.writeStream.trigger(**TRIGGER_OPTION)\n            .format(\"kafka\")\n            .options(**self.options)\n            .queryName(self.query_name)\n            .start()\n        )\n\n        if self.query_wait_interval:\n            while query.isActive:\n                if query.lastProgress:\n                    logging.info(query.lastProgress)\n                time.sleep(self.query_wait_interval)\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/kinesis/","title":"Write to Kinesis","text":""},{"location":"sdk/code-reference/pipelines/destinations/spark/kinesis/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.kinesis.SparkKinesisDestination","title":"<code>SparkKinesisDestination</code>","text":"<p>             Bases: <code>DestinationInterface</code></p> <p>This Kinesis destination class is used to write batch or streaming data to Kinesis. Kinesis configurations need to be specified as options in a dictionary.</p>"},{"location":"sdk/code-reference/pipelines/destinations/spark/kinesis/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.kinesis.SparkKinesisDestination--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.destinations import SparkKinesisDestination\n\nkinesis_destination = SparkKinesisDestination(\n    data=df,\n    options={\n        \"endpointUrl\": \"https://kinesis.{REGION}.amazonaws.com\",\n        \"awsAccessKey\": \"{YOUR-AWS-ACCESS-KEY}\",\n        \"awsSecretKey\": \"{YOUR-AWS-SECRET-KEY}\",\n        \"streamName\": \"{YOUR-STREAM-NAME}\"\n    },\n    mode=\"update\",\n    trigger=\"10 seconds\",\n    query_name=\"KinesisDestination\",\n    query_wait_interval=None\n)\n\nkinesis_destination.write_stream()\n\nOR\n\nkinesis_destination.write_batch()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Dataframe to be written to Delta</p> required <code>options</code> <code>dict</code> <p>A dictionary of Kinesis configurations (See Attributes table below). All Configuration options for Kinesis can be found here.</p> required <code>mode</code> <code>str</code> <p>Method of writing to Kinesis - append, complete, update</p> <code>'update'</code> <code>trigger</code> <code>optional str</code> <p>Frequency of the write operation. Specify \"availableNow\" to execute a trigger once, otherwise specify a time period such as \"30 seconds\", \"5 minutes\". Set to \"0 seconds\" if you do not want to use a trigger. (stream) Default is 10 seconds</p> <code>'10 seconds'</code> <code>query_name</code> <code>str</code> <p>Unique name for the query in associated SparkSession</p> <code>'KinesisDestination'</code> <code>query_wait_interval</code> <code>optional int</code> <p>If set, waits for the streaming query to complete before returning. (stream) Default is None</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>endpointUrl</code> <code>str</code> <p>Endpoint of the kinesis stream.</p> <code>awsAccessKey</code> <code>str</code> <p>AWS access key.</p> <code>awsSecretKey</code> <code>str</code> <p>AWS secret access key corresponding to the access key.</p> <code>streamName</code> <code>List[str]</code> <p>Name of the streams in Kinesis to write to.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/kinesis.py</code> <pre><code>class SparkKinesisDestination(DestinationInterface):\n\"\"\"\n    This Kinesis destination class is used to write batch or streaming data to Kinesis. Kinesis configurations need to be specified as options in a dictionary.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.destinations import SparkKinesisDestination\n\n    kinesis_destination = SparkKinesisDestination(\n        data=df,\n        options={\n            \"endpointUrl\": \"https://kinesis.{REGION}.amazonaws.com\",\n            \"awsAccessKey\": \"{YOUR-AWS-ACCESS-KEY}\",\n            \"awsSecretKey\": \"{YOUR-AWS-SECRET-KEY}\",\n            \"streamName\": \"{YOUR-STREAM-NAME}\"\n        },\n        mode=\"update\",\n        trigger=\"10 seconds\",\n        query_name=\"KinesisDestination\",\n        query_wait_interval=None\n    )\n\n    kinesis_destination.write_stream()\n\n    OR\n\n    kinesis_destination.write_batch()\n    ```\n\n    Parameters:\n        data (DataFrame): Dataframe to be written to Delta\n        options (dict): A dictionary of Kinesis configurations (See Attributes table below). All Configuration options for Kinesis can be found [here.](https://github.com/qubole/kinesis-sql#kinesis-sink-configuration){ target=\"_blank\" }\n        mode (str): Method of writing to Kinesis - append, complete, update\n        trigger (optional str): Frequency of the write operation. Specify \"availableNow\" to execute a trigger once, otherwise specify a time period such as \"30 seconds\", \"5 minutes\". Set to \"0 seconds\" if you do not want to use a trigger. (stream) Default is 10 seconds\n        query_name (str): Unique name for the query in associated SparkSession\n        query_wait_interval (optional int): If set, waits for the streaming query to complete before returning. (stream) Default is None\n\n    Attributes:\n        endpointUrl (str): Endpoint of the kinesis stream.\n        awsAccessKey (str): AWS access key.\n        awsSecretKey (str): AWS secret access key corresponding to the access key.\n        streamName (List[str]): Name of the streams in Kinesis to write to.\n    \"\"\"\n\n    data: DataFrame\n    options: dict\n    mode: str\n    trigger: str\n    query_name: str\n    query_wait_interval: int\n\n    def __init__(\n        self,\n        data: DataFrame,\n        options: dict,\n        mode: str = \"update\",\n        trigger: str = \"10 seconds\",\n        query_name=\"KinesisDestination\",\n        query_wait_interval: int = None,\n    ) -&gt; None:\n        self.data = data\n        self.options = options\n        self.mode = mode\n        self.trigger = trigger\n        self.query_name = query_name\n        self.query_wait_interval = query_wait_interval\n\n    @staticmethod\n    def system_type():\n\"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK_DATABRICKS\n        \"\"\"\n        return SystemType.PYSPARK_DATABRICKS\n\n    @staticmethod\n    def libraries():\n        spark_libraries = Libraries()\n        return spark_libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_write_validation(self):\n        return True\n\n    def post_write_validation(self):\n        return True\n\n    def write_batch(self):\n\"\"\"\n        Writes batch data to Kinesis.\n        \"\"\"\n        try:\n            return self.data.write.format(\"kinesis\").options(**self.options).save()\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def write_stream(self):\n\"\"\"\n        Writes steaming data to Kinesis.\n        \"\"\"\n        try:\n            TRIGGER_OPTION = (\n                {\"availableNow\": True}\n                if self.trigger == \"availableNow\"\n                else {\"processingTime\": self.trigger}\n            )\n            query = (\n                self.data.writeStream.trigger(**TRIGGER_OPTION)\n                .format(\"kinesis\")\n                .outputMode(self.mode)\n                .options(**self.options)\n                .queryName(self.query_name)\n                .start()\n            )\n\n            if self.query_wait_interval:\n                while query.isActive:\n                    if query.lastProgress:\n                        logging.info(query.lastProgress)\n                    time.sleep(self.query_wait_interval)\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/kinesis/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.kinesis.SparkKinesisDestination.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK_DATABRICKS</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/kinesis.py</code> <pre><code>@staticmethod\ndef system_type():\n\"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK_DATABRICKS\n    \"\"\"\n    return SystemType.PYSPARK_DATABRICKS\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/kinesis/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.kinesis.SparkKinesisDestination.write_batch","title":"<code>write_batch()</code>","text":"<p>Writes batch data to Kinesis.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/kinesis.py</code> <pre><code>def write_batch(self):\n\"\"\"\n    Writes batch data to Kinesis.\n    \"\"\"\n    try:\n        return self.data.write.format(\"kinesis\").options(**self.options).save()\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/kinesis/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.kinesis.SparkKinesisDestination.write_stream","title":"<code>write_stream()</code>","text":"<p>Writes steaming data to Kinesis.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/kinesis.py</code> <pre><code>def write_stream(self):\n\"\"\"\n    Writes steaming data to Kinesis.\n    \"\"\"\n    try:\n        TRIGGER_OPTION = (\n            {\"availableNow\": True}\n            if self.trigger == \"availableNow\"\n            else {\"processingTime\": self.trigger}\n        )\n        query = (\n            self.data.writeStream.trigger(**TRIGGER_OPTION)\n            .format(\"kinesis\")\n            .outputMode(self.mode)\n            .options(**self.options)\n            .queryName(self.query_name)\n            .start()\n        )\n\n        if self.query_wait_interval:\n            while query.isActive:\n                if query.lastProgress:\n                    logging.info(query.lastProgress)\n                time.sleep(self.query_wait_interval)\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/pcdm_latest_to_delta/","title":"Write Process Control Data Model Latest Values to Delta","text":""},{"location":"sdk/code-reference/pipelines/destinations/spark/pcdm_latest_to_delta/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.pcdm_latest_to_delta.SparkPCDMLatestToDeltaDestination","title":"<code>SparkPCDMLatestToDeltaDestination</code>","text":"<p>             Bases: <code>DestinationInterface</code></p> <p>The Process Control Data Model Latest Values written to Delta.</p>"},{"location":"sdk/code-reference/pipelines/destinations/spark/pcdm_latest_to_delta/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.pcdm_latest_to_delta.SparkPCDMLatestToDeltaDestination--example","title":"Example","text":"<p><pre><code>#PCDM Latest To Delta Destination for Streaming Queries\n\nfrom rtdip_sdk.pipelines.destinations import SparkPCDMLatestToDeltaDestination\n\npcdm_latest_to_delta_destination = SparkPCDMLatestToDeltaDestination(\n    data=df,\n    options={\n        \"checkpointLocation\": \"{/CHECKPOINT-LOCATION/}\"\n    },\n    destination=\"{DELTA_TABLE_PATH}\",\n    mode=\"append\",\n    trigger=\"10 seconds\",\n    query_name=\"PCDMLatestToDeltaDestination\",\n    query_wait_interval=None\n)\n\npcdm_latest_to_delta_destination.write_stream()\n</code></pre> <pre><code>#PCDM Latest To Delta Destination for Batch Queries\n\nfrom rtdip_sdk.pipelines.destinations import SparkPCDMLatestToDeltaDestination\n\npcdm_latest_to_delta_destination = SparkPCDMLatestToDeltaDestination(\n    data=df,\n    options={\n        \"maxRecordsPerFile\", \"10000\"\n    },\n    destination=\"{DELTA_TABLE_PATH}\",\n    mode=\"overwrite\",\n    trigger=\"10 seconds\",\n    query_name=\"PCDMLatestToDeltaDestination\",\n    query_wait_interval=None\n)\n\npcdm_latest_to_delta_destination.write_batch()\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Dataframe to be merged into a Delta Table</p> required <code>options</code> <code>dict</code> <p>Options that can be specified for a Delta Table read operation (See Attributes table below). Further information on the options is available for batch and streaming.</p> required <code>destination</code> <code>str</code> <p>Either the name of the Hive Metastore or Unity Catalog Delta Table or the path to the Delta table to store the latest values</p> required <code>mode</code> <code>str</code> <p>Method of writing to Delta Table - append/overwrite (batch), append/complete (stream)</p> <code>None</code> <code>trigger</code> <code>optional str</code> <p>Frequency of the write operation. Specify \"availableNow\" to execute a trigger once, otherwise specify a time period such as \"30 seconds\", \"5 minutes\". Set to \"0 seconds\" if you do not want to use a trigger. (stream) Default is 10 seconds</p> <code>'10 seconds'</code> <code>query_name</code> <code>str</code> <p>Unique name for the query in associated SparkSession</p> <code>'PCDMLatestToDeltaDestination'</code> <code>query_wait_interval</code> <code>optional int</code> <p>If set, waits for the streaming query to complete before returning. (stream) Default is None</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>checkpointLocation</code> <code>str</code> <p>Path to checkpoint files. (Streaming)</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/pcdm_latest_to_delta.py</code> <pre><code>class SparkPCDMLatestToDeltaDestination(DestinationInterface):\n\"\"\"\n    The Process Control Data Model Latest Values written to Delta.\n\n    Example\n    --------\n    ```python\n    #PCDM Latest To Delta Destination for Streaming Queries\n\n    from rtdip_sdk.pipelines.destinations import SparkPCDMLatestToDeltaDestination\n\n    pcdm_latest_to_delta_destination = SparkPCDMLatestToDeltaDestination(\n        data=df,\n        options={\n            \"checkpointLocation\": \"{/CHECKPOINT-LOCATION/}\"\n        },\n        destination=\"{DELTA_TABLE_PATH}\",\n        mode=\"append\",\n        trigger=\"10 seconds\",\n        query_name=\"PCDMLatestToDeltaDestination\",\n        query_wait_interval=None\n    )\n\n    pcdm_latest_to_delta_destination.write_stream()\n    ```\n    ```python\n    #PCDM Latest To Delta Destination for Batch Queries\n\n    from rtdip_sdk.pipelines.destinations import SparkPCDMLatestToDeltaDestination\n\n    pcdm_latest_to_delta_destination = SparkPCDMLatestToDeltaDestination(\n        data=df,\n        options={\n            \"maxRecordsPerFile\", \"10000\"\n        },\n        destination=\"{DELTA_TABLE_PATH}\",\n        mode=\"overwrite\",\n        trigger=\"10 seconds\",\n        query_name=\"PCDMLatestToDeltaDestination\",\n        query_wait_interval=None\n    )\n\n    pcdm_latest_to_delta_destination.write_batch()\n    ```\n\n    Parameters:\n        data (DataFrame): Dataframe to be merged into a Delta Table\n        options (dict): Options that can be specified for a Delta Table read operation (See Attributes table below). Further information on the options is available for [batch](https://docs.delta.io/latest/delta-batch.html#write-to-a-table){ target=\"_blank\" } and [streaming](https://docs.delta.io/latest/delta-streaming.html#delta-table-as-a-sink){ target=\"_blank\" }.\n        destination (str): Either the name of the Hive Metastore or Unity Catalog Delta Table **or** the path to the Delta table to store the latest values\n        mode (str): Method of writing to Delta Table - append/overwrite (batch), append/complete (stream)\n        trigger (optional str): Frequency of the write operation. Specify \"availableNow\" to execute a trigger once, otherwise specify a time period such as \"30 seconds\", \"5 minutes\". Set to \"0 seconds\" if you do not want to use a trigger. (stream) Default is 10 seconds\n        query_name (str): Unique name for the query in associated SparkSession\n        query_wait_interval (optional int): If set, waits for the streaming query to complete before returning. (stream) Default is None\n\n    Attributes:\n        checkpointLocation (str): Path to checkpoint files. (Streaming)\n    \"\"\"\n\n    spark: SparkSession\n    data: DataFrame\n    options: dict\n    destination: str\n    mode: str\n    trigger: str\n    query_name: str\n    query_wait_interval: int\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        data: DataFrame,\n        options: dict,\n        destination: str,\n        mode: str = None,\n        trigger=\"10 seconds\",\n        query_name: str = \"PCDMLatestToDeltaDestination\",\n        query_wait_interval: int = None,\n    ) -&gt; None:\n        self.spark = spark\n        self.data = data\n        self.destination = destination\n        self.options = options\n        self.mode = mode\n        self.trigger = trigger\n        self.query_name = query_name\n        self.query_wait_interval = query_wait_interval\n\n    @staticmethod\n    def system_type():\n\"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        libraries.add_maven_library(get_default_package(\"spark_delta_core\"))\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_write_validation(self):\n        return True\n\n    def post_write_validation(self):\n        return True\n\n    def _write_latest_to_delta(self, df: DataFrame, epoch_id=None):  # NOSONAR\n        df.persist()\n\n        latest_df = (\n            df.withColumn(\n                \"Latest\",\n                max(struct(\"EventTime\", \"Status\")).over(Window.partitionBy(\"TagName\")),\n            )\n            .withColumn(\n                \"GoodLatest\",\n                when(\n                    col(\"Latest.Status\") == \"Good\",\n                    struct(col(\"EventTime\"), col(\"Value\"), col(\"ValueType\")),\n                ).otherwise(\n                    max(\n                        when(\n                            col(\"Status\") == \"Good\",\n                            struct(\"EventTime\", \"Value\", \"ValueType\"),\n                        )\n                    ).over(Window.partitionBy(\"TagName\"))\n                ),\n            )\n            .filter(col(\"EventTime\") == col(\"Latest.EventTime\"))\n            .drop(\"Latest\")\n            .dropDuplicates([\"TagName\"])\n        )\n\n        when_matched_update_list = [\n            DeltaMergeConditionValues(\n                condition=\"source.EventTime &gt; target.EventTime AND (source.GoodLatest.EventTime IS NULL OR source.GoodLatest.EventTime &lt;= target.GoodEventTime)\",\n                values={\n                    \"EventTime\": \"source.EventTime\",\n                    \"Status\": \"source.Status\",\n                    \"Value\": \"source.Value\",\n                    \"ValueType\": \"source.ValueType\",\n                },\n            ),\n            DeltaMergeConditionValues(\n                condition=\"source.EventTime &gt; target.EventTime AND source.GoodLatest.EventTime IS NOT NULL AND source.GoodLatest.EventTime &gt; target.GoodEventTime\",\n                values={\n                    \"EventTime\": \"source.EventTime\",\n                    \"Status\": \"source.Status\",\n                    \"Value\": \"source.Value\",\n                    \"ValueType\": \"source.ValueType\",\n                    \"GoodEventTime\": \"source.GoodLatest.EventTime\",\n                    \"GoodValue\": \"source.GoodLatest.Value\",\n                    \"GoodValueType\": \"source.GoodLatest.ValueType\",\n                },\n            ),\n            DeltaMergeConditionValues(\n                condition=\"source.EventTime &lt;= target.EventTime AND source.GoodLatest.EventTime IS NOT NULL AND source.GoodLatest.EventTime &gt; target.GoodEventTime\",\n                values={\n                    \"GoodEventTime\": \"source.GoodLatest.EventTime\",\n                    \"GoodValue\": \"source.GoodLatest.Value\",\n                    \"GoodValueType\": \"source.GoodLatest.ValueType\",\n                },\n            ),\n        ]\n\n        when_not_matched_insert_list = [\n            DeltaMergeConditionValues(\n                values={\n                    \"TagName\": \"source.TagName\",\n                    \"EventTime\": \"source.EventTime\",\n                    \"Status\": \"source.Status\",\n                    \"Value\": \"source.Value\",\n                    \"ValueType\": \"source.ValueType\",\n                    \"GoodEventTime\": \"source.GoodLatest.EventTime\",\n                    \"GoodValue\": \"source.GoodLatest.Value\",\n                    \"GoodValueType\": \"source.GoodLatest.ValueType\",\n                },\n            )\n        ]\n\n        merge_condition = \"source.TagName = target.TagName\"\n\n        SparkDeltaMergeDestination(\n            spark=self.spark,\n            data=latest_df,\n            destination=self.destination,\n            options=self.options,\n            merge_condition=merge_condition,\n            when_matched_update_list=when_matched_update_list,\n            when_not_matched_insert_list=when_not_matched_insert_list,\n            trigger=self.trigger,\n            query_name=self.query_name,\n        ).write_batch()\n\n        df.unpersist()\n\n    def write_batch(self):\n\"\"\"\n        Writes Process Control Data Model data to Delta\n        \"\"\"\n        try:\n            self._write_latest_to_delta(self.data)\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def write_stream(self):\n\"\"\"\n        Writes streaming Process Control Data Model data to Delta using foreachBatch\n        \"\"\"\n        try:\n            TRIGGER_OPTION = (\n                {\"availableNow\": True}\n                if self.trigger == \"availableNow\"\n                else {\"processingTime\": self.trigger}\n            )\n\n            query = (\n                self.data.writeStream.trigger(**TRIGGER_OPTION)\n                .format(\"delta\")\n                .foreachBatch(self._write_latest_to_delta)\n                .queryName(self.query_name)\n                .outputMode(\"append\")\n                .options(**self.options)\n                .start()\n            )\n\n            if self.query_wait_interval:\n                while query.isActive:\n                    if query.lastProgress:\n                        logging.info(query.lastProgress)\n                    time.sleep(self.query_wait_interval)\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/pcdm_latest_to_delta/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.pcdm_latest_to_delta.SparkPCDMLatestToDeltaDestination.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/pcdm_latest_to_delta.py</code> <pre><code>@staticmethod\ndef system_type():\n\"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/pcdm_latest_to_delta/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.pcdm_latest_to_delta.SparkPCDMLatestToDeltaDestination.write_batch","title":"<code>write_batch()</code>","text":"<p>Writes Process Control Data Model data to Delta</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/pcdm_latest_to_delta.py</code> <pre><code>def write_batch(self):\n\"\"\"\n    Writes Process Control Data Model data to Delta\n    \"\"\"\n    try:\n        self._write_latest_to_delta(self.data)\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/pcdm_latest_to_delta/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.pcdm_latest_to_delta.SparkPCDMLatestToDeltaDestination.write_stream","title":"<code>write_stream()</code>","text":"<p>Writes streaming Process Control Data Model data to Delta using foreachBatch</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/pcdm_latest_to_delta.py</code> <pre><code>def write_stream(self):\n\"\"\"\n    Writes streaming Process Control Data Model data to Delta using foreachBatch\n    \"\"\"\n    try:\n        TRIGGER_OPTION = (\n            {\"availableNow\": True}\n            if self.trigger == \"availableNow\"\n            else {\"processingTime\": self.trigger}\n        )\n\n        query = (\n            self.data.writeStream.trigger(**TRIGGER_OPTION)\n            .format(\"delta\")\n            .foreachBatch(self._write_latest_to_delta)\n            .queryName(self.query_name)\n            .outputMode(\"append\")\n            .options(**self.options)\n            .start()\n        )\n\n        if self.query_wait_interval:\n            while query.isActive:\n                if query.lastProgress:\n                    logging.info(query.lastProgress)\n                time.sleep(self.query_wait_interval)\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/pcdm_to_delta/","title":"Write Process Control Data Model to Delta","text":""},{"location":"sdk/code-reference/pipelines/destinations/spark/pcdm_to_delta/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.pcdm_to_delta.SparkPCDMToDeltaDestination","title":"<code>SparkPCDMToDeltaDestination</code>","text":"<p>             Bases: <code>DestinationInterface</code></p> <p>The Process Control Data Model written to Delta.</p>"},{"location":"sdk/code-reference/pipelines/destinations/spark/pcdm_to_delta/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.pcdm_to_delta.SparkPCDMToDeltaDestination--example","title":"Example","text":"<p><pre><code>#PCDM Latest To Delta Destination for Streaming Queries\n\nfrom rtdip_sdk.pipelines.destinations import SparkPCDMToDeltaDestination\n\npcdm_to_delta_destination = SparkPCDMToDeltaDestination(\n    data=df,\n    options={\n        \"checkpointLocation\": \"{/CHECKPOINT-LOCATION/}\"\n    },\n    destination_float=\"{DELTA_TABLE_PATH_FLOAT}\",\n    destination_string=\"{DELTA_TABLE_PATH_STRING}\",\n    destination_integer=\"{DELTA_TABLE_PATH_INTEGER}\",\n    mode=\"append\",\n    trigger=\"10 seconds\",\n    query_name=\"PCDMToDeltaDestination\",\n    query_wait_interval=None,\n    merge=True,\n    try_broadcast_join=False,\n    remove_nanoseconds=False,\n    remove_duplicates-True\n)\n\npcdm_to_delta_destination.write_stream()\n</code></pre> <pre><code>#PCDM Latest To Delta Destination for Batch Queries\n\nfrom rtdip_sdk.pipelines.destinations import SparkPCDMToDeltaDestination\n\npcdm_to_delta_destination = SparkPCDMToDeltaDestination(\n    data=df,\n    options={\n        \"maxRecordsPerFile\", \"10000\"\n    },\n    destination_float=\"{DELTA_TABLE_PATH_FLOAT}\",\n    destination_string=\"{DELTA_TABLE_PATH_STRING}\",\n    destination_integer=\"{DELTA_TABLE_PATH_INTEGER}\",\n    mode=\"overwrite\",\n    trigger=\"10 seconds\",\n    query_name=\"PCDMToDeltaDestination\",\n    query_wait_interval=None,\n    merge=True,\n    try_broadcast_join=False,\n    remove_nanoseconds=False,\n    remove_duplicates-True\n)\n\npcdm_to_delta_destination.write_batch()\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Dataframe to be merged into a Delta Table</p> required <code>options</code> <code>dict</code> <p>Options that can be specified for a Delta Table read operation (See Attributes table below). Further information on the options is available for batch and streaming.</p> required <code>destination_float</code> <code>str</code> <p>Either the name of the Hive Metastore or Unity Catalog Delta Table or the path to the Delta table to store float values.</p> required <code>destination_string</code> <code>Optional str</code> <p>Either the name of the Hive Metastore or Unity Catalog Delta Table or the path to the Delta table to store string values.</p> <code>None</code> <code>destination_integer</code> <code>Optional str</code> <p>Either the name of the Hive Metastore or Unity Catalog Delta Table or the path to the Delta table to store integer values</p> <code>None</code> <code>mode</code> <code>str</code> <p>Method of writing to Delta Table - append/overwrite (batch), append/complete (stream)</p> <code>None</code> <code>trigger</code> <code>optional str</code> <p>Frequency of the write operation. Specify \"availableNow\" to execute a trigger once, otherwise specify a time period such as \"30 seconds\", \"5 minutes\". Set to \"0 seconds\" if you do not want to use a trigger. (stream) Default is 10 seconds</p> <code>'10 seconds'</code> <code>query_name</code> <code>str</code> <p>Unique name for the query in associated SparkSession</p> <code>'PCDMToDeltaDestination'</code> <code>query_wait_interval</code> <code>optional int</code> <p>If set, waits for the streaming query to complete before returning. (stream) Default is None</p> <code>None</code> <code>merge</code> <code>bool</code> <p>Use Delta Merge to perform inserts, updates and deletes</p> <code>True</code> <code>try_broadcast_join</code> <code>bool</code> <p>Attempts to perform a broadcast join in the merge which can leverage data skipping using partition pruning and file pruning automatically. Can fail if dataframe being merged is large and therefore more suitable for streaming merges than batch merges</p> <code>False</code> <code>remove_nanoseconds</code> <code>bool</code> <p>Removes nanoseconds from the EventTime column and replaces with zeros</p> <code>False</code> <code>remove_duplicates</code> <code>bool</code> <p>Removes duplicates before writing the data</p> <code>True</code> <p>Attributes:</p> Name Type Description <code>checkpointLocation</code> <code>str</code> <p>Path to checkpoint files. (Streaming)</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/pcdm_to_delta.py</code> <pre><code>class SparkPCDMToDeltaDestination(DestinationInterface):\n\"\"\"\n    The Process Control Data Model written to Delta.\n\n    Example\n    --------\n    ```python\n    #PCDM Latest To Delta Destination for Streaming Queries\n\n    from rtdip_sdk.pipelines.destinations import SparkPCDMToDeltaDestination\n\n    pcdm_to_delta_destination = SparkPCDMToDeltaDestination(\n        data=df,\n        options={\n            \"checkpointLocation\": \"{/CHECKPOINT-LOCATION/}\"\n        },\n        destination_float=\"{DELTA_TABLE_PATH_FLOAT}\",\n        destination_string=\"{DELTA_TABLE_PATH_STRING}\",\n        destination_integer=\"{DELTA_TABLE_PATH_INTEGER}\",\n        mode=\"append\",\n        trigger=\"10 seconds\",\n        query_name=\"PCDMToDeltaDestination\",\n        query_wait_interval=None,\n        merge=True,\n        try_broadcast_join=False,\n        remove_nanoseconds=False,\n        remove_duplicates-True\n    )\n\n    pcdm_to_delta_destination.write_stream()\n    ```\n    ```python\n    #PCDM Latest To Delta Destination for Batch Queries\n\n    from rtdip_sdk.pipelines.destinations import SparkPCDMToDeltaDestination\n\n    pcdm_to_delta_destination = SparkPCDMToDeltaDestination(\n        data=df,\n        options={\n            \"maxRecordsPerFile\", \"10000\"\n        },\n        destination_float=\"{DELTA_TABLE_PATH_FLOAT}\",\n        destination_string=\"{DELTA_TABLE_PATH_STRING}\",\n        destination_integer=\"{DELTA_TABLE_PATH_INTEGER}\",\n        mode=\"overwrite\",\n        trigger=\"10 seconds\",\n        query_name=\"PCDMToDeltaDestination\",\n        query_wait_interval=None,\n        merge=True,\n        try_broadcast_join=False,\n        remove_nanoseconds=False,\n        remove_duplicates-True\n    )\n\n    pcdm_to_delta_destination.write_batch()\n    ```\n\n    Parameters:\n        data (DataFrame): Dataframe to be merged into a Delta Table\n        options (dict): Options that can be specified for a Delta Table read operation (See Attributes table below). Further information on the options is available for [batch](https://docs.delta.io/latest/delta-batch.html#write-to-a-table){ target=\"_blank\" } and [streaming](https://docs.delta.io/latest/delta-streaming.html#delta-table-as-a-sink){ target=\"_blank\" }.\n        destination_float (str): Either the name of the Hive Metastore or Unity Catalog Delta Table **or** the path to the Delta table to store float values.\n        destination_string (Optional str): Either the name of the Hive Metastore or Unity Catalog Delta Table **or** the path to the Delta table to store string values.\n        destination_integer (Optional str): Either the name of the Hive Metastore or Unity Catalog Delta Table **or** the path to the Delta table to store integer values\n        mode (str): Method of writing to Delta Table - append/overwrite (batch), append/complete (stream)\n        trigger (optional str): Frequency of the write operation. Specify \"availableNow\" to execute a trigger once, otherwise specify a time period such as \"30 seconds\", \"5 minutes\". Set to \"0 seconds\" if you do not want to use a trigger. (stream) Default is 10 seconds\n        query_name (str): Unique name for the query in associated SparkSession\n        query_wait_interval (optional int): If set, waits for the streaming query to complete before returning. (stream) Default is None\n        merge (bool): Use Delta Merge to perform inserts, updates and deletes\n        try_broadcast_join (bool): Attempts to perform a broadcast join in the merge which can leverage data skipping using partition pruning and file pruning automatically. Can fail if dataframe being merged is large and therefore more suitable for streaming merges than batch merges\n        remove_nanoseconds (bool): Removes nanoseconds from the EventTime column and replaces with zeros\n        remove_duplicates (bool: Removes duplicates before writing the data\n\n    Attributes:\n        checkpointLocation (str): Path to checkpoint files. (Streaming)\n    \"\"\"\n\n    spark: SparkSession\n    data: DataFrame\n    options: dict\n    destination_float: str\n    destination_string: str\n    destination_integer: str\n    mode: str\n    trigger: str\n    query_name: str\n    query_wait_interval: int\n    merge: bool\n    try_broadcast_join: bool\n    remove_nanoseconds: bool\n    remove_duplicates: bool\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        data: DataFrame,\n        options: dict,\n        destination_float: str,\n        destination_string: str = None,\n        destination_integer: str = None,\n        mode: str = None,\n        trigger=\"10 seconds\",\n        query_name: str = \"PCDMToDeltaDestination\",\n        query_wait_interval: int = None,\n        merge: bool = True,\n        try_broadcast_join=False,\n        remove_nanoseconds: bool = False,\n        remove_duplicates: bool = True,\n    ) -&gt; None:\n        self.spark = spark\n        self.data = data\n        self.destination_float = destination_float\n        self.destination_string = destination_string\n        self.destination_integer = destination_integer\n        self.options = options\n        self.mode = mode\n        self.trigger = trigger\n        self.query_name = query_name\n        self.query_wait_interval = query_wait_interval\n        self.merge = merge\n        self.try_broadcast_join = try_broadcast_join\n        self.remove_nanoseconds = remove_nanoseconds\n        self.remove_duplicates = remove_duplicates\n\n    @staticmethod\n    def system_type():\n\"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        libraries.add_maven_library(get_default_package(\"spark_delta_core\"))\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_write_validation(self):\n        return True\n\n    def post_write_validation(self):\n        return True\n\n    def _get_eventdate_string(self, df: DataFrame) -&gt; str:\n        dates_df = df.select(\"EventDate\").distinct()\n        dates_df = dates_df.select(\n            date_format(\"EventDate\", \"yyyy-MM-dd\").alias(\"EventDate\")\n        )\n        dates_list = list(dates_df.toPandas()[\"EventDate\"])\n        return str(dates_list).replace(\"[\", \"\").replace(\"]\", \"\")\n\n    def _write_delta_merge(self, df: DataFrame, destination: str):\n        df = df.select(\n            \"EventDate\", \"TagName\", \"EventTime\", \"Status\", \"Value\", \"ChangeType\"\n        )\n        when_matched_update_list = [\n            DeltaMergeConditionValues(\n                condition=\"(source.ChangeType IN ('insert', 'update', 'upsert')) AND ((source.Status != target.Status) OR (source.Value != target.Value))\",\n                values={\n                    \"EventDate\": \"source.EventDate\",\n                    \"TagName\": \"source.TagName\",\n                    \"EventTime\": \"source.EventTime\",\n                    \"Status\": \"source.Status\",\n                    \"Value\": \"source.Value\",\n                },\n            )\n        ]\n        when_matched_delete_list = [\n            DeltaMergeCondition(condition=\"source.ChangeType = 'delete'\")\n        ]\n        when_not_matched_insert_list = [\n            DeltaMergeConditionValues(\n                condition=\"(source.ChangeType IN ('insert', 'update', 'upsert'))\",\n                values={\n                    \"EventDate\": \"source.EventDate\",\n                    \"TagName\": \"source.TagName\",\n                    \"EventTime\": \"source.EventTime\",\n                    \"Status\": \"source.Status\",\n                    \"Value\": \"source.Value\",\n                },\n            )\n        ]\n\n        merge_condition = \"source.EventDate = target.EventDate AND source.TagName = target.TagName AND source.EventTime = target.EventTime\"\n\n        perform_merge = True\n        if self.try_broadcast_join != True:\n            eventdate_string = self._get_eventdate_string(df)\n            if eventdate_string == None or eventdate_string == \"\":\n                perform_merge = False\n            else:\n                merge_condition = (\n                    \"target.EventDate in ({}) AND \".format(eventdate_string)\n                    + merge_condition\n                )\n\n        if perform_merge == True:\n            SparkDeltaMergeDestination(\n                spark=self.spark,\n                data=df,\n                destination=destination,\n                options=self.options,\n                merge_condition=merge_condition,\n                when_matched_update_list=when_matched_update_list,\n                when_matched_delete_list=when_matched_delete_list,\n                when_not_matched_insert_list=when_not_matched_insert_list,\n                try_broadcast_join=self.try_broadcast_join,\n                trigger=self.trigger,\n                query_name=self.query_name,\n            ).write_batch()\n\n    def _write_delta_batch(self, df: DataFrame, destination: str):\n        if self.merge == True:\n            if \"EventDate\" not in df.columns:\n                df = df.withColumn(\"EventDate\", date_format(\"EventTime\", \"yyyy-MM-dd\"))\n\n            self._write_delta_merge(\n                df.filter(col(\"ChangeType\").isin(\"insert\", \"update\", \"upsert\")),\n                destination,\n            )\n            self._write_delta_merge(\n                df.filter(col(\"ChangeType\") == \"delete\"), destination\n            )\n        else:\n            df = df.select(\"TagName\", \"EventTime\", \"Status\", \"Value\")\n            SparkDeltaDestination(\n                data=df,\n                destination=destination,\n                options=self.options,\n                mode=self.mode,\n                trigger=self.trigger,\n                query_name=self.query_name,\n            ).write_batch()\n\n    def _write_data_by_type(self, df: DataFrame):\n        if self.merge == True:\n            df = df.withColumn(\n                \"ChangeType\",\n                when(df[\"ChangeType\"].isin(\"insert\", \"update\"), \"upsert\").otherwise(\n                    df[\"ChangeType\"]\n                ),\n            )\n\n        if self.remove_nanoseconds == True:\n            df = df.withColumn(\n                \"EventTime\",\n                (floor(col(\"EventTime\").cast(\"double\") * 1000) / 1000).cast(\n                    \"timestamp\"\n                ),\n            )\n\n        if self.remove_duplicates == True:\n            df = df.drop_duplicates([\"TagName\", \"EventTime\", \"ChangeType\"])\n\n        float_df = df.filter(ValueTypeConstants.FLOAT_VALUE).withColumn(\n            \"Value\", col(\"Value\").cast(\"float\")\n        )\n        self._write_delta_batch(float_df, self.destination_float)\n\n        if self.destination_string != None:\n            string_df = df.filter(ValueTypeConstants.STRING_VALUE)\n            self._write_delta_batch(string_df, self.destination_string)\n\n        if self.destination_integer != None:\n            integer_df = df.filter(ValueTypeConstants.INTEGER_VALUE).withColumn(\n                \"Value\", col(\"Value\").cast(\"integer\")\n            )\n            self._write_delta_batch(integer_df, self.destination_integer)\n\n    def _write_stream_microbatches(self, df: DataFrame, epoch_id=None):  # NOSONAR\n        df.persist()\n        self._write_data_by_type(df)\n        df.unpersist()\n\n    def write_batch(self):\n\"\"\"\n        Writes Process Control Data Model data to Delta\n        \"\"\"\n        try:\n            if self.try_broadcast_join != True:\n                self.data.persist()\n\n            self._write_data_by_type(self.data)\n\n            if self.try_broadcast_join != True:\n                self.data.unpersist()\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def write_stream(self):\n\"\"\"\n        Writes streaming Process Control Data Model data to Delta using foreachBatch\n        \"\"\"\n        try:\n            TRIGGER_OPTION = (\n                {\"availableNow\": True}\n                if self.trigger == \"availableNow\"\n                else {\"processingTime\": self.trigger}\n            )\n            if self.merge == True:\n                query = (\n                    self.data.writeStream.trigger(**TRIGGER_OPTION)\n                    .format(\"delta\")\n                    .foreachBatch(self._write_stream_microbatches)\n                    .queryName(self.query_name)\n                    .outputMode(\"update\")\n                    .options(**self.options)\n                    .start()\n                )\n            else:\n                default_checkpoint_location = None\n                float_checkpoint_location = None\n                string_checkpoint_location = None\n                integer_checkpoint_location = None\n\n                append_options = self.options.copy()\n                if \"checkpointLocation\" in self.options:\n                    default_checkpoint_location = self.options[\"checkpointLocation\"]\n                    if default_checkpoint_location[-1] != \"/\":\n                        default_checkpoint_location += \"/\"\n                    float_checkpoint_location = default_checkpoint_location + \"float\"\n                    string_checkpoint_location = default_checkpoint_location + \"string\"\n                    integer_checkpoint_location = (\n                        default_checkpoint_location + \"integer\"\n                    )\n\n                if float_checkpoint_location is not None:\n                    append_options[\"checkpointLocation\"] = float_checkpoint_location\n\n                delta_float = SparkDeltaDestination(\n                    data=self.data.select(\"TagName\", \"EventTime\", \"Status\", \"Value\")\n                    .filter(ValueTypeConstants.FLOAT_VALUE)\n                    .withColumn(\"Value\", col(\"Value\").cast(\"float\")),\n                    destination=self.destination_float,\n                    options=append_options,\n                    mode=self.mode,\n                    trigger=self.trigger,\n                    query_name=self.query_name + \"_float\",\n                )\n\n                delta_float.write_stream()\n\n                if self.destination_string != None:\n                    if string_checkpoint_location is not None:\n                        append_options[\n                            \"checkpointLocation\"\n                        ] = string_checkpoint_location\n\n                    delta_string = SparkDeltaDestination(\n                        data=self.data.select(\n                            \"TagName\", \"EventTime\", \"Status\", \"Value\"\n                        ).filter(ValueTypeConstants.STRING_VALUE),\n                        destination=self.destination_string,\n                        options=append_options,\n                        mode=self.mode,\n                        trigger=self.trigger,\n                        query_name=self.query_name + \"_string\",\n                    )\n\n                    delta_string.write_stream()\n\n                if self.destination_integer != None:\n                    if integer_checkpoint_location is not None:\n                        append_options[\n                            \"checkpointLocation\"\n                        ] = integer_checkpoint_location\n\n                    delta_integer = SparkDeltaDestination(\n                        data=self.data.select(\"TagName\", \"EventTime\", \"Status\", \"Value\")\n                        .filter(ValueTypeConstants.INTEGER_VALUE)\n                        .withColumn(\"Value\", col(\"Value\").cast(\"integer\")),\n                        destination=self.destination_integer,\n                        options=append_options,\n                        mode=self.mode,\n                        trigger=self.trigger,\n                        query_name=self.query_name + \"_integer\",\n                    )\n\n                    delta_integer.write_stream()\n\n                if self.query_wait_interval:\n                    while self.spark.streams.active != []:\n                        for query in self.spark.streams.active:\n                            if query.lastProgress:\n                                logging.info(\n                                    \"{}: {}\".format(query.name, query.lastProgress)\n                                )\n                        time.sleep(self.query_wait_interval)\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/pcdm_to_delta/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.pcdm_to_delta.SparkPCDMToDeltaDestination.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/pcdm_to_delta.py</code> <pre><code>@staticmethod\ndef system_type():\n\"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/pcdm_to_delta/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.pcdm_to_delta.SparkPCDMToDeltaDestination.write_batch","title":"<code>write_batch()</code>","text":"<p>Writes Process Control Data Model data to Delta</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/pcdm_to_delta.py</code> <pre><code>def write_batch(self):\n\"\"\"\n    Writes Process Control Data Model data to Delta\n    \"\"\"\n    try:\n        if self.try_broadcast_join != True:\n            self.data.persist()\n\n        self._write_data_by_type(self.data)\n\n        if self.try_broadcast_join != True:\n            self.data.unpersist()\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/pcdm_to_delta/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.pcdm_to_delta.SparkPCDMToDeltaDestination.write_stream","title":"<code>write_stream()</code>","text":"<p>Writes streaming Process Control Data Model data to Delta using foreachBatch</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/pcdm_to_delta.py</code> <pre><code>def write_stream(self):\n\"\"\"\n    Writes streaming Process Control Data Model data to Delta using foreachBatch\n    \"\"\"\n    try:\n        TRIGGER_OPTION = (\n            {\"availableNow\": True}\n            if self.trigger == \"availableNow\"\n            else {\"processingTime\": self.trigger}\n        )\n        if self.merge == True:\n            query = (\n                self.data.writeStream.trigger(**TRIGGER_OPTION)\n                .format(\"delta\")\n                .foreachBatch(self._write_stream_microbatches)\n                .queryName(self.query_name)\n                .outputMode(\"update\")\n                .options(**self.options)\n                .start()\n            )\n        else:\n            default_checkpoint_location = None\n            float_checkpoint_location = None\n            string_checkpoint_location = None\n            integer_checkpoint_location = None\n\n            append_options = self.options.copy()\n            if \"checkpointLocation\" in self.options:\n                default_checkpoint_location = self.options[\"checkpointLocation\"]\n                if default_checkpoint_location[-1] != \"/\":\n                    default_checkpoint_location += \"/\"\n                float_checkpoint_location = default_checkpoint_location + \"float\"\n                string_checkpoint_location = default_checkpoint_location + \"string\"\n                integer_checkpoint_location = (\n                    default_checkpoint_location + \"integer\"\n                )\n\n            if float_checkpoint_location is not None:\n                append_options[\"checkpointLocation\"] = float_checkpoint_location\n\n            delta_float = SparkDeltaDestination(\n                data=self.data.select(\"TagName\", \"EventTime\", \"Status\", \"Value\")\n                .filter(ValueTypeConstants.FLOAT_VALUE)\n                .withColumn(\"Value\", col(\"Value\").cast(\"float\")),\n                destination=self.destination_float,\n                options=append_options,\n                mode=self.mode,\n                trigger=self.trigger,\n                query_name=self.query_name + \"_float\",\n            )\n\n            delta_float.write_stream()\n\n            if self.destination_string != None:\n                if string_checkpoint_location is not None:\n                    append_options[\n                        \"checkpointLocation\"\n                    ] = string_checkpoint_location\n\n                delta_string = SparkDeltaDestination(\n                    data=self.data.select(\n                        \"TagName\", \"EventTime\", \"Status\", \"Value\"\n                    ).filter(ValueTypeConstants.STRING_VALUE),\n                    destination=self.destination_string,\n                    options=append_options,\n                    mode=self.mode,\n                    trigger=self.trigger,\n                    query_name=self.query_name + \"_string\",\n                )\n\n                delta_string.write_stream()\n\n            if self.destination_integer != None:\n                if integer_checkpoint_location is not None:\n                    append_options[\n                        \"checkpointLocation\"\n                    ] = integer_checkpoint_location\n\n                delta_integer = SparkDeltaDestination(\n                    data=self.data.select(\"TagName\", \"EventTime\", \"Status\", \"Value\")\n                    .filter(ValueTypeConstants.INTEGER_VALUE)\n                    .withColumn(\"Value\", col(\"Value\").cast(\"integer\")),\n                    destination=self.destination_integer,\n                    options=append_options,\n                    mode=self.mode,\n                    trigger=self.trigger,\n                    query_name=self.query_name + \"_integer\",\n                )\n\n                delta_integer.write_stream()\n\n            if self.query_wait_interval:\n                while self.spark.streams.active != []:\n                    for query in self.spark.streams.active:\n                        if query.lastProgress:\n                            logging.info(\n                                \"{}: {}\".format(query.name, query.lastProgress)\n                            )\n                    time.sleep(self.query_wait_interval)\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/rest_api/","title":"Write to Rest API","text":""},{"location":"sdk/code-reference/pipelines/destinations/spark/rest_api/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.rest_api.SparkRestAPIDestination","title":"<code>SparkRestAPIDestination</code>","text":"<p>             Bases: <code>DestinationInterface</code></p> <p>The Spark Rest API Destination is used to write data to a Rest API.</p> <p>The payload sent to the API is constructed by converting each row in the DataFrame to Json.</p> <p>Note</p> <p>While it is possible to use the <code>write_batch</code> method, it is easy to overwhlem a Rest API with large volumes of data. Consider reducing data volumes when writing to a Rest API in Batch mode to prevent API errors including throtting.</p>"},{"location":"sdk/code-reference/pipelines/destinations/spark/rest_api/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.rest_api.SparkRestAPIDestination--example","title":"Example","text":"<p><pre><code>#Rest API Destination for Streaming Queries\n\nfrom rtdip_sdk.pipelines.destinations import SparkRestAPIDestination\n\nrest_api_destination = SparkRestAPIDestination(\n    data=df,\n    options={\n        \"checkpointLocation\": \"{/CHECKPOINT-LOCATION/}\"\n    },\n    url=\"{REST-API-URL}\",\n    headers = {\n        'Authorization': 'Bearer {}'.format(\"{TOKEN}\")\n    },\n    batch_size=100,\n    method=\"POST\",\n    parallelism=8,\n    trigger=\"1 minute\",\n    query_name=\"DeltaRestAPIDestination\",\n    query_wait_interval=None\n)\n\nrest_api_destination.write_stream()\n</code></pre> <pre><code>#Rest API Destination for Batch Queries\n\nfrom rtdip_sdk.pipelines.destinations import SparkRestAPIDestination\n\nrest_api_destination = SparkRestAPIDestination(\n    data=df,\n    options={},\n    url=\"{REST-API-URL}\",\n    headers = {\n        'Authorization': 'Bearer {}'.format(\"{TOKEN}\")\n    },\n    batch_size=10,\n    method=\"POST\",\n    parallelism=4,\n    trigger=\"1 minute\",\n    query_name=\"DeltaRestAPIDestination\",\n    query_wait_interval=None\n)\n\nrest_api_destination.write_stream()\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Dataframe to be merged into a Delta Table</p> required <code>options</code> <code>dict</code> <p>A dictionary of options for streaming writes</p> required <code>url</code> <code>str</code> <p>The Rest API Url</p> required <code>headers</code> <code>dict</code> <p>A dictionary of headers to be provided to the Rest API</p> required <code>batch_size</code> <code>int</code> <p>The number of DataFrame rows to be used in each Rest API call</p> required <code>method</code> <code>str</code> <p>The method to be used when calling the Rest API. Allowed values are POST, PATCH and PUT</p> <code>'POST'</code> <code>parallelism</code> <code>int</code> <p>The number of concurrent calls to be made to the Rest API</p> <code>8</code> <code>trigger</code> <code>optional str</code> <p>Frequency of the write operation. Specify \"availableNow\" to execute a trigger once, otherwise specify a time period such as \"30 seconds\", \"5 minutes\". Set to \"0 seconds\" if you do not want to use a trigger. (stream) Default is 10 seconds</p> <code>'1 minutes'</code> <code>query_name</code> <code>str</code> <p>Unique name for the query in associated SparkSession</p> <code>'DeltaRestAPIDestination'</code> <code>query_wait_interval</code> <code>optional int</code> <p>If set, waits for the streaming query to complete before returning. (stream) Default is None</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>checkpointLocation</code> <code>str</code> <p>Path to checkpoint files. (Streaming)</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/rest_api.py</code> <pre><code>class SparkRestAPIDestination(DestinationInterface):\n\"\"\"\n    The Spark Rest API Destination is used to write data to a Rest API.\n\n    The payload sent to the API is constructed by converting each row in the DataFrame to Json.\n\n    !!! Note\n        While it is possible to use the `write_batch` method, it is easy to overwhlem a Rest API with large volumes of data.\n        Consider reducing data volumes when writing to a Rest API in Batch mode to prevent API errors including throtting.\n\n    Example\n    --------\n    ```python\n    #Rest API Destination for Streaming Queries\n\n    from rtdip_sdk.pipelines.destinations import SparkRestAPIDestination\n\n    rest_api_destination = SparkRestAPIDestination(\n        data=df,\n        options={\n            \"checkpointLocation\": \"{/CHECKPOINT-LOCATION/}\"\n        },\n        url=\"{REST-API-URL}\",\n        headers = {\n            'Authorization': 'Bearer {}'.format(\"{TOKEN}\")\n        },\n        batch_size=100,\n        method=\"POST\",\n        parallelism=8,\n        trigger=\"1 minute\",\n        query_name=\"DeltaRestAPIDestination\",\n        query_wait_interval=None\n    )\n\n    rest_api_destination.write_stream()\n    ```\n    ```python\n    #Rest API Destination for Batch Queries\n\n    from rtdip_sdk.pipelines.destinations import SparkRestAPIDestination\n\n    rest_api_destination = SparkRestAPIDestination(\n        data=df,\n        options={},\n        url=\"{REST-API-URL}\",\n        headers = {\n            'Authorization': 'Bearer {}'.format(\"{TOKEN}\")\n        },\n        batch_size=10,\n        method=\"POST\",\n        parallelism=4,\n        trigger=\"1 minute\",\n        query_name=\"DeltaRestAPIDestination\",\n        query_wait_interval=None\n    )\n\n    rest_api_destination.write_stream()\n    ```\n\n    Parameters:\n        data (DataFrame): Dataframe to be merged into a Delta Table\n        options (dict): A dictionary of options for streaming writes\n        url (str): The Rest API Url\n        headers (dict): A dictionary of headers to be provided to the Rest API\n        batch_size (int): The number of DataFrame rows to be used in each Rest API call\n        method (str): The method to be used when calling the Rest API. Allowed values are POST, PATCH and PUT\n        parallelism (int): The number of concurrent calls to be made to the Rest API\n        trigger (optional str): Frequency of the write operation. Specify \"availableNow\" to execute a trigger once, otherwise specify a time period such as \"30 seconds\", \"5 minutes\". Set to \"0 seconds\" if you do not want to use a trigger. (stream) Default is 10 seconds\n        query_name (str): Unique name for the query in associated SparkSession\n        query_wait_interval (optional int): If set, waits for the streaming query to complete before returning. (stream) Default is None\n\n    Attributes:\n        checkpointLocation (str): Path to checkpoint files. (Streaming)\n    \"\"\"\n\n    data: DataFrame\n    options: dict\n    url: str\n    headers: dict\n    batch_size: int\n    method: str\n    parallelism: int\n    trigger: str\n    query_name: str\n    query_wait_interval: int\n\n    def __init__(\n        self,\n        data: DataFrame,\n        options: dict,\n        url: str,\n        headers: dict,\n        batch_size: int,\n        method: str = \"POST\",\n        parallelism: int = 8,\n        trigger=\"1 minutes\",\n        query_name: str = \"DeltaRestAPIDestination\",\n        query_wait_interval: int = None,\n    ) -&gt; None:\n        self.data = data\n        self.options = options\n        self.url = url\n        self.headers = headers\n        self.batch_size = batch_size\n        self.method = method\n        self.parallelism = parallelism\n        self.trigger = trigger\n        self.query_name = query_name\n        self.query_wait_interval = query_wait_interval\n\n    @staticmethod\n    def system_type():\n\"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        libraries.add_pypi_library(get_default_package(\"api_requests\"))\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_write_validation(self):\n        return True\n\n    def post_write_validation(self):\n        return True\n\n    def _pre_batch_records_for_api_call(self, micro_batch_df: DataFrame):\n        batch_count = math.ceil(micro_batch_df.count() / self.batch_size)\n        micro_batch_df = (\n            micro_batch_df.withColumn(\"content\", to_json(struct(col(\"*\"))))\n            .withColumn(\"row_number\", row_number().over(Window().orderBy(lit(\"A\"))))\n            .withColumn(\"batch_id\", col(\"row_number\") % batch_count)\n        )\n        return micro_batch_df.groupBy(\"batch_id\").agg(\n            concat_ws(\",|\", collect_list(\"content\")).alias(\"payload\")\n        )\n\n    def _api_micro_batch(self, micro_batch_df: DataFrame, epoch_id=None):  # NOSONAR\n        url = self.url\n        method = self.method\n        headers = self.headers\n\n        @udf(\"string\")\n        def _rest_api_execute(data):\n            session = requests.Session()\n            adapter = HTTPAdapter(max_retries=3)\n            session.mount(\"http://\", adapter)  # NOSONAR\n            session.mount(\"https://\", adapter)\n\n            if method == \"POST\":\n                response = session.post(url, headers=headers, data=data, verify=False)\n            elif method == \"PATCH\":\n                response = session.patch(url, headers=headers, data=data, verify=False)\n            elif method == \"PUT\":\n                response = session.put(url, headers=headers, data=data, verify=False)\n            else:\n                raise Exception(\"Method {} is not supported\".format(method))  # NOSONAR\n\n            if not (response.status_code == 200 or response.status_code == 201):\n                raise Exception(\n                    \"Response status : {} .Response message : {}\".format(\n                        str(response.status_code), response.text\n                    )\n                )  # NOSONAR\n\n            return str(response.status_code)\n\n        micro_batch_df.persist()\n        micro_batch_df = self._pre_batch_records_for_api_call(micro_batch_df)\n\n        micro_batch_df = micro_batch_df.repartition(self.parallelism)\n\n        (\n            micro_batch_df.withColumn(\n                \"rest_api_response_code\", _rest_api_execute(micro_batch_df[\"payload\"])\n            ).collect()\n        )\n        micro_batch_df.unpersist()\n\n    def write_batch(self):\n\"\"\"\n        Writes batch data to a Rest API\n        \"\"\"\n        try:\n            return self._api_micro_batch(self.data)\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def write_stream(self):\n\"\"\"\n        Writes streaming data to a Rest API\n        \"\"\"\n        try:\n            TRIGGER_OPTION = (\n                {\"availableNow\": True}\n                if self.trigger == \"availableNow\"\n                else {\"processingTime\": self.trigger}\n            )\n            query = (\n                self.data.writeStream.trigger(**TRIGGER_OPTION)\n                .foreachBatch(self._api_micro_batch)\n                .queryName(self.query_name)\n                .outputMode(\"update\")\n                .options(**self.options)\n                .start()\n            )\n\n            if self.query_wait_interval:\n                while query.isActive:\n                    if query.lastProgress:\n                        logging.info(query.lastProgress)\n                    time.sleep(self.query_wait_interval)\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/rest_api/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.rest_api.SparkRestAPIDestination.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/rest_api.py</code> <pre><code>@staticmethod\ndef system_type():\n\"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/rest_api/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.rest_api.SparkRestAPIDestination.write_batch","title":"<code>write_batch()</code>","text":"<p>Writes batch data to a Rest API</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/rest_api.py</code> <pre><code>def write_batch(self):\n\"\"\"\n    Writes batch data to a Rest API\n    \"\"\"\n    try:\n        return self._api_micro_batch(self.data)\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/rest_api/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.rest_api.SparkRestAPIDestination.write_stream","title":"<code>write_stream()</code>","text":"<p>Writes streaming data to a Rest API</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/rest_api.py</code> <pre><code>def write_stream(self):\n\"\"\"\n    Writes streaming data to a Rest API\n    \"\"\"\n    try:\n        TRIGGER_OPTION = (\n            {\"availableNow\": True}\n            if self.trigger == \"availableNow\"\n            else {\"processingTime\": self.trigger}\n        )\n        query = (\n            self.data.writeStream.trigger(**TRIGGER_OPTION)\n            .foreachBatch(self._api_micro_batch)\n            .queryName(self.query_name)\n            .outputMode(\"update\")\n            .options(**self.options)\n            .start()\n        )\n\n        if self.query_wait_interval:\n            while query.isActive:\n                if query.lastProgress:\n                    logging.info(query.lastProgress)\n                time.sleep(self.query_wait_interval)\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/secrets/azure_key_vault/","title":"Databricks Secret Scope","text":""},{"location":"sdk/code-reference/pipelines/secrets/azure_key_vault/#src.sdk.python.rtdip_sdk.pipelines.secrets.azure_key_vault.AzureKeyVaultSecrets","title":"<code>AzureKeyVaultSecrets</code>","text":"<p>             Bases: <code>SecretsInterface</code></p> <p>Retrieves and creates/updates secrets in Azure Key Vault. For more information about Azure Key Vaults, see here.</p>"},{"location":"sdk/code-reference/pipelines/secrets/azure_key_vault/#src.sdk.python.rtdip_sdk.pipelines.secrets.azure_key_vault.AzureKeyVaultSecrets--example","title":"Example","text":"<p><pre><code># Retrieves Secrets from Azure Key Vault\n\nfrom rtdip_sdk.pipelines.secrets import AzureKeyVaultSecrets\n\nget_key_vault_secret = AzureKeyVaultSecrets(\n    vault=\"https://{YOUR-KEY-VAULT}.azure.net/\",\n    key=\"{KEY}\",\n    secret=None,\n    credential=\"{CREDENTIAL}\",\n    kwargs=None\n)\n\nget_key_vault_secret.get()\n</code></pre> <pre><code># Creates or Updates Secrets in Azure Key Vault\n\nfrom rtdip_sdk.pipelines.secrets import AzureKeyVaultSecrets\n\nset_key_vault_secret = AzureKeyVaultSecrets(\n    vault=\"https://{YOUR-KEY-VAULT}.azure.net/\",\n    key=\"{KEY}\",\n    secret=\"{SECRET-TO-BE-SET}\",\n    credential=\"{CREDENTIAL}\",\n    kwargs=None\n)\n\nset_key_vault_secret.set()\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>vault</code> <code>str</code> <p>Azure Key Vault URL</p> required <code>key</code> <code>str</code> <p>Key for the secret</p> required <code>secret</code> <code>str</code> <p>Secret or Password to be set in the Azure Key Vault</p> <code>None</code> <code>credential</code> <code>str</code> <p>Credential for authenticating with Azure Key Vault</p> <code>None</code> <code>kwargs</code> <code>dict</code> <p>List of additional parameters to be passed when creating a Azure Key Vault Client. Please see here for more details on parameters that can be provided to the client</p> <code>None</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/secrets/azure_key_vault.py</code> <pre><code>class AzureKeyVaultSecrets(SecretsInterface):\n\"\"\"\n    Retrieves and creates/updates secrets in Azure Key Vault. For more information about Azure Key Vaults, see [here.](https://learn.microsoft.com/en-gb/azure/key-vault/general/overview)\n\n    Example\n    -------\n    ```python\n    # Retrieves Secrets from Azure Key Vault\n\n    from rtdip_sdk.pipelines.secrets import AzureKeyVaultSecrets\n\n    get_key_vault_secret = AzureKeyVaultSecrets(\n        vault=\"https://{YOUR-KEY-VAULT}.azure.net/\",\n        key=\"{KEY}\",\n        secret=None,\n        credential=\"{CREDENTIAL}\",\n        kwargs=None\n    )\n\n    get_key_vault_secret.get()\n\n    ```\n    ```python\n    # Creates or Updates Secrets in Azure Key Vault\n\n    from rtdip_sdk.pipelines.secrets import AzureKeyVaultSecrets\n\n    set_key_vault_secret = AzureKeyVaultSecrets(\n        vault=\"https://{YOUR-KEY-VAULT}.azure.net/\",\n        key=\"{KEY}\",\n        secret=\"{SECRET-TO-BE-SET}\",\n        credential=\"{CREDENTIAL}\",\n        kwargs=None\n    )\n\n    set_key_vault_secret.set()\n    ```\n\n    Parameters:\n        vault (str): Azure Key Vault URL\n        key (str): Key for the secret\n        secret (str): Secret or Password to be set in the Azure Key Vault\n        credential (str): Credential for authenticating with Azure Key Vault\n        kwargs (dict): List of additional parameters to be passed when creating a Azure Key Vault Client. Please see [here](https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/keyvault/azure-keyvault-secrets) for more details on parameters that can be provided to the client\n    \"\"\"\n\n    vault: str\n    key: str\n    secret: str\n    credential: str\n    kwargs: dict\n\n    def __init__(\n        self,\n        vault: str,\n        key: str,\n        secret: str = None,\n        credential=None,\n        kwargs: dict = None,\n    ):\n        self.vault = vault\n        self.key = key\n        self.secret = secret\n        self.credential = credential\n        self.kwargs = {} if kwargs is None else kwargs\n        self.client = self._get_akv_client()\n\n    @staticmethod\n    def system_type():\n\"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYTHON\n        \"\"\"\n        return SystemType.PYTHON\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        libraries.add_pypi_library(get_default_package(\"azure_key_vault_secret\"))\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def _get_akv_client(self):\n        return SecretClient(\n            vault_url=\"https://{}.vault.azure.net\".format(self.vault),\n            credential=self.credential,\n            **self.kwargs\n        )\n\n    def get(self):\n\"\"\"\n        Retrieves the secret from the Azure Key Vault\n        \"\"\"\n        response = self.client.get_secret(name=self.key)\n        return response.value\n\n    def set(self):\n\"\"\"\n        Creates or updates a secret in the Azure Key Vault\n        \"\"\"\n        self.client.set_secret(name=self.key, value=self.secret)\n        return True\n</code></pre>"},{"location":"sdk/code-reference/pipelines/secrets/azure_key_vault/#src.sdk.python.rtdip_sdk.pipelines.secrets.azure_key_vault.AzureKeyVaultSecrets.get","title":"<code>get()</code>","text":"<p>Retrieves the secret from the Azure Key Vault</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/secrets/azure_key_vault.py</code> <pre><code>def get(self):\n\"\"\"\n    Retrieves the secret from the Azure Key Vault\n    \"\"\"\n    response = self.client.get_secret(name=self.key)\n    return response.value\n</code></pre>"},{"location":"sdk/code-reference/pipelines/secrets/azure_key_vault/#src.sdk.python.rtdip_sdk.pipelines.secrets.azure_key_vault.AzureKeyVaultSecrets.set","title":"<code>set()</code>","text":"<p>Creates or updates a secret in the Azure Key Vault</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/secrets/azure_key_vault.py</code> <pre><code>def set(self):\n\"\"\"\n    Creates or updates a secret in the Azure Key Vault\n    \"\"\"\n    self.client.set_secret(name=self.key, value=self.secret)\n    return True\n</code></pre>"},{"location":"sdk/code-reference/pipelines/secrets/azure_key_vault/#src.sdk.python.rtdip_sdk.pipelines.secrets.azure_key_vault.AzureKeyVaultSecrets.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYTHON</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/secrets/azure_key_vault.py</code> <pre><code>@staticmethod\ndef system_type():\n\"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYTHON\n    \"\"\"\n    return SystemType.PYTHON\n</code></pre>"},{"location":"sdk/code-reference/pipelines/secrets/databricks/","title":"Databricks Secret Scope","text":""},{"location":"sdk/code-reference/pipelines/secrets/databricks/#src.sdk.python.rtdip_sdk.pipelines.secrets.databricks.DatabricksSecrets","title":"<code>DatabricksSecrets</code>","text":"<p>             Bases: <code>SecretsInterface</code></p> <p>Retrieves secrets from Databricks Secret Scopes. For more information about Databricks Secret Scopes, see here.</p>"},{"location":"sdk/code-reference/pipelines/secrets/databricks/#src.sdk.python.rtdip_sdk.pipelines.secrets.databricks.DatabricksSecrets--example","title":"Example","text":"<pre><code># Reads Secrets from Databricks Secret Scopes\n\nfrom rtdip_sdk.pipelines.secrets import DatabricksSecrets\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\nget_databricks_secret = DatabricksSecrets(\n    spark=spark,\n    vault=\"{NAME-OF-DATABRICKS-SECRET-SCOPE}\"\n    key=\"{KEY-NAME-OF-SECRET}\",\n)\n\nget_databricks_secret.get()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session required to read data from a Delta table</p> required <code>vault</code> <code>str</code> <p>Name of the Databricks Secret Scope</p> required <code>key</code> <code>str</code> <p>Name/Key of the secret in the Databricks Secret Scope</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/secrets/databricks.py</code> <pre><code>class DatabricksSecrets(SecretsInterface):\n\"\"\"\n    Retrieves secrets from Databricks Secret Scopes. For more information about Databricks Secret Scopes, see [here.](https://docs.databricks.com/security/secrets/secret-scopes.html)\n\n    Example\n    -------\n    ```python\n    # Reads Secrets from Databricks Secret Scopes\n\n    from rtdip_sdk.pipelines.secrets import DatabricksSecrets\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    get_databricks_secret = DatabricksSecrets(\n        spark=spark,\n        vault=\"{NAME-OF-DATABRICKS-SECRET-SCOPE}\"\n        key=\"{KEY-NAME-OF-SECRET}\",\n    )\n\n    get_databricks_secret.get()\n    ```\n\n    Parameters:\n        spark: Spark Session required to read data from a Delta table\n        vault: Name of the Databricks Secret Scope\n        key: Name/Key of the secret in the Databricks Secret Scope\n    \"\"\"\n\n    spark: SparkSession\n    vault: str\n    key: str\n\n    def __init__(self, spark: SparkSession, vault: str, key: str):\n        self.spark = spark\n        self.vault = vault\n        self.key = key\n\n    @staticmethod\n    def system_type():\n\"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK on Databricks\n        \"\"\"\n        return SystemType.PYSPARK_DATABRICKS\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def get(self):\n\"\"\"\n        Retrieves the secret from the Databricks Secret Scope\n        \"\"\"\n        dbutils = get_dbutils(self.spark)\n        return dbutils.secrets.get(scope=self.vault, key=self.key)\n\n    def set(self):\n\"\"\"\n        Sets the secret in the Secret Scope\n        Raises:\n            NotImplementedError: Will be implemented at a later point in time\n        \"\"\"\n        return NotImplementedError\n</code></pre>"},{"location":"sdk/code-reference/pipelines/secrets/databricks/#src.sdk.python.rtdip_sdk.pipelines.secrets.databricks.DatabricksSecrets.get","title":"<code>get()</code>","text":"<p>Retrieves the secret from the Databricks Secret Scope</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/secrets/databricks.py</code> <pre><code>def get(self):\n\"\"\"\n    Retrieves the secret from the Databricks Secret Scope\n    \"\"\"\n    dbutils = get_dbutils(self.spark)\n    return dbutils.secrets.get(scope=self.vault, key=self.key)\n</code></pre>"},{"location":"sdk/code-reference/pipelines/secrets/databricks/#src.sdk.python.rtdip_sdk.pipelines.secrets.databricks.DatabricksSecrets.set","title":"<code>set()</code>","text":"<p>Sets the secret in the Secret Scope Raises:     NotImplementedError: Will be implemented at a later point in time</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/secrets/databricks.py</code> <pre><code>def set(self):\n\"\"\"\n    Sets the secret in the Secret Scope\n    Raises:\n        NotImplementedError: Will be implemented at a later point in time\n    \"\"\"\n    return NotImplementedError\n</code></pre>"},{"location":"sdk/code-reference/pipelines/secrets/databricks/#src.sdk.python.rtdip_sdk.pipelines.secrets.databricks.DatabricksSecrets.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK on Databricks</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/secrets/databricks.py</code> <pre><code>@staticmethod\ndef system_type():\n\"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK on Databricks\n    \"\"\"\n    return SystemType.PYSPARK_DATABRICKS\n</code></pre>"},{"location":"sdk/code-reference/pipelines/secrets/hashicorp_vault/","title":"Databricks Secret Scope","text":""},{"location":"sdk/code-reference/pipelines/secrets/hashicorp_vault/#src.sdk.python.rtdip_sdk.pipelines.secrets.hashicorp_vault.HashiCorpVaultSecrets","title":"<code>HashiCorpVaultSecrets</code>","text":"<p>             Bases: <code>SecretsInterface</code></p> <p>Retrieves and creates/updates secrets in a Hashicorp Vault. For more information about Hashicorp Vaults, see here.</p>"},{"location":"sdk/code-reference/pipelines/secrets/hashicorp_vault/#src.sdk.python.rtdip_sdk.pipelines.secrets.hashicorp_vault.HashiCorpVaultSecrets--example","title":"Example","text":"<p><pre><code># Retrieves Secrets from HashiCorp Vault\n\nfrom rtdip_sdk.pipelines.secrets import HashiCorpVaultSecrets\n\nget_hashicorp_secret = HashiCorpVaultSecrets(\n    vault=\"http://127.0.0.1:8200\",\n    key=\"{KEY}\",\n    secret=None,\n    credential=\"{CREDENTIAL}\",\n    kwargs=None\n)\n\nget_hashicorp_secret.get()\n</code></pre> <pre><code># Creates or Updates Secrets in Hashicorp Vault\n\nfrom rtdip_sdk.pipelines.secrets import HashiCorpVaultSecrets\n\nset_hashicorp_secret = AzureKeyVaultSecrets(\n    vault=\"http://127.0.0.1:8200\",\n    key=\"{KEY}\",\n    secret=\"{SECRET-TO-BE-SET}\",\n    credential=\"{CREDENTIAL}\",\n    kwargs=None\n)\n\nset_hashicorp_secret.set()\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>vault</code> <code>str</code> <p>Hashicorp Vault URL</p> required <code>key</code> <code>str</code> <p>Name/Key of the secret in the Hashicorp Vault</p> required <code>secret</code> <code>str</code> <p>Secret or Password to be stored in the Hashicorp Vault</p> <code>None</code> <code>credential</code> <code>str</code> <p>Token for authentication with the Hashicorp Vault</p> <code>None</code> <code>kwargs</code> <code>dict</code> <p>List of additional parameters to be passed when creating a Hashicorp Vault Client. Please see here for more details on parameters that can be provided to the client</p> <code>{}</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/secrets/hashicorp_vault.py</code> <pre><code>class HashiCorpVaultSecrets(SecretsInterface):\n\"\"\"\n    Retrieves and creates/updates secrets in a Hashicorp Vault. For more information about Hashicorp Vaults, see [here.](https://developer.hashicorp.com/vault/docs/get-started/developer-qs)\n\n    Example\n    -------\n    ```python\n    # Retrieves Secrets from HashiCorp Vault\n\n    from rtdip_sdk.pipelines.secrets import HashiCorpVaultSecrets\n\n    get_hashicorp_secret = HashiCorpVaultSecrets(\n        vault=\"http://127.0.0.1:8200\",\n        key=\"{KEY}\",\n        secret=None,\n        credential=\"{CREDENTIAL}\",\n        kwargs=None\n    )\n\n    get_hashicorp_secret.get()\n\n    ```\n    ```python\n    # Creates or Updates Secrets in Hashicorp Vault\n\n    from rtdip_sdk.pipelines.secrets import HashiCorpVaultSecrets\n\n    set_hashicorp_secret = AzureKeyVaultSecrets(\n        vault=\"http://127.0.0.1:8200\",\n        key=\"{KEY}\",\n        secret=\"{SECRET-TO-BE-SET}\",\n        credential=\"{CREDENTIAL}\",\n        kwargs=None\n    )\n\n    set_hashicorp_secret.set()\n    ```\n\n    Parameters:\n        vault (str): Hashicorp Vault URL\n        key (str): Name/Key of the secret in the Hashicorp Vault\n        secret (str): Secret or Password to be stored in the Hashicorp Vault\n        credential (str): Token for authentication with the Hashicorp Vault\n        kwargs (dict): List of additional parameters to be passed when creating a Hashicorp Vault Client. Please see [here](https://hvac.readthedocs.io/en/stable/overview.html#initialize-the-client) for more details on parameters that can be provided to the client\n    \"\"\"\n\n    vault: str\n    key: str\n    secret: str\n    credential: str\n\n    def __init__(\n        self,\n        vault: str,\n        key: str,\n        secret: str = None,\n        credential: str = None,\n        kwargs: dict = {},\n    ):  # NOSONAR\n        self.vault = vault\n        self.key = key\n        self.secret = secret\n        self.credential = credential\n        self.kwargs = kwargs\n        self.client = self._get_hvac_client()\n\n    @staticmethod\n    def system_type():\n\"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYTHON\n        \"\"\"\n        return SystemType.PYTHON\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        libraries.add_pypi_library(get_default_package(\"hashicorp_vault\"))\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def _get_hvac_client(self):\n        return hvac.Client(url=self.vault, token=self.credential, **self.kwargs)\n\n    def get(self):\n\"\"\"\n        Retrieves the secret from the Hashicorp Vault\n        \"\"\"\n        response = self.client.secrets.kv.read_secret_version(path=self.key)\n        return response[\"data\"][\"data\"][\"password\"]\n\n    def set(self):\n\"\"\"\n        Creates or updates a secret in the Hashicorp Vault\n        \"\"\"\n        self.client.secrets.kv.v2.create_or_update_secret(\n            path=self.key,\n            secret=dict(password=self.secret),\n        )\n        return True\n</code></pre>"},{"location":"sdk/code-reference/pipelines/secrets/hashicorp_vault/#src.sdk.python.rtdip_sdk.pipelines.secrets.hashicorp_vault.HashiCorpVaultSecrets.get","title":"<code>get()</code>","text":"<p>Retrieves the secret from the Hashicorp Vault</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/secrets/hashicorp_vault.py</code> <pre><code>def get(self):\n\"\"\"\n    Retrieves the secret from the Hashicorp Vault\n    \"\"\"\n    response = self.client.secrets.kv.read_secret_version(path=self.key)\n    return response[\"data\"][\"data\"][\"password\"]\n</code></pre>"},{"location":"sdk/code-reference/pipelines/secrets/hashicorp_vault/#src.sdk.python.rtdip_sdk.pipelines.secrets.hashicorp_vault.HashiCorpVaultSecrets.set","title":"<code>set()</code>","text":"<p>Creates or updates a secret in the Hashicorp Vault</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/secrets/hashicorp_vault.py</code> <pre><code>def set(self):\n\"\"\"\n    Creates or updates a secret in the Hashicorp Vault\n    \"\"\"\n    self.client.secrets.kv.v2.create_or_update_secret(\n        path=self.key,\n        secret=dict(password=self.secret),\n    )\n    return True\n</code></pre>"},{"location":"sdk/code-reference/pipelines/secrets/hashicorp_vault/#src.sdk.python.rtdip_sdk.pipelines.secrets.hashicorp_vault.HashiCorpVaultSecrets.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYTHON</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/secrets/hashicorp_vault.py</code> <pre><code>@staticmethod\ndef system_type():\n\"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYTHON\n    \"\"\"\n    return SystemType.PYTHON\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/python/delta/","title":"Read from Delta","text":""},{"location":"sdk/code-reference/pipelines/sources/python/delta/#src.sdk.python.rtdip_sdk.pipelines.sources.python.delta.PythonDeltaSource","title":"<code>PythonDeltaSource</code>","text":"<p>             Bases: <code>SourceInterface</code></p> <p>The Python Delta Source is used to read data from a Delta table without using Apache Spark, returning a Polars LazyFrame.</p>"},{"location":"sdk/code-reference/pipelines/sources/python/delta/#src.sdk.python.rtdip_sdk.pipelines.sources.python.delta.PythonDeltaSource--example","title":"Example","text":"AzureAWS <pre><code>from rtdip_sdk.pipelines.sources import PythonDeltaSource\n\npath = \"abfss://{FILE-SYSTEM}@{ACCOUNT-NAME}.dfs.core.windows.net/{PATH}/{FILE-NAME}\n\npython_delta_source = PythonDeltaSource(\n    path=path,\n    version=None,\n    storage_options={\n        \"azure_storage_account_name\": \"{AZURE-STORAGE-ACCOUNT-NAME}\",\n        \"azure_storage_account_key\": \"{AZURE-STORAGE-ACCOUNT-KEY}\"\n    },\n    pyarrow_options=None,\n    without_files=False\n)\n\npython_delta_source.read_batch()\n</code></pre> <pre><code>from rtdip_sdk.pipelines.sources import PythonDeltaSource\n\npath = \"https://s3.{REGION-CODE}.amazonaws.com/{BUCKET-NAME}/{KEY-NAME}\"\n\npython_delta_source = PythonDeltaSource(\n    path=path,\n    version=None,\n    storage_options={\n        \"aws_access_key_id\": \"{AWS-ACCESS-KEY-ID}\",\n        \"aws_secret_access_key\": \"{AWS-SECRET-ACCESS-KEY}\"\n    },\n    pyarrow_options=None,\n    without_files=False\n)\n\npython_delta_source.read_batch()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the Delta table. Can be local or in S3/Azure storage</p> required <code>version</code> <code>optional int</code> <p>Specify the Delta table version to read from. Defaults to the latest version</p> <code>None</code> <code>storage_options</code> <code>optional dict</code> <p>Used to read from AWS/Azure storage. For AWS use format {\"aws_access_key_id\": \"&lt;&gt;\", \"aws_secret_access_key\":\"&lt;&gt;\"}. For Azure use format {\"azure_storage_account_name\": \"&lt;&gt;\", \"azure_storage_account_key\": \"&lt;&gt;\"}.</p> <code>None</code> <code>pyarrow_options</code> <code>optional dict</code> <p>Data Access and Efficiency options when reading from Delta. See to_pyarrow_dataset.</p> <code>None</code> <code>without_files</code> <code>optional bool</code> <p>If True loads the table without tracking files</p> <code>False</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/python/delta.py</code> <pre><code>class PythonDeltaSource(SourceInterface):\n\"\"\"\n    The Python Delta Source is used to read data from a Delta table without using Apache Spark, returning a Polars LazyFrame.\n\n     Example\n    --------\n    === \"Azure\"\n\n        ```python\n        from rtdip_sdk.pipelines.sources import PythonDeltaSource\n\n        path = \"abfss://{FILE-SYSTEM}@{ACCOUNT-NAME}.dfs.core.windows.net/{PATH}/{FILE-NAME}\n\n        python_delta_source = PythonDeltaSource(\n            path=path,\n            version=None,\n            storage_options={\n                \"azure_storage_account_name\": \"{AZURE-STORAGE-ACCOUNT-NAME}\",\n                \"azure_storage_account_key\": \"{AZURE-STORAGE-ACCOUNT-KEY}\"\n            },\n            pyarrow_options=None,\n            without_files=False\n        )\n\n        python_delta_source.read_batch()\n        ```\n    === \"AWS\"\n\n        ```python\n        from rtdip_sdk.pipelines.sources import PythonDeltaSource\n\n        path = \"https://s3.{REGION-CODE}.amazonaws.com/{BUCKET-NAME}/{KEY-NAME}\"\n\n        python_delta_source = PythonDeltaSource(\n            path=path,\n            version=None,\n            storage_options={\n                \"aws_access_key_id\": \"{AWS-ACCESS-KEY-ID}\",\n                \"aws_secret_access_key\": \"{AWS-SECRET-ACCESS-KEY}\"\n            },\n            pyarrow_options=None,\n            without_files=False\n        )\n\n        python_delta_source.read_batch()\n        ```\n\n    Parameters:\n        path (str): Path to the Delta table. Can be local or in S3/Azure storage\n        version (optional int): Specify the Delta table version to read from. Defaults to the latest version\n        storage_options (optional dict): Used to read from AWS/Azure storage. For AWS use format {\"aws_access_key_id\": \"&lt;&gt;\", \"aws_secret_access_key\":\"&lt;&gt;\"}. For Azure use format {\"azure_storage_account_name\": \"&lt;&gt;\", \"azure_storage_account_key\": \"&lt;&gt;\"}.\n        pyarrow_options (optional dict): Data Access and Efficiency options when reading from Delta. See [to_pyarrow_dataset](https://delta-io.github.io/delta-rs/python/api_reference.html#deltalake.table.DeltaTable.to_pyarrow_dataset){ target=\"_blank\" }.\n        without_files (optional bool): If True loads the table without tracking files\n    \"\"\"\n\n    path: str\n    version: int\n    storage_options: dict\n    pyarrow_options: dict\n    without_files: bool\n\n    def __init__(\n        self,\n        path: str,\n        version: int = None,\n        storage_options: dict = None,\n        pyarrow_options: dict = None,\n        without_files: bool = False,\n    ):\n        self.path = path\n        self.version = version\n        self.storage_options = storage_options\n        self.pyarrow_options = pyarrow_options\n        self.without_files = without_files\n\n    @staticmethod\n    def system_type():\n\"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYTHON\n        \"\"\"\n        return SystemType.PYTHON\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_read_validation(self):\n        return True\n\n    def post_read_validation(self):\n        return True\n\n    def read_batch(self) -&gt; LazyFrame:\n\"\"\"\n        Reads data from a Delta table into a Polars LazyFrame\n        \"\"\"\n        without_files_dict = {\"without_files\": self.without_files}\n        lf = pl.scan_delta(\n            source=self.path,\n            version=self.version,\n            storage_options=self.storage_options,\n            delta_table_options=without_files_dict,\n            pyarrow_options=self.pyarrow_options,\n        )\n        return lf\n\n    def read_stream(self):\n\"\"\"\n        Raises:\n            NotImplementedError: Reading from a Delta table using Python is only possible for batch reads. To perform a streaming read, use the read_stream method of the SparkDeltaSource component.\n        \"\"\"\n        raise NotImplementedError(\n            \"Reading from a Delta table using Python is only possible for batch reads. To perform a streaming read, use the read_stream method of the SparkDeltaSource component\"\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/python/delta/#src.sdk.python.rtdip_sdk.pipelines.sources.python.delta.PythonDeltaSource.read_batch","title":"<code>read_batch()</code>","text":"<p>Reads data from a Delta table into a Polars LazyFrame</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/python/delta.py</code> <pre><code>def read_batch(self) -&gt; LazyFrame:\n\"\"\"\n    Reads data from a Delta table into a Polars LazyFrame\n    \"\"\"\n    without_files_dict = {\"without_files\": self.without_files}\n    lf = pl.scan_delta(\n        source=self.path,\n        version=self.version,\n        storage_options=self.storage_options,\n        delta_table_options=without_files_dict,\n        pyarrow_options=self.pyarrow_options,\n    )\n    return lf\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/python/delta/#src.sdk.python.rtdip_sdk.pipelines.sources.python.delta.PythonDeltaSource.read_stream","title":"<code>read_stream()</code>","text":"<p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Reading from a Delta table using Python is only possible for batch reads. To perform a streaming read, use the read_stream method of the SparkDeltaSource component.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/python/delta.py</code> <pre><code>def read_stream(self):\n\"\"\"\n    Raises:\n        NotImplementedError: Reading from a Delta table using Python is only possible for batch reads. To perform a streaming read, use the read_stream method of the SparkDeltaSource component.\n    \"\"\"\n    raise NotImplementedError(\n        \"Reading from a Delta table using Python is only possible for batch reads. To perform a streaming read, use the read_stream method of the SparkDeltaSource component\"\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/python/delta/#src.sdk.python.rtdip_sdk.pipelines.sources.python.delta.PythonDeltaSource.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYTHON</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/python/delta.py</code> <pre><code>@staticmethod\ndef system_type():\n\"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYTHON\n    \"\"\"\n    return SystemType.PYTHON\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/python/delta_sharing/","title":"Read from Delta with Delta Sharing","text":""},{"location":"sdk/code-reference/pipelines/sources/python/delta_sharing/#src.sdk.python.rtdip_sdk.pipelines.sources.python.delta_sharing.PythonDeltaSharingSource","title":"<code>PythonDeltaSharingSource</code>","text":"<p>             Bases: <code>SourceInterface</code></p> <p>The Python Delta Sharing Source is used to read data from a Delta table with Delta Sharing configured, without using Apache Spark.</p>"},{"location":"sdk/code-reference/pipelines/sources/python/delta_sharing/#src.sdk.python.rtdip_sdk.pipelines.sources.python.delta_sharing.PythonDeltaSharingSource--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.sources import PythonDeltaSharingSource\n\npython_delta_sharing_source = PythonDeltaSharingSource(\n    profile_path=\"{CREDENTIAL-FILE-LOCATION}\",\n    share_name=\"{SHARE-NAME}\",\n    schema_name=\"{SCHEMA-NAME}\",\n    table_name=\"{TABLE-NAME}\"\n)\n\npython_delta_sharing_source.read_batch()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>profile_path</code> <code>str</code> <p>Location of the credential file. Can be any URL supported by FSSPEC</p> required <code>share_name</code> <code>str</code> <p>The value of 'share=' for the table</p> required <code>schema_name</code> <code>str</code> <p>The value of 'schema=' for the table</p> required <code>table_name</code> <code>str</code> <p>The value of 'name=' for the table</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/python/delta_sharing.py</code> <pre><code>class PythonDeltaSharingSource(SourceInterface):\n\"\"\"\n    The Python Delta Sharing Source is used to read data from a Delta table with Delta Sharing configured, without using Apache Spark.\n\n    Example\n    -------\n    ```python\n    from rtdip_sdk.pipelines.sources import PythonDeltaSharingSource\n\n    python_delta_sharing_source = PythonDeltaSharingSource(\n        profile_path=\"{CREDENTIAL-FILE-LOCATION}\",\n        share_name=\"{SHARE-NAME}\",\n        schema_name=\"{SCHEMA-NAME}\",\n        table_name=\"{TABLE-NAME}\"\n    )\n\n    python_delta_sharing_source.read_batch()\n    ```\n\n    Parameters:\n        profile_path (str): Location of the credential file. Can be any URL supported by [FSSPEC](https://filesystem-spec.readthedocs.io/en/latest/index.html){ target=\"_blank\" }\n        share_name (str): The value of 'share=' for the table\n        schema_name (str): The value of 'schema=' for the table\n        table_name (str): The value of 'name=' for the table\n    \"\"\"\n\n    profile_path: str\n    share_name: str\n    schema_name: str\n    table_name: str\n\n    def __init__(\n        self, profile_path: str, share_name: str, schema_name: str, table_name: str\n    ):\n        self.profile_path = profile_path\n        self.share_name = share_name\n        self.schema_name = schema_name\n        self.table_name = table_name\n\n    @staticmethod\n    def system_type():\n\"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYTHON\n        \"\"\"\n        return SystemType.PYTHON\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_read_validation(self):\n        return True\n\n    def post_read_validation(self):\n        return True\n\n    def read_batch(self) -&gt; LazyFrame:\n\"\"\"\n        Reads data from a Delta table with Delta Sharing into a Polars LazyFrame.\n        \"\"\"\n        pandas_df = delta_sharing.load_as_pandas(\n            f\"{self.profile_path}#{self.share_name}.{self.schema_name}.{self.table_name}\"\n        )\n        polars_lazyframe = pl.from_pandas(pandas_df).lazy()\n        return polars_lazyframe\n\n    def read_stream(self):\n\"\"\"\n        Raises:\n            NotImplementedError: Reading from a Delta table with Delta Sharing using Python is only possible for batch reads.\n        \"\"\"\n        raise NotImplementedError(\n            \"Reading from a Delta table with Delta Sharing using Python is only possible for batch reads.\"\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/python/delta_sharing/#src.sdk.python.rtdip_sdk.pipelines.sources.python.delta_sharing.PythonDeltaSharingSource.read_batch","title":"<code>read_batch()</code>","text":"<p>Reads data from a Delta table with Delta Sharing into a Polars LazyFrame.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/python/delta_sharing.py</code> <pre><code>def read_batch(self) -&gt; LazyFrame:\n\"\"\"\n    Reads data from a Delta table with Delta Sharing into a Polars LazyFrame.\n    \"\"\"\n    pandas_df = delta_sharing.load_as_pandas(\n        f\"{self.profile_path}#{self.share_name}.{self.schema_name}.{self.table_name}\"\n    )\n    polars_lazyframe = pl.from_pandas(pandas_df).lazy()\n    return polars_lazyframe\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/python/delta_sharing/#src.sdk.python.rtdip_sdk.pipelines.sources.python.delta_sharing.PythonDeltaSharingSource.read_stream","title":"<code>read_stream()</code>","text":"<p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Reading from a Delta table with Delta Sharing using Python is only possible for batch reads.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/python/delta_sharing.py</code> <pre><code>def read_stream(self):\n\"\"\"\n    Raises:\n        NotImplementedError: Reading from a Delta table with Delta Sharing using Python is only possible for batch reads.\n    \"\"\"\n    raise NotImplementedError(\n        \"Reading from a Delta table with Delta Sharing using Python is only possible for batch reads.\"\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/python/delta_sharing/#src.sdk.python.rtdip_sdk.pipelines.sources.python.delta_sharing.PythonDeltaSharingSource.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYTHON</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/python/delta_sharing.py</code> <pre><code>@staticmethod\ndef system_type():\n\"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYTHON\n    \"\"\"\n    return SystemType.PYTHON\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/python/entsoe/","title":"Read from ENTSO-E API","text":""},{"location":"sdk/code-reference/pipelines/sources/python/entsoe/#src.sdk.python.rtdip_sdk.pipelines.sources.python.entsoe.PythonEntsoeSource","title":"<code>PythonEntsoeSource</code>","text":"<p>             Bases: <code>SourceInterface</code></p> <p>The Python ENTSO-E Source is used to read day-ahead prices from ENTSO-E.</p>"},{"location":"sdk/code-reference/pipelines/sources/python/entsoe/#src.sdk.python.rtdip_sdk.pipelines.sources.python.entsoe.PythonEntsoeSource--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.sources import PythonEntsoeSource\n\nentsoe_source = PythonEntsoeSource(\n    api_key={API_KEY},\n    start='20230101',\n    end='20231001',\n    country_code='NL'\n)\n\nentsoe_source.read_batch()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>API token for ENTSO-E, to request access see documentation here</p> required <code>start</code> <code>str</code> <p>Start time in the format YYYYMMDD</p> required <code>end</code> <code>str</code> <p>End time in the format YYYYMMDD</p> required <code>country_code</code> <code>str</code> <p>Country code to query from. A full list of country codes can be found here</p> required <code>resolution</code> <code>optional str</code> <p>Frequency of values; '60T' for hourly values, '30T' for half-hourly values or '15T' for quarterly values</p> <code>'60T'</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/python/entsoe.py</code> <pre><code>class PythonEntsoeSource(SourceInterface):\n\"\"\"\n    The Python ENTSO-E Source is used to read day-ahead prices from ENTSO-E.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.sources import PythonEntsoeSource\n\n    entsoe_source = PythonEntsoeSource(\n        api_key={API_KEY},\n        start='20230101',\n        end='20231001',\n        country_code='NL'\n    )\n\n    entsoe_source.read_batch()\n    ```\n\n    Args:\n        api_key (str): API token for ENTSO-E, to request access see documentation [here](https://transparency.entsoe.eu/content/static_content/Static%20content/web%20api/Guide.html#_authentication_and_authorisation)\n        start (str): Start time in the format YYYYMMDD\n        end (str): End time in the format YYYYMMDD\n        country_code (str): Country code to query from. A full list of country codes can be found [here](https://github.com/EnergieID/entsoe-py/blob/master/entsoe/mappings.py#L48)\n        resolution (optional str): Frequency of values; '60T' for hourly values, '30T' for half-hourly values or '15T' for quarterly values\n    \"\"\"\n\n    api_key: str\n    start: str\n    end: str\n    country_code: str\n    resolution: str\n\n    def __init__(\n        self, api_key, start, end, country_code, resolution: str = \"60T\"\n    ) -&gt; None:\n        self.key = api_key\n        self.start = pd.Timestamp(start, tz=\"UTC\")\n        self.end = pd.Timestamp(end, tz=\"UTC\")\n        self.country = country_code\n        self.resolution = resolution\n\n    @staticmethod\n    def system_type():\n\"\"\"\n        Attributes:\n            SystemType (Environment): Requires Python\n        \"\"\"\n        return SystemType.PYTHON\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_read_validation(self):\n        return True\n\n    def post_read_validation(self):\n        return True\n\n    def read_batch(self) -&gt; DataFrame:\n\"\"\"\n        Reads batch from ENTSO-E API.\n        \"\"\"\n        client = EntsoePandasClient(api_key=self.key)\n        df = client.query_day_ahead_prices(self.country, start=self.start, end=self.end)\n        df = pd.DataFrame(df, columns=[\"Price\"])\n        df[\"Name\"] = \"APX\"\n        return df\n\n    def read_stream(self):\n\"\"\"\n        Raises:\n            NotImplementedError: ENTSO-E connector does not support the stream operation.\n        \"\"\"\n        raise NotImplementedError(\n            \"ENTSO-E connector does not support the stream operation.\"\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/python/entsoe/#src.sdk.python.rtdip_sdk.pipelines.sources.python.entsoe.PythonEntsoeSource.read_batch","title":"<code>read_batch()</code>","text":"<p>Reads batch from ENTSO-E API.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/python/entsoe.py</code> <pre><code>def read_batch(self) -&gt; DataFrame:\n\"\"\"\n    Reads batch from ENTSO-E API.\n    \"\"\"\n    client = EntsoePandasClient(api_key=self.key)\n    df = client.query_day_ahead_prices(self.country, start=self.start, end=self.end)\n    df = pd.DataFrame(df, columns=[\"Price\"])\n    df[\"Name\"] = \"APX\"\n    return df\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/python/entsoe/#src.sdk.python.rtdip_sdk.pipelines.sources.python.entsoe.PythonEntsoeSource.read_stream","title":"<code>read_stream()</code>","text":"<p>Raises:</p> Type Description <code>NotImplementedError</code> <p>ENTSO-E connector does not support the stream operation.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/python/entsoe.py</code> <pre><code>def read_stream(self):\n\"\"\"\n    Raises:\n        NotImplementedError: ENTSO-E connector does not support the stream operation.\n    \"\"\"\n    raise NotImplementedError(\n        \"ENTSO-E connector does not support the stream operation.\"\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/python/entsoe/#src.sdk.python.rtdip_sdk.pipelines.sources.python.entsoe.PythonEntsoeSource.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires Python</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/python/entsoe.py</code> <pre><code>@staticmethod\ndef system_type():\n\"\"\"\n    Attributes:\n        SystemType (Environment): Requires Python\n    \"\"\"\n    return SystemType.PYTHON\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/python/mffbas/","title":"Read from MFFBAS API","text":""},{"location":"sdk/code-reference/pipelines/sources/python/mffbas/#src.sdk.python.rtdip_sdk.pipelines.sources.python.mffbas.PythonMFFBASSource","title":"<code>PythonMFFBASSource</code>","text":"<p>             Bases: <code>SourceInterface</code></p> <p>The Python MFFBAS Source is used to read the Standaard Jaar Verbruiksprofielen (Standard Consumption Profiles) from the MFFBAS API. More information on the Standard Consumption Profiles can be found here.</p>"},{"location":"sdk/code-reference/pipelines/sources/python/mffbas/#src.sdk.python.rtdip_sdk.pipelines.sources.python.mffbas.PythonMFFBASSource--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.sources import PythonMFFBASSource\n\nsjv_source = PythonMFFBASSource(\n   start=\"2024-01-01\",\n   end=\"2024-01-02\"\n)\n\nsjv_source.read_batch()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>str</code> <p>Start date in the format YYYY-MM-DD</p> required <code>end</code> <code>str</code> <p>End date in the format YYYY-MM-DD</p> required <p>Note</p> <p>It is not possible to collect fractions over a period before 2023-04-01 with this API. Requests are limited to a maximum of 31 days at a time.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/python/mffbas.py</code> <pre><code>class PythonMFFBASSource(SourceInterface):\n\"\"\"\n    The Python MFFBAS Source is used to read the Standaard Jaar Verbruiksprofielen (Standard Consumption Profiles) from the MFFBAS API. More information on the Standard Consumption Profiles can be found [here](https://www.mffbas.nl/documenten/).\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.sources import PythonMFFBASSource\n\n    sjv_source = PythonMFFBASSource(\n       start=\"2024-01-01\",\n       end=\"2024-01-02\"\n    )\n\n    sjv_source.read_batch()\n    ```\n\n    Args:\n       start (str): Start date in the format YYYY-MM-DD\n       end (str): End date in the format YYYY-MM-DD\n\n    !!! note \"Note\"\n        It is not possible to collect fractions over a period before 2023-04-01 with this API. Requests are limited to a maximum of 31 days at a time.\n\n    \"\"\"\n\n    start: str\n    end: str\n\n    def __init__(self, start, end) -&gt; None:\n        self.start = start\n        self.end = end\n\n    @staticmethod\n    def system_type():\n\"\"\"\n        Attributes:\n              SystemType (Environment): Requires Python\n        \"\"\"\n        return SystemType.PYTHON\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_read_validation(self):\n        return True\n\n    def post_read_validation(self):\n        return True\n\n    def _pull_data(self):\n        url = \"https://gateway.edsn.nl/energyvalues/profile-fractions-series/v1/profile-fractions\"\n\n        parameters = {\n            \"startdate\": self.start,\n            \"enddate\": self.end,\n            \"pftype\": \"STANDARD\",\n            \"product\": \"023\",\n        }\n\n        response = requests.request(\"GET\", url, params=parameters)\n\n        code = response.status_code\n\n        if code != 200:\n            raise HTTPError(\n                f\"Unable to access URL `{url}`.\"\n                f\" Received status code {code} with message {response.content}\"\n            )\n\n        data = response.json()\n\n        return data\n\n    def _prepare_data(self):\n        data = self._pull_data()\n        df = pd.DataFrame.from_dict(data[\"Detail_SeriesList\"])\n\n        df.rename(columns={\"calendar_date\": \"Versienr\"}, inplace=True)\n        df = df.explode(\"PointList\")\n        df = pd.concat(\n            [df.drop([\"PointList\"], axis=1), df[\"PointList\"].apply(pd.Series)], axis=1\n        )\n        df[\"direction\"] = df[\"direction\"].map({\"E17\": \"A\", \"E18\": \"I\"})\n        df[\"profiles\"] = df[\n            [\"profileCategory\", \"determinedConsumption\", \"direction\"]\n        ].agg(lambda x: \"_\".join(x.dropna()), axis=1)\n        df[\"Versienr\"] = pd.to_datetime(df[\"Versienr\"]) + pd.to_timedelta(\n            df[\"pos\"] * 15, unit=\"min\"\n        )\n        df = df[df[\"pos\"] &lt; 96]\n        drop = [\n            \"direction\",\n            \"pFdate_version\",\n            \"profileCategory\",\n            \"determinedConsumption\",\n            \"pos\",\n            \"resolution\",\n            \"profileStatus_quality\",\n        ]\n        df.drop(columns=drop, axis=1, inplace=True)\n\n        result = df.pivot(index=\"Versienr\", columns=\"profiles\", values=\"qnt\")\n        result[\"year_created\"] = result.index.strftime(\"%Y-%m-%d\")\n\n        return result\n\n    def read_batch(self) -&gt; DataFrame:\n\"\"\"\n        Reads batch from the MFFBAS API.\n        \"\"\"\n        try:\n            df = self._prepare_data()\n            return df\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def read_stream(self):\n\"\"\"\n        Raises:\n              NotImplementedError: MFFBAS connector does not support the stream operation.\n        \"\"\"\n        raise NotImplementedError(\n            \"MFFBAS connector does not support the stream operation.\"\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/python/mffbas/#src.sdk.python.rtdip_sdk.pipelines.sources.python.mffbas.PythonMFFBASSource.read_batch","title":"<code>read_batch()</code>","text":"<p>Reads batch from the MFFBAS API.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/python/mffbas.py</code> <pre><code>def read_batch(self) -&gt; DataFrame:\n\"\"\"\n    Reads batch from the MFFBAS API.\n    \"\"\"\n    try:\n        df = self._prepare_data()\n        return df\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/python/mffbas/#src.sdk.python.rtdip_sdk.pipelines.sources.python.mffbas.PythonMFFBASSource.read_stream","title":"<code>read_stream()</code>","text":"<p>Raises:</p> Type Description <code>NotImplementedError</code> <p>MFFBAS connector does not support the stream operation.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/python/mffbas.py</code> <pre><code>def read_stream(self):\n\"\"\"\n    Raises:\n          NotImplementedError: MFFBAS connector does not support the stream operation.\n    \"\"\"\n    raise NotImplementedError(\n        \"MFFBAS connector does not support the stream operation.\"\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/python/mffbas/#src.sdk.python.rtdip_sdk.pipelines.sources.python.mffbas.PythonMFFBASSource.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires Python</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/python/mffbas.py</code> <pre><code>@staticmethod\ndef system_type():\n\"\"\"\n    Attributes:\n          SystemType (Environment): Requires Python\n    \"\"\"\n    return SystemType.PYTHON\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/autoloader/","title":"Read from Autoloader","text":""},{"location":"sdk/code-reference/pipelines/sources/spark/autoloader/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.autoloader.DataBricksAutoLoaderSource","title":"<code>DataBricksAutoLoaderSource</code>","text":"<p>             Bases: <code>SourceInterface</code></p> <p>The Spark Auto Loader is used to read new data files as they arrive in cloud storage. Further information on Auto Loader is available here</p>"},{"location":"sdk/code-reference/pipelines/sources/spark/autoloader/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.autoloader.DataBricksAutoLoaderSource--example","title":"Example","text":"ADLS Gen2AWS S3GCS <pre><code>from rtdip_sdk.pipelines.sources import DataBricksAutoLoaderSource\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\noptions = {}\npath = \"abfss://{FILE-SYSTEM}@{ACCOUNT-NAME}.dfs.core.windows.net/{PATH}/{FILE-NAME}\nformat = \"{DESIRED-FILE-FORMAT}\"\n\nDataBricksAutoLoaderSource(spark, options, path, format).read_stream()\n\nOR\n\nDataBricksAutoLoaderSource(spark, options, path, format).read_batch()\n</code></pre> <pre><code>from rtdip_sdk.pipelines.sources import DataBricksAutoLoaderSource\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\noptions = {}\npath = \"https://s3.{REGION-CODE}.amazonaws.com/{BUCKET-NAME}/{KEY-NAME}\"\nformat = \"{DESIRED-FILE-FORMAT}\"\n\nDataBricksAutoLoaderSource(spark, options, path, format).read_stream()\n\nOR\n\nDataBricksAutoLoaderSource(spark, options, path, format).read_batch()\n</code></pre> <pre><code>from rtdip_sdk.pipelines.sources import DataBricksAutoLoaderSource\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\noptions = {}\npath = \"gs://{BUCKET-NAME}/{FILE-PATH}\"\nformat = \"{DESIRED-FILE-FORMAT}\"\n\nDataBricksAutoLoaderSource(spark, options, path, format).read_stream()\n\nOR\n\nDataBricksAutoLoaderSource(spark, options, path, format).read_batch()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session required to read data from cloud storage</p> required <code>options</code> <code>dict</code> <p>Options that can be specified for configuring the Auto Loader. Further information on the options available are here</p> required <code>path</code> <code>str</code> <p>The cloud storage path</p> required <code>format</code> <code>str</code> <p>Specifies the file format to be read. Supported formats are available here</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/autoloader.py</code> <pre><code>class DataBricksAutoLoaderSource(SourceInterface):\n\"\"\"\n    The Spark Auto Loader is used to read new data files as they arrive in cloud storage. Further information on Auto Loader is available [here](https://docs.databricks.com/ingestion/auto-loader/index.html)\n\n    Example\n    --------\n    === \"ADLS Gen2\"\n\n        ```python\n        from rtdip_sdk.pipelines.sources import DataBricksAutoLoaderSource\n        from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n        # Not required if using Databricks\n        spark = SparkSessionUtility(config={}).execute()\n\n        options = {}\n        path = \"abfss://{FILE-SYSTEM}@{ACCOUNT-NAME}.dfs.core.windows.net/{PATH}/{FILE-NAME}\n        format = \"{DESIRED-FILE-FORMAT}\"\n\n        DataBricksAutoLoaderSource(spark, options, path, format).read_stream()\n\n        OR\n\n        DataBricksAutoLoaderSource(spark, options, path, format).read_batch()\n        ```\n    === \"AWS S3\"\n\n        ```python\n        from rtdip_sdk.pipelines.sources import DataBricksAutoLoaderSource\n        from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n        # Not required if using Databricks\n        spark = SparkSessionUtility(config={}).execute()\n\n        options = {}\n        path = \"https://s3.{REGION-CODE}.amazonaws.com/{BUCKET-NAME}/{KEY-NAME}\"\n        format = \"{DESIRED-FILE-FORMAT}\"\n\n        DataBricksAutoLoaderSource(spark, options, path, format).read_stream()\n\n        OR\n\n        DataBricksAutoLoaderSource(spark, options, path, format).read_batch()\n        ```\n    === \"GCS\"\n\n        ```python\n        from rtdip_sdk.pipelines.sources import DataBricksAutoLoaderSource\n        from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n        # Not required if using Databricks\n        spark = SparkSessionUtility(config={}).execute()\n\n        options = {}\n        path = \"gs://{BUCKET-NAME}/{FILE-PATH}\"\n        format = \"{DESIRED-FILE-FORMAT}\"\n\n        DataBricksAutoLoaderSource(spark, options, path, format).read_stream()\n\n        OR\n\n        DataBricksAutoLoaderSource(spark, options, path, format).read_batch()\n        ```\n\n    Parameters:\n        spark (SparkSession): Spark Session required to read data from cloud storage\n        options (dict): Options that can be specified for configuring the Auto Loader. Further information on the options available are [here](https://docs.databricks.com/ingestion/auto-loader/options.html)\n        path (str): The cloud storage path\n        format (str): Specifies the file format to be read. Supported formats are available [here](https://docs.databricks.com/ingestion/auto-loader/options.html#file-format-options)\n    \"\"\"\n\n    spark: SparkSession\n    options: dict\n    path: str\n\n    def __init__(\n        self, spark: SparkSession, options: dict, path: str, format: str\n    ) -&gt; None:\n        self.spark = spark\n        self.options = options\n        self.path = path\n        self.options[\"cloudFiles.format\"] = format\n\n    @staticmethod\n    def system_type():\n\"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK on Databricks\n        \"\"\"\n        return SystemType.PYSPARK_DATABRICKS\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        libraries.add_maven_library(get_default_package(\"spark_delta_core\"))\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_read_validation(self):\n        return True\n\n    def post_read_validation(self, df: DataFrame):\n        return True\n\n    def read_batch(self):\n\"\"\"\n        Raises:\n            NotImplementedError: Auto Loader only supports streaming reads. To perform a batch read, use the read_stream method of this component and specify the Trigger on the write_stream to be `availableNow` to perform batch-like reads of cloud storage files.\n        \"\"\"\n        raise NotImplementedError(\n            \"Auto Loader only supports streaming reads. To perform a batch read, use the read_stream method and specify Trigger on the write_stream as `availableNow`\"\n        )\n\n    def read_stream(self) -&gt; DataFrame:\n\"\"\"\n        Performs streaming reads of files in cloud storage.\n        \"\"\"\n        try:\n            return (\n                self.spark.readStream.format(\"cloudFiles\")\n                .options(**self.options)\n                .load(self.path)\n            )\n\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/autoloader/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.autoloader.DataBricksAutoLoaderSource.read_batch","title":"<code>read_batch()</code>","text":"<p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Auto Loader only supports streaming reads. To perform a batch read, use the read_stream method of this component and specify the Trigger on the write_stream to be <code>availableNow</code> to perform batch-like reads of cloud storage files.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/autoloader.py</code> <pre><code>def read_batch(self):\n\"\"\"\n    Raises:\n        NotImplementedError: Auto Loader only supports streaming reads. To perform a batch read, use the read_stream method of this component and specify the Trigger on the write_stream to be `availableNow` to perform batch-like reads of cloud storage files.\n    \"\"\"\n    raise NotImplementedError(\n        \"Auto Loader only supports streaming reads. To perform a batch read, use the read_stream method and specify Trigger on the write_stream as `availableNow`\"\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/autoloader/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.autoloader.DataBricksAutoLoaderSource.read_stream","title":"<code>read_stream()</code>","text":"<p>Performs streaming reads of files in cloud storage.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/autoloader.py</code> <pre><code>def read_stream(self) -&gt; DataFrame:\n\"\"\"\n    Performs streaming reads of files in cloud storage.\n    \"\"\"\n    try:\n        return (\n            self.spark.readStream.format(\"cloudFiles\")\n            .options(**self.options)\n            .load(self.path)\n        )\n\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/autoloader/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.autoloader.DataBricksAutoLoaderSource.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK on Databricks</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/autoloader.py</code> <pre><code>@staticmethod\ndef system_type():\n\"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK on Databricks\n    \"\"\"\n    return SystemType.PYSPARK_DATABRICKS\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/delta/","title":"Read from Delta","text":""},{"location":"sdk/code-reference/pipelines/sources/spark/delta/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.delta.SparkDeltaSource","title":"<code>SparkDeltaSource</code>","text":"<p>             Bases: <code>SourceInterface</code></p> <p>The Spark Delta Source is used to read data from a Delta table.</p>"},{"location":"sdk/code-reference/pipelines/sources/spark/delta/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.delta.SparkDeltaSource--example","title":"Example","text":"<p><pre><code>#Delta Source for Streaming Queries\n\nfrom rtdip_sdk.pipelines.sources import SparkDeltaSource\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\ndelta_source = SparkDeltaSource(\n    spark=spark,\n    options={\n        \"maxFilesPerTrigger\": 1000,\n        \"ignoreChanges: True,\n        \"startingVersion\": 0\n    },\n    table_name=\"{YOUR-DELTA-TABLE-PATH}\"\n)\n\ndelta_source.read_stream()\n</code></pre> <pre><code>#Delta Source for Batch Queries\n\nfrom rtdip_sdk.pipelines.sources import SparkDeltaSource\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\ndelta_source = SparkDeltaSource(\n    spark=spark,\n    options={\n        \"versionAsOf\": 0,\n        \"timestampAsOf\": \"yyyy-mm-dd hh:mm:ss[.fffffffff]\"\n    },\n    table_name=\"{YOUR-DELTA-TABLE-PATH}\"\n)\n\ndelta_source.read_batch()\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session required to read data from a Delta table.</p> required <code>options</code> <code>dict</code> <p>Options that can be specified for a Delta Table read operation (See Attributes table below). Further information on the options is available for batch and streaming.</p> required <code>table_name</code> <code>str</code> <p>Name of the Hive Metastore or Unity Catalog Delta Table</p> required <p>Attributes:</p> Name Type Description <code>maxFilesPerTrigger</code> <code>int</code> <p>How many new files to be considered in every micro-batch. The default is 1000. (Streaming)</p> <code>maxBytesPerTrigger</code> <code>int</code> <p>How much data gets processed in each micro-batch. (Streaming)</p> <code>ignoreDeletes</code> <code>bool str</code> <p>Ignore transactions that delete data at partition boundaries. (Streaming)</p> <code>ignoreChanges</code> <code>bool str</code> <p>Pre-process updates if files had to be rewritten in the source table due to a data changing operation. (Streaming)</p> <code>startingVersion</code> <code>int str</code> <p>The Delta Lake version to start from. (Streaming)</p> <code>startingTimestamp</code> <code>datetime str</code> <p>The timestamp to start from. (Streaming)</p> <code>withEventTimeOrder</code> <code>bool str</code> <p>Whether the initial snapshot should be processed with event time order. (Streaming)</p> <code>timestampAsOf</code> <code>datetime str</code> <p>Query the Delta Table from a specific point in time. (Batch)</p> <code>versionAsOf</code> <code>int str</code> <p>Query the Delta Table from a specific version. (Batch)</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/delta.py</code> <pre><code>class SparkDeltaSource(SourceInterface):\n\"\"\"\n    The Spark Delta Source is used to read data from a Delta table.\n\n    Example\n    --------\n    ```python\n    #Delta Source for Streaming Queries\n\n    from rtdip_sdk.pipelines.sources import SparkDeltaSource\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    delta_source = SparkDeltaSource(\n        spark=spark,\n        options={\n            \"maxFilesPerTrigger\": 1000,\n            \"ignoreChanges: True,\n            \"startingVersion\": 0\n        },\n        table_name=\"{YOUR-DELTA-TABLE-PATH}\"\n    )\n\n    delta_source.read_stream()\n    ```\n    ```python\n    #Delta Source for Batch Queries\n\n    from rtdip_sdk.pipelines.sources import SparkDeltaSource\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    delta_source = SparkDeltaSource(\n        spark=spark,\n        options={\n            \"versionAsOf\": 0,\n            \"timestampAsOf\": \"yyyy-mm-dd hh:mm:ss[.fffffffff]\"\n        },\n        table_name=\"{YOUR-DELTA-TABLE-PATH}\"\n    )\n\n    delta_source.read_batch()\n    ```\n\n    Parameters:\n        spark (SparkSession): Spark Session required to read data from a Delta table.\n        options (dict): Options that can be specified for a Delta Table read operation (See Attributes table below). Further information on the options is available for [batch](https://docs.delta.io/latest/delta-batch.html#read-a-table){ target=\"_blank\" } and [streaming](https://docs.delta.io/latest/delta-streaming.html#delta-table-as-a-source){ target=\"_blank\" }.\n        table_name (str): Name of the Hive Metastore or Unity Catalog Delta Table\n\n    Attributes:\n        maxFilesPerTrigger (int): How many new files to be considered in every micro-batch. The default is 1000. (Streaming)\n        maxBytesPerTrigger (int): How much data gets processed in each micro-batch. (Streaming)\n        ignoreDeletes (bool str): Ignore transactions that delete data at partition boundaries. (Streaming)\n        ignoreChanges (bool str): Pre-process updates if files had to be rewritten in the source table due to a data changing operation. (Streaming)\n        startingVersion (int str): The Delta Lake version to start from. (Streaming)\n        startingTimestamp (datetime str): The timestamp to start from. (Streaming)\n        withEventTimeOrder (bool str): Whether the initial snapshot should be processed with event time order. (Streaming)\n        timestampAsOf (datetime str): Query the Delta Table from a specific point in time. (Batch)\n        versionAsOf (int str): Query the Delta Table from a specific version. (Batch)\n    \"\"\"\n\n    spark: SparkSession\n    options: dict\n    table_name: str\n\n    def __init__(self, spark: SparkSession, options: dict, table_name: str) -&gt; None:\n        self.spark = spark\n        self.options = options\n        self.table_name = table_name\n\n    @staticmethod\n    def system_type():\n\"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        libraries.add_maven_library(get_default_package(\"spark_delta_core\"))\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_read_validation(self):\n        return True\n\n    def post_read_validation(self):\n        return True\n\n    def read_batch(self):\n\"\"\"\n        Reads batch data from Delta. Most of the options provided by the Apache Spark DataFrame read API are supported for performing batch reads on Delta tables.\n        \"\"\"\n        try:\n            return (\n                self.spark.read.format(\"delta\")\n                .options(**self.options)\n                .table(self.table_name)\n            )\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def read_stream(self) -&gt; DataFrame:\n\"\"\"\n        Reads streaming data from Delta. All of the data in the table is processed as well as any new data that arrives after the stream started. .load() can take table name or path.\n        \"\"\"\n        try:\n            return (\n                self.spark.readStream.format(\"delta\")\n                .options(**self.options)\n                .load(self.table_name)\n            )\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/delta/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.delta.SparkDeltaSource.read_batch","title":"<code>read_batch()</code>","text":"<p>Reads batch data from Delta. Most of the options provided by the Apache Spark DataFrame read API are supported for performing batch reads on Delta tables.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/delta.py</code> <pre><code>def read_batch(self):\n\"\"\"\n    Reads batch data from Delta. Most of the options provided by the Apache Spark DataFrame read API are supported for performing batch reads on Delta tables.\n    \"\"\"\n    try:\n        return (\n            self.spark.read.format(\"delta\")\n            .options(**self.options)\n            .table(self.table_name)\n        )\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/delta/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.delta.SparkDeltaSource.read_stream","title":"<code>read_stream()</code>","text":"<p>Reads streaming data from Delta. All of the data in the table is processed as well as any new data that arrives after the stream started. .load() can take table name or path.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/delta.py</code> <pre><code>def read_stream(self) -&gt; DataFrame:\n\"\"\"\n    Reads streaming data from Delta. All of the data in the table is processed as well as any new data that arrives after the stream started. .load() can take table name or path.\n    \"\"\"\n    try:\n        return (\n            self.spark.readStream.format(\"delta\")\n            .options(**self.options)\n            .load(self.table_name)\n        )\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/delta/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.delta.SparkDeltaSource.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/delta.py</code> <pre><code>@staticmethod\ndef system_type():\n\"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/delta_sharing/","title":"Read from Delta sharing","text":""},{"location":"sdk/code-reference/pipelines/sources/spark/delta_sharing/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.delta_sharing.SparkDeltaSharingSource","title":"<code>SparkDeltaSharingSource</code>","text":"<p>             Bases: <code>SourceInterface</code></p> <p>The Spark Delta Sharing Source is used to read data from a Delta table where Delta sharing is configured</p>"},{"location":"sdk/code-reference/pipelines/sources/spark/delta_sharing/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.delta_sharing.SparkDeltaSharingSource--example","title":"Example","text":"<p><pre><code>#Delta Sharing Source for Streaming Queries\n\nfrom rtdip_sdk.pipelines.sources import SparkDeltaSharingSource\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\ndelta_sharing_source = SparkDeltaSharingSource(\n    spark=spark,\n    options={\n        \"maxFilesPerTrigger\": 1000,\n        \"ignoreChanges: True,\n        \"startingVersion\": 0\n    },\n    table_name=\"{YOUR-DELTA-TABLE-PATH}\"\n)\n\ndelta_sharing_source.read_stream()\n</code></pre> <pre><code>#Delta Sharing Source for Batch Queries\n\nfrom rtdip_sdk.pipelines.sources import SparkDeltaSharingSource\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\ndelta_sharing_source = SparkDeltaSharingSource(\n    spark=spark,\n    options={\n        \"versionAsOf\": 0,\n        \"timestampAsOf\": \"yyyy-mm-dd hh:mm:ss[.fffffffff]\"\n    },\n    table_name=\"{YOUR-DELTA-TABLE-PATH}\"\n)\n\ndelta_sharing_source.read_batch()\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session required to read data from a Delta table</p> required <code>options</code> <code>dict</code> <p>Options that can be specified for a Delta Table read operation (See Attributes table below). Further information on the options is available here</p> required <code>table_path</code> <code>str</code> <p>Path to credentials file and Delta table to query</p> required <p>Attributes:</p> Name Type Description <code>ignoreDeletes</code> <code>bool str</code> <p>Ignore transactions that delete data at partition boundaries. (Streaming)</p> <code>ignoreChanges</code> <code>bool str</code> <p>Pre-process updates if files had to be rewritten in the source table due to a data changing operation. (Streaming)</p> <code>startingVersion</code> <code>int str</code> <p>The Delta Lake version to start from. (Streaming)</p> <code>startingTimestamp</code> <code>datetime str</code> <p>The timestamp to start from. (Streaming)</p> <code>maxFilesPerTrigger</code> <code>int</code> <p>How many new files to be considered in every micro-batch. The default is 1000. (Streaming)</p> <code>maxBytesPerTrigger</code> <code>int</code> <p>How much data gets processed in each micro-batch. (Streaming)</p> <code>readChangeFeed</code> <code>bool str</code> <p>Stream read the change data feed of the shared table. (Batch &amp; Streaming)</p> <code>timestampAsOf</code> <code>datetime str</code> <p>Query the Delta Table from a specific point in time. (Batch)</p> <code>versionAsOf</code> <code>int str</code> <p>Query the Delta Table from a specific version. (Batch)</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/delta_sharing.py</code> <pre><code>class SparkDeltaSharingSource(SourceInterface):\n\"\"\"\n    The Spark Delta Sharing Source is used to read data from a Delta table where Delta sharing is configured\n\n    Example\n    --------\n    ```python\n    #Delta Sharing Source for Streaming Queries\n\n    from rtdip_sdk.pipelines.sources import SparkDeltaSharingSource\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    delta_sharing_source = SparkDeltaSharingSource(\n        spark=spark,\n        options={\n            \"maxFilesPerTrigger\": 1000,\n            \"ignoreChanges: True,\n            \"startingVersion\": 0\n        },\n        table_name=\"{YOUR-DELTA-TABLE-PATH}\"\n    )\n\n    delta_sharing_source.read_stream()\n    ```\n    ```python\n    #Delta Sharing Source for Batch Queries\n\n    from rtdip_sdk.pipelines.sources import SparkDeltaSharingSource\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    delta_sharing_source = SparkDeltaSharingSource(\n        spark=spark,\n        options={\n            \"versionAsOf\": 0,\n            \"timestampAsOf\": \"yyyy-mm-dd hh:mm:ss[.fffffffff]\"\n        },\n        table_name=\"{YOUR-DELTA-TABLE-PATH}\"\n    )\n\n    delta_sharing_source.read_batch()\n    ```\n\n    Parameters:\n        spark (SparkSession): Spark Session required to read data from a Delta table\n        options (dict): Options that can be specified for a Delta Table read operation (See Attributes table below). Further information on the options is available [here](https://docs.databricks.com/data-sharing/read-data-open.html#apache-spark-read-shared-data){ target=\"_blank\" }\n        table_path (str): Path to credentials file and Delta table to query\n\n    Attributes:\n        ignoreDeletes (bool str): Ignore transactions that delete data at partition boundaries. (Streaming)\n        ignoreChanges (bool str): Pre-process updates if files had to be rewritten in the source table due to a data changing operation. (Streaming)\n        startingVersion (int str): The Delta Lake version to start from. (Streaming)\n        startingTimestamp (datetime str): The timestamp to start from. (Streaming)\n        maxFilesPerTrigger (int): How many new files to be considered in every micro-batch. The default is 1000. (Streaming)\n        maxBytesPerTrigger (int): How much data gets processed in each micro-batch. (Streaming)\n        readChangeFeed (bool str): Stream read the change data feed of the shared table. (Batch &amp; Streaming)\n        timestampAsOf (datetime str): Query the Delta Table from a specific point in time. (Batch)\n        versionAsOf (int str): Query the Delta Table from a specific version. (Batch)\n    \"\"\"\n\n    spark: SparkSession\n    options: dict\n    table_path: str\n\n    def __init__(self, spark: SparkSession, options: dict, table_path: str) -&gt; None:\n        self.spark = spark\n        self.options = options\n        self.table_path = table_path\n\n    @staticmethod\n    def system_type():\n\"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        libraries.add_maven_library(get_default_package(\"spark_delta_sharing\"))\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_read_validation(self):\n        return True\n\n    def post_read_validation(self):\n        return True\n\n    def read_batch(self):\n\"\"\"\n        Reads batch data from Delta. Most of the options provided by the Apache Spark DataFrame read API are supported for performing batch reads on Delta tables.\n        \"\"\"\n        try:\n            return (\n                self.spark.read.format(\"deltaSharing\")\n                .options(**self.options)\n                .table(self.table_path)\n            )\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def read_stream(self) -&gt; DataFrame:\n\"\"\"\n        Reads streaming data from Delta. All of the data in the table is processed as well as any new data that arrives after the stream started. .load() can take table name or path.\n        \"\"\"\n        try:\n            return (\n                self.spark.readStream.format(\"deltaSharing\")\n                .options(**self.options)\n                .load(self.table_path)\n            )\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/delta_sharing/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.delta_sharing.SparkDeltaSharingSource.read_batch","title":"<code>read_batch()</code>","text":"<p>Reads batch data from Delta. Most of the options provided by the Apache Spark DataFrame read API are supported for performing batch reads on Delta tables.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/delta_sharing.py</code> <pre><code>def read_batch(self):\n\"\"\"\n    Reads batch data from Delta. Most of the options provided by the Apache Spark DataFrame read API are supported for performing batch reads on Delta tables.\n    \"\"\"\n    try:\n        return (\n            self.spark.read.format(\"deltaSharing\")\n            .options(**self.options)\n            .table(self.table_path)\n        )\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/delta_sharing/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.delta_sharing.SparkDeltaSharingSource.read_stream","title":"<code>read_stream()</code>","text":"<p>Reads streaming data from Delta. All of the data in the table is processed as well as any new data that arrives after the stream started. .load() can take table name or path.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/delta_sharing.py</code> <pre><code>def read_stream(self) -&gt; DataFrame:\n\"\"\"\n    Reads streaming data from Delta. All of the data in the table is processed as well as any new data that arrives after the stream started. .load() can take table name or path.\n    \"\"\"\n    try:\n        return (\n            self.spark.readStream.format(\"deltaSharing\")\n            .options(**self.options)\n            .load(self.table_path)\n        )\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/delta_sharing/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.delta_sharing.SparkDeltaSharingSource.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/delta_sharing.py</code> <pre><code>@staticmethod\ndef system_type():\n\"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/eventhub/","title":"Read from an Eventhub","text":""},{"location":"sdk/code-reference/pipelines/sources/spark/eventhub/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.eventhub.SparkEventhubSource","title":"<code>SparkEventhubSource</code>","text":"<p>             Bases: <code>SourceInterface</code></p> <p>This Spark source class is used to read batch or streaming data from Eventhubs. Eventhub configurations need to be specified as options in a dictionary. Additionally, there are more optional configurations which can be found here. If using startingPosition or endingPosition make sure to check out the Event Position section for more details and examples.</p>"},{"location":"sdk/code-reference/pipelines/sources/spark/eventhub/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.eventhub.SparkEventhubSource--example","title":"Example","text":"<p><pre><code>#Eventhub Source for Streaming Queries\n\nfrom rtdip_sdk.pipelines.sources import SparkEventhubSource\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\nimport json\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\nconnectionString = \"Endpoint=sb://{NAMESPACE}.servicebus.windows.net/;SharedAccessKeyName={ACCESS_KEY_NAME};SharedAccessKey={ACCESS_KEY}=;EntityPath={EVENT_HUB_NAME}\"\n\nstartingEventPosition = {\n\"offset\": -1,\n\"seqNo\": -1,\n\"enqueuedTime\": None,\n\"isInclusive\": True\n}\n\neventhub_source = SparkEventhubSource(\n    spark=spark,\n    options = {\n        \"eventhubs.connectionString\": connectionString,\n        \"eventhubs.consumerGroup\": \"{YOUR-CONSUMER-GROUP}\",\n        \"eventhubs.startingPosition\": json.dumps(startingEventPosition),\n        \"maxEventsPerTrigger\" : 1000\n    }\n)\n\neventhub_source.read_stream()\n</code></pre> <pre><code> #Eventhub Source for Batch Queries\n\nfrom rtdip_sdk.pipelines.sources import SparkEventhubSource\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\nimport json\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\nconnectionString = \"Endpoint=sb://{NAMESPACE}.servicebus.windows.net/;SharedAccessKeyName={ACCESS_KEY_NAME};SharedAccessKey={ACCESS_KEY}=;EntityPath={EVENT_HUB_NAME}\"\n\nstartingEventPosition = {\n    \"offset\": -1,\n    \"seqNo\": -1,\n    \"enqueuedTime\": None,\n    \"isInclusive\": True\n}\n\nendingEventPosition = {\n    \"offset\": None,\n    \"seqNo\": -1,\n    \"enqueuedTime\": endTime,\n    \"isInclusive\": True\n}\n\neventhub_source = SparkEventhubSource(\n    spark,\n    options = {\n        \"eventhubs.connectionString\": connectionString,\n        \"eventhubs.consumerGroup\": \"{YOUR-CONSUMER-GROUP}\",\n        \"eventhubs.startingPosition\": json.dumps(startingEventPosition),\n        \"eventhubs.endingPosition\": json.dumps(endingEventPosition)\n    }\n)\n\neventhub_source.read_batch()\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session</p> required <code>options</code> <code>dict</code> <p>A dictionary of Eventhub configurations (See Attributes table below)</p> required <p>Attributes:</p> Name Type Description <code>eventhubs.connectionString</code> <code>str</code> <p>Eventhubs connection string is required to connect to the Eventhubs service. (Streaming and Batch)</p> <code>eventhubs.consumerGroup</code> <code>str</code> <p>A consumer group is a view of an entire eventhub. Consumer groups enable multiple consuming applications to each have a separate view of the event stream, and to read the stream independently at their own pace and with their own offsets. (Streaming and Batch)</p> <code>eventhubs.startingPosition</code> <code>JSON str</code> <p>The starting position for your Structured Streaming job. If a specific EventPosition is not set for a partition using startingPositions, then we use the EventPosition set in startingPosition. If nothing is set in either option, we will begin consuming from the end of the partition. (Streaming and Batch)</p> <code>eventhubs.endingPosition</code> <code>JSON str</code> <p>(JSON str): The ending position of a batch query. This works the same as startingPosition. (Batch)</p> <code>maxEventsPerTrigger</code> <code>long</code> <p>Rate limit on maximum number of events processed per trigger interval. The specified total number of events will be proportionally split across partitions of different volume. (Stream)</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/eventhub.py</code> <pre><code>class SparkEventhubSource(SourceInterface):\n\"\"\"\n    This Spark source class is used to read batch or streaming data from Eventhubs. Eventhub configurations need to be specified as options in a dictionary.\n    Additionally, there are more optional configurations which can be found [here.](https://github.com/Azure/azure-event-hubs-spark/blob/master/docs/PySpark/structured-streaming-pyspark.md#event-hubs-configuration){ target=\"_blank\" }\n    If using startingPosition or endingPosition make sure to check out the **Event Position** section for more details and examples.\n\n    Example\n    --------\n    ```python\n    #Eventhub Source for Streaming Queries\n\n    from rtdip_sdk.pipelines.sources import SparkEventhubSource\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n    import json\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    connectionString = \"Endpoint=sb://{NAMESPACE}.servicebus.windows.net/;SharedAccessKeyName={ACCESS_KEY_NAME};SharedAccessKey={ACCESS_KEY}=;EntityPath={EVENT_HUB_NAME}\"\n\n    startingEventPosition = {\n    \"offset\": -1,\n    \"seqNo\": -1,\n    \"enqueuedTime\": None,\n    \"isInclusive\": True\n    }\n\n    eventhub_source = SparkEventhubSource(\n        spark=spark,\n        options = {\n            \"eventhubs.connectionString\": connectionString,\n            \"eventhubs.consumerGroup\": \"{YOUR-CONSUMER-GROUP}\",\n            \"eventhubs.startingPosition\": json.dumps(startingEventPosition),\n            \"maxEventsPerTrigger\" : 1000\n        }\n    )\n\n    eventhub_source.read_stream()\n    ```\n    ```python\n     #Eventhub Source for Batch Queries\n\n    from rtdip_sdk.pipelines.sources import SparkEventhubSource\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n    import json\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    connectionString = \"Endpoint=sb://{NAMESPACE}.servicebus.windows.net/;SharedAccessKeyName={ACCESS_KEY_NAME};SharedAccessKey={ACCESS_KEY}=;EntityPath={EVENT_HUB_NAME}\"\n\n    startingEventPosition = {\n        \"offset\": -1,\n        \"seqNo\": -1,\n        \"enqueuedTime\": None,\n        \"isInclusive\": True\n    }\n\n    endingEventPosition = {\n        \"offset\": None,\n        \"seqNo\": -1,\n        \"enqueuedTime\": endTime,\n        \"isInclusive\": True\n    }\n\n    eventhub_source = SparkEventhubSource(\n        spark,\n        options = {\n            \"eventhubs.connectionString\": connectionString,\n            \"eventhubs.consumerGroup\": \"{YOUR-CONSUMER-GROUP}\",\n            \"eventhubs.startingPosition\": json.dumps(startingEventPosition),\n            \"eventhubs.endingPosition\": json.dumps(endingEventPosition)\n        }\n    )\n\n    eventhub_source.read_batch()\n    ```\n\n    Parameters:\n        spark (SparkSession): Spark Session\n        options (dict): A dictionary of Eventhub configurations (See Attributes table below)\n\n    Attributes:\n        eventhubs.connectionString (str):  Eventhubs connection string is required to connect to the Eventhubs service. (Streaming and Batch)\n        eventhubs.consumerGroup (str): A consumer group is a view of an entire eventhub. Consumer groups enable multiple consuming applications to each have a separate view of the event stream, and to read the stream independently at their own pace and with their own offsets. (Streaming and Batch)\n        eventhubs.startingPosition (JSON str): The starting position for your Structured Streaming job. If a specific EventPosition is not set for a partition using startingPositions, then we use the EventPosition set in startingPosition. If nothing is set in either option, we will begin consuming from the end of the partition. (Streaming and Batch)\n        eventhubs.endingPosition: (JSON str): The ending position of a batch query. This works the same as startingPosition. (Batch)\n        maxEventsPerTrigger (long): Rate limit on maximum number of events processed per trigger interval. The specified total number of events will be proportionally split across partitions of different volume. (Stream)\n\n    \"\"\"\n\n    spark: SparkSession\n    options: dict\n\n    def __init__(self, spark: SparkSession, options: dict) -&gt; None:\n        self.spark = spark\n        self.options = options\n        self.schema = EVENTHUB_SCHEMA\n\n    @staticmethod\n    def system_type():\n\"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        spark_libraries = Libraries()\n        spark_libraries.add_maven_library(get_default_package(\"spark_azure_eventhub\"))\n        return spark_libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_read_validation(self) -&gt; bool:\n        return True\n\n    def post_read_validation(self, df: DataFrame) -&gt; bool:\n        assert df.schema == self.schema\n        return True\n\n    def read_batch(self) -&gt; DataFrame:\n\"\"\"\n        Reads batch data from Eventhubs.\n        \"\"\"\n        eventhub_connection_string = \"eventhubs.connectionString\"\n        try:\n            if eventhub_connection_string in self.options:\n                sc = self.spark.sparkContext\n                self.options[\n                    eventhub_connection_string\n                ] = sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(\n                    self.options[eventhub_connection_string]\n                )\n\n            return self.spark.read.format(\"eventhubs\").options(**self.options).load()\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def read_stream(self) -&gt; DataFrame:\n\"\"\"\n        Reads streaming data from Eventhubs.\n        \"\"\"\n        eventhub_connection_string = \"eventhubs.connectionString\"\n        try:\n            if eventhub_connection_string in self.options:\n                sc = self.spark.sparkContext\n                self.options[\n                    eventhub_connection_string\n                ] = sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(\n                    self.options[eventhub_connection_string]\n                )\n\n            return (\n                self.spark.readStream.format(\"eventhubs\").options(**self.options).load()\n            )\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/eventhub/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.eventhub.SparkEventhubSource.read_batch","title":"<code>read_batch()</code>","text":"<p>Reads batch data from Eventhubs.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/eventhub.py</code> <pre><code>def read_batch(self) -&gt; DataFrame:\n\"\"\"\n    Reads batch data from Eventhubs.\n    \"\"\"\n    eventhub_connection_string = \"eventhubs.connectionString\"\n    try:\n        if eventhub_connection_string in self.options:\n            sc = self.spark.sparkContext\n            self.options[\n                eventhub_connection_string\n            ] = sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(\n                self.options[eventhub_connection_string]\n            )\n\n        return self.spark.read.format(\"eventhubs\").options(**self.options).load()\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/eventhub/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.eventhub.SparkEventhubSource.read_stream","title":"<code>read_stream()</code>","text":"<p>Reads streaming data from Eventhubs.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/eventhub.py</code> <pre><code>def read_stream(self) -&gt; DataFrame:\n\"\"\"\n    Reads streaming data from Eventhubs.\n    \"\"\"\n    eventhub_connection_string = \"eventhubs.connectionString\"\n    try:\n        if eventhub_connection_string in self.options:\n            sc = self.spark.sparkContext\n            self.options[\n                eventhub_connection_string\n            ] = sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(\n                self.options[eventhub_connection_string]\n            )\n\n        return (\n            self.spark.readStream.format(\"eventhubs\").options(**self.options).load()\n        )\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/eventhub/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.eventhub.SparkEventhubSource.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/eventhub.py</code> <pre><code>@staticmethod\ndef system_type():\n\"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iot_hub/","title":"Read from an IoT Hub","text":""},{"location":"sdk/code-reference/pipelines/sources/spark/iot_hub/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iot_hub.SparkIoThubSource","title":"<code>SparkIoThubSource</code>","text":"<p>             Bases: <code>SourceInterface</code></p> <p>This Spark source class is used to read batch or streaming data from an IoT Hub. IoT Hub configurations need to be specified as options in a dictionary. Additionally, there are more optional configurations which can be found here. If using startingPosition or endingPosition make sure to check out the Event Position section for more details and examples.</p>"},{"location":"sdk/code-reference/pipelines/sources/spark/iot_hub/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iot_hub.SparkIoThubSource--example","title":"Example","text":"<p><pre><code>#IoT Hub Source for Streaming Queries\n\nfrom rtdip_sdk.pipelines.sources import SparkIoThubSource\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\nimport json\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\nconnectionString = \"Endpoint=sb://{NAMESPACE}.servicebus.windows.net/;SharedAccessKeyName={ACCESS_KEY_NAME};SharedAccessKey={ACCESS_KEY}=;EntityPath={EVENT_HUB_NAME}\"\n\nstartingEventPosition = {\n\"offset\": -1,\n\"seqNo\": -1,\n\"enqueuedTime\": None,\n\"isInclusive\": True\n}\n\niot_hub_source = SparkIoThubSource(\n    spark=spark,\n    options = {\n        \"eventhubs.connectionString\": connectionString,\n        \"eventhubs.consumerGroup\": \"{YOUR-CONSUMER-GROUP}\",\n        \"eventhubs.startingPosition\": json.dumps(startingEventPosition),\n        \"maxEventsPerTrigger\" : 1000\n    }\n)\n\niot_hub_source.read_stream()\n</code></pre> <pre><code> #IoT Hub Source for Batch Queries\n\nfrom rtdip_sdk.pipelines.sources import SparkIoThubSource\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\nimport json\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\nconnectionString = \"Endpoint=sb://{NAMESPACE}.servicebus.windows.net/;SharedAccessKeyName={ACCESS_KEY_NAME};SharedAccessKey={ACCESS_KEY}=;EntityPath={EVENT_HUB_NAME}\"\n\nstartingEventPosition = {\n    \"offset\": -1,\n    \"seqNo\": -1,\n    \"enqueuedTime\": None,\n    \"isInclusive\": True\n}\n\nendingEventPosition = {\n    \"offset\": None,\n    \"seqNo\": -1,\n    \"enqueuedTime\": endTime,\n    \"isInclusive\": True\n}\n\niot_hub_source = SparkIoThubSource(\n    spark,\n    options = {\n        \"eventhubs.connectionString\": connectionString,\n        \"eventhubs.consumerGroup\": \"{YOUR-CONSUMER-GROUP}\",\n        \"eventhubs.startingPosition\": json.dumps(startingEventPosition),\n        \"eventhubs.endingPosition\": json.dumps(endingEventPosition)\n    }\n)\n\niot_hub_source.read_batch()\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session</p> required <code>options</code> <code>dict</code> <p>A dictionary of IoT Hub configurations (See Attributes table below)</p> required <p>Attributes:</p> Name Type Description <code>eventhubs.connectionString</code> <code>str</code> <p>IoT Hub connection string is required to connect to the Eventhubs service. (Streaming and Batch)</p> <code>eventhubs.consumerGroup</code> <code>str</code> <p>A consumer group is a view of an entire IoT Hub. Consumer groups enable multiple consuming applications to each have a separate view of the event stream, and to read the stream independently at their own pace and with their own offsets. (Streaming and Batch)</p> <code>eventhubs.startingPosition</code> <code>JSON str</code> <p>The starting position for your Structured Streaming job. If a specific EventPosition is not set for a partition using startingPositions, then we use the EventPosition set in startingPosition. If nothing is set in either option, we will begin consuming from the end of the partition. (Streaming and Batch)</p> <code>eventhubs.endingPosition</code> <code>JSON str</code> <p>(JSON str): The ending position of a batch query. This works the same as startingPosition. (Batch)</p> <code>maxEventsPerTrigger</code> <code>long</code> <p>Rate limit on maximum number of events processed per trigger interval. The specified total number of events will be proportionally split across partitions of different volume. (Stream)</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iot_hub.py</code> <pre><code>class SparkIoThubSource(SourceInterface):\n\"\"\"\n    This Spark source class is used to read batch or streaming data from an IoT Hub. IoT Hub configurations need to be specified as options in a dictionary.\n    Additionally, there are more optional configurations which can be found [here.](https://github.com/Azure/azure-event-hubs-spark/blob/master/docs/PySpark/structured-streaming-pyspark.md#event-hubs-configuration){ target=\"_blank\" }\n    If using startingPosition or endingPosition make sure to check out the **Event Position** section for more details and examples.\n\n    Example\n    --------\n    ```python\n    #IoT Hub Source for Streaming Queries\n\n    from rtdip_sdk.pipelines.sources import SparkIoThubSource\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n    import json\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    connectionString = \"Endpoint=sb://{NAMESPACE}.servicebus.windows.net/;SharedAccessKeyName={ACCESS_KEY_NAME};SharedAccessKey={ACCESS_KEY}=;EntityPath={EVENT_HUB_NAME}\"\n\n    startingEventPosition = {\n    \"offset\": -1,\n    \"seqNo\": -1,\n    \"enqueuedTime\": None,\n    \"isInclusive\": True\n    }\n\n    iot_hub_source = SparkIoThubSource(\n        spark=spark,\n        options = {\n            \"eventhubs.connectionString\": connectionString,\n            \"eventhubs.consumerGroup\": \"{YOUR-CONSUMER-GROUP}\",\n            \"eventhubs.startingPosition\": json.dumps(startingEventPosition),\n            \"maxEventsPerTrigger\" : 1000\n        }\n    )\n\n    iot_hub_source.read_stream()\n    ```\n    ```python\n     #IoT Hub Source for Batch Queries\n\n    from rtdip_sdk.pipelines.sources import SparkIoThubSource\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n    import json\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    connectionString = \"Endpoint=sb://{NAMESPACE}.servicebus.windows.net/;SharedAccessKeyName={ACCESS_KEY_NAME};SharedAccessKey={ACCESS_KEY}=;EntityPath={EVENT_HUB_NAME}\"\n\n    startingEventPosition = {\n        \"offset\": -1,\n        \"seqNo\": -1,\n        \"enqueuedTime\": None,\n        \"isInclusive\": True\n    }\n\n    endingEventPosition = {\n        \"offset\": None,\n        \"seqNo\": -1,\n        \"enqueuedTime\": endTime,\n        \"isInclusive\": True\n    }\n\n    iot_hub_source = SparkIoThubSource(\n        spark,\n        options = {\n            \"eventhubs.connectionString\": connectionString,\n            \"eventhubs.consumerGroup\": \"{YOUR-CONSUMER-GROUP}\",\n            \"eventhubs.startingPosition\": json.dumps(startingEventPosition),\n            \"eventhubs.endingPosition\": json.dumps(endingEventPosition)\n        }\n    )\n\n    iot_hub_source.read_batch()\n    ```\n\n    Parameters:\n        spark (SparkSession): Spark Session\n        options (dict): A dictionary of IoT Hub configurations (See Attributes table below)\n\n    Attributes:\n        eventhubs.connectionString (str):  IoT Hub connection string is required to connect to the Eventhubs service. (Streaming and Batch)\n        eventhubs.consumerGroup (str): A consumer group is a view of an entire IoT Hub. Consumer groups enable multiple consuming applications to each have a separate view of the event stream, and to read the stream independently at their own pace and with their own offsets. (Streaming and Batch)\n        eventhubs.startingPosition (JSON str): The starting position for your Structured Streaming job. If a specific EventPosition is not set for a partition using startingPositions, then we use the EventPosition set in startingPosition. If nothing is set in either option, we will begin consuming from the end of the partition. (Streaming and Batch)\n        eventhubs.endingPosition: (JSON str): The ending position of a batch query. This works the same as startingPosition. (Batch)\n        maxEventsPerTrigger (long): Rate limit on maximum number of events processed per trigger interval. The specified total number of events will be proportionally split across partitions of different volume. (Stream)\n\n    \"\"\"\n\n    options: dict\n    spark: SparkSession\n\n    def __init__(self, spark: SparkSession, options: dict) -&gt; None:\n        self.spark = spark\n        self.schema = EVENTHUB_SCHEMA\n        self.options = options\n\n    @staticmethod\n    def system_type():\n\"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    @staticmethod\n    def libraries():\n        spark_libraries = Libraries()\n        spark_libraries.add_maven_library(get_default_package(\"spark_azure_eventhub\"))\n        return spark_libraries\n\n    def pre_read_validation(self) -&gt; bool:\n        return True\n\n    def post_read_validation(self, df: DataFrame) -&gt; bool:\n        assert df.schema == self.schema\n        return True\n\n    def read_batch(self) -&gt; DataFrame:\n\"\"\"\n        Reads batch data from IoT Hubs.\n        \"\"\"\n        iothub_connection_string = \"eventhubs.connectionString\"\n        try:\n            if iothub_connection_string in self.options:\n                sc = self.spark.sparkContext\n                self.options[\n                    iothub_connection_string\n                ] = sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(\n                    self.options[iothub_connection_string]\n                )\n\n            return self.spark.read.format(\"eventhubs\").options(**self.options).load()\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def read_stream(self) -&gt; DataFrame:\n\"\"\"\n        Reads streaming data from IoT Hubs.\n        \"\"\"\n        iothub_connection_string = \"eventhubs.connectionString\"\n        try:\n            if iothub_connection_string in self.options:\n                sc = self.spark.sparkContext\n                self.options[\n                    iothub_connection_string\n                ] = sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(\n                    self.options[iothub_connection_string]\n                )\n\n            return (\n                self.spark.readStream.format(\"eventhubs\").options(**self.options).load()\n            )\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iot_hub/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iot_hub.SparkIoThubSource.read_batch","title":"<code>read_batch()</code>","text":"<p>Reads batch data from IoT Hubs.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iot_hub.py</code> <pre><code>def read_batch(self) -&gt; DataFrame:\n\"\"\"\n    Reads batch data from IoT Hubs.\n    \"\"\"\n    iothub_connection_string = \"eventhubs.connectionString\"\n    try:\n        if iothub_connection_string in self.options:\n            sc = self.spark.sparkContext\n            self.options[\n                iothub_connection_string\n            ] = sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(\n                self.options[iothub_connection_string]\n            )\n\n        return self.spark.read.format(\"eventhubs\").options(**self.options).load()\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iot_hub/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iot_hub.SparkIoThubSource.read_stream","title":"<code>read_stream()</code>","text":"<p>Reads streaming data from IoT Hubs.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iot_hub.py</code> <pre><code>def read_stream(self) -&gt; DataFrame:\n\"\"\"\n    Reads streaming data from IoT Hubs.\n    \"\"\"\n    iothub_connection_string = \"eventhubs.connectionString\"\n    try:\n        if iothub_connection_string in self.options:\n            sc = self.spark.sparkContext\n            self.options[\n                iothub_connection_string\n            ] = sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(\n                self.options[iothub_connection_string]\n            )\n\n        return (\n            self.spark.readStream.format(\"eventhubs\").options(**self.options).load()\n        )\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iot_hub/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iot_hub.SparkIoThubSource.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iot_hub.py</code> <pre><code>@staticmethod\ndef system_type():\n\"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/kafka/","title":"Read from Kafka","text":""},{"location":"sdk/code-reference/pipelines/sources/spark/kafka/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.kafka.SparkKafkaSource","title":"<code>SparkKafkaSource</code>","text":"<p>             Bases: <code>SourceInterface</code></p> <p>This Spark source class is used to read batch or streaming data from Kafka. Required and optional configurations can be found in the Attributes tables below.</p> <p>Additionally, there are more optional configurations which can be found here.</p>"},{"location":"sdk/code-reference/pipelines/sources/spark/kafka/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.kafka.SparkKafkaSource--example","title":"Example","text":"<p><pre><code> #Kafka Source for Streaming Queries\n\nfrom rtdip_sdk.pipelines.sources import SparkKafkaSource\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\nkafka_source = SparkKafkaSource(\n    spark=spark,\n    options={\n        \"kafka.bootstrap.servers\": \"{HOST_1}:{PORT_1},{HOST_2}:{PORT_2}\",\n        \"subscribe\": \"{TOPIC_1},{TOPIC_2}\",\n        \"includeHeaders\", \"true\"\n    }\n)\n\nkafka_source.read_stream()\n</code></pre> <pre><code> #Kafka Source for Batch Queries\n\nfrom rtdip_sdk.pipelines.sources import SparkKafkaSource\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\nkafka_source = SparkKafkaSource(\n    spark=spark,\n    options={\n        \"kafka.bootstrap.servers\": \"{HOST_1}:{PORT_1},{HOST_2}:{PORT_2}\",\n        \"subscribe\": \"{TOPIC_1},{TOPIC_2}\",\n        \"startingOffsets\": \"earliest\",\n        \"endingOffsets\": \"latest\"\n    }\n)\n\nkafka_source.read_batch()\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session</p> required <code>options</code> <code>dict</code> <p>A dictionary of Kafka configurations (See Attributes tables below). For more information on configuration options see here</p> required <p>The following attributes are the most common configurations for Kafka.</p> <p>The only configuration that must be set for the Kafka source for both batch and streaming queries is listed below.</p> <p>Attributes:</p> Name Type Description <code>kafka.bootstrap.servers</code> <code>A comma-separated list of host\ufe30port</code> <p>The Kafka \"bootstrap.servers\" configuration. (Streaming and Batch)</p> <p>There are multiple ways of specifying which topics to subscribe to. You should provide only one of these attributes:</p> <p>Attributes:</p> Name Type Description <code>assign</code> <code>json string {\"topicA\"\ufe30[0,1],\"topicB\"\ufe30[2,4]}</code> <p>Specific TopicPartitions to consume. Only one of \"assign\", \"subscribe\" or \"subscribePattern\" options can be specified for Kafka source. (Streaming and Batch)</p> <code>subscribe</code> <code>A comma-separated list of topics</code> <p>The topic list to subscribe. Only one of \"assign\", \"subscribe\" or \"subscribePattern\" options can be specified for Kafka source. (Streaming and Batch)</p> <code>subscribePattern</code> <code>Java regex string</code> <p>The pattern used to subscribe to topic(s). Only one of \"assign, \"subscribe\" or \"subscribePattern\" options can be specified for Kafka source. (Streaming and Batch)</p> <p>The following configurations are optional:</p> <p>Attributes:</p> Name Type Description <code>startingTimestamp</code> <code>timestamp str</code> <p>The start point of timestamp when a query is started, a string specifying a starting timestamp for all partitions in topics being subscribed. Please refer the note on starting timestamp offset options below. (Streaming and Batch)</p> <code>startingOffsetsByTimestamp</code> <code>JSON str</code> <p>The start point of timestamp when a query is started, a json string specifying a starting timestamp for each TopicPartition. Please refer the note on starting timestamp offset options below. (Streaming and Batch)</p> <code>startingOffsets</code> <code>\"earliest\", \"latest\" (streaming only), or JSON string</code> <p>The start point when a query is started, either \"earliest\" which is from the earliest offsets, \"latest\" which is just from the latest offsets, or a json string specifying a starting offset for each TopicPartition. In the json, -2 as an offset can be used to refer to earliest, -1 to latest.</p> <code>endingTimestamp</code> <code>timestamp str</code> <p>The end point when a batch query is ended, a json string specifying an ending timestamp for all partitions in topics being subscribed. Please refer the note on ending timestamp offset options below. (Batch)</p> <code>endingOffsetsByTimestamp</code> <code>JSON str</code> <p>The end point when a batch query is ended, a json string specifying an ending timestamp for each TopicPartition. Please refer the note on ending timestamp offset options below. (Batch)</p> <code>endingOffsets</code> <code>latest or JSON str</code> <p>The end point when a batch query is ended, either \"latest\" which is just referred to the latest, or a json string specifying an ending offset for each TopicPartition. In the json, -1 as an offset can be used to refer to latest, and -2 (earliest) as an offset is not allowed. (Batch)</p> <code>maxOffsetsPerTrigger</code> <code>long</code> <p>Rate limit on maximum number of offsets processed per trigger interval. The specified total number of offsets will be proportionally split across topicPartitions of different volume. (Streaming)</p> <code>minOffsetsPerTrigger</code> <code>long</code> <p>Minimum number of offsets to be processed per trigger interval. The specified total number of offsets will be proportionally split across topicPartitions of different volume. (Streaming)</p> <code>failOnDataLoss</code> <code>bool</code> <p>Whether to fail the query when it's possible that data is lost (e.g., topics are deleted, or offsets are out of range). This may be a false alarm. You can disable it when it doesn't work as you expected.</p> <code>minPartitions</code> <code>int</code> <p>Desired minimum number of partitions to read from Kafka. By default, Spark has a 1-1 mapping of topicPartitions to Spark partitions consuming from Kafka. (Streaming and Batch)</p> <code>includeHeaders</code> <code>bool</code> <p>Whether to include the Kafka headers in the row. (Streaming and Batch)</p> <p>Starting Timestamp Offset Note</p> <p>If Kafka doesn't return the matched offset, the behavior will follow to the value of the option <code>startingOffsetsByTimestampStrategy</code>.</p> <p><code>startingTimestamp</code> takes precedence over <code>startingOffsetsByTimestamp</code> and startingOffsets.</p> <p>For streaming queries, this only applies when a new query is started, and that resuming will always pick up from where the query left off. Newly discovered partitions during a query will start at earliest.</p> <p>Ending Timestamp Offset Note</p> <p>If Kafka doesn't return the matched offset, the offset will be set to latest.</p> <p><code>endingOffsetsByTimestamp</code> takes precedence over <code>endingOffsets</code>.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/kafka.py</code> <pre><code>class SparkKafkaSource(SourceInterface):\n\"\"\"\n    This Spark source class is used to read batch or streaming data from Kafka. Required and optional configurations can be found in the Attributes tables below.\n\n    Additionally, there are more optional configurations which can be found [here.](https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html){ target=\"_blank\" }\n\n    Example\n    --------\n    ```python\n     #Kafka Source for Streaming Queries\n\n    from rtdip_sdk.pipelines.sources import SparkKafkaSource\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    kafka_source = SparkKafkaSource(\n        spark=spark,\n        options={\n            \"kafka.bootstrap.servers\": \"{HOST_1}:{PORT_1},{HOST_2}:{PORT_2}\",\n            \"subscribe\": \"{TOPIC_1},{TOPIC_2}\",\n            \"includeHeaders\", \"true\"\n        }\n    )\n\n    kafka_source.read_stream()\n    ```\n    ```python\n     #Kafka Source for Batch Queries\n\n    from rtdip_sdk.pipelines.sources import SparkKafkaSource\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    kafka_source = SparkKafkaSource(\n        spark=spark,\n        options={\n            \"kafka.bootstrap.servers\": \"{HOST_1}:{PORT_1},{HOST_2}:{PORT_2}\",\n            \"subscribe\": \"{TOPIC_1},{TOPIC_2}\",\n            \"startingOffsets\": \"earliest\",\n            \"endingOffsets\": \"latest\"\n        }\n    )\n\n    kafka_source.read_batch()\n    ```\n\n    Parameters:\n        spark (SparkSession): Spark Session\n        options (dict): A dictionary of Kafka configurations (See Attributes tables below). For more information on configuration options see [here](https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html){ target=\"_blank\" }\n\n    The following attributes are the most common configurations for Kafka.\n\n    The only configuration that must be set for the Kafka source for both batch and streaming queries is listed below.\n\n    Attributes:\n        kafka.bootstrap.servers (A comma-separated list of host\ufe30port):  The Kafka \"bootstrap.servers\" configuration. (Streaming and Batch)\n\n    There are multiple ways of specifying which topics to subscribe to. You should provide only one of these attributes:\n\n    Attributes:\n        assign (json string {\"topicA\"\ufe30[0,1],\"topicB\"\ufe30[2,4]}):  Specific TopicPartitions to consume. Only one of \"assign\", \"subscribe\" or \"subscribePattern\" options can be specified for Kafka source. (Streaming and Batch)\n        subscribe (A comma-separated list of topics): The topic list to subscribe. Only one of \"assign\", \"subscribe\" or \"subscribePattern\" options can be specified for Kafka source. (Streaming and Batch)\n        subscribePattern (Java regex string): The pattern used to subscribe to topic(s). Only one of \"assign, \"subscribe\" or \"subscribePattern\" options can be specified for Kafka source. (Streaming and Batch)\n\n    The following configurations are optional:\n\n    Attributes:\n        startingTimestamp (timestamp str): The start point of timestamp when a query is started, a string specifying a starting timestamp for all partitions in topics being subscribed. Please refer the note on starting timestamp offset options below. (Streaming and Batch)\n        startingOffsetsByTimestamp (JSON str): The start point of timestamp when a query is started, a json string specifying a starting timestamp for each TopicPartition. Please refer the note on starting timestamp offset options below. (Streaming and Batch)\n        startingOffsets (\"earliest\", \"latest\" (streaming only), or JSON string): The start point when a query is started, either \"earliest\" which is from the earliest offsets, \"latest\" which is just from the latest offsets, or a json string specifying a starting offset for each TopicPartition. In the json, -2 as an offset can be used to refer to earliest, -1 to latest.\n        endingTimestamp (timestamp str): The end point when a batch query is ended, a json string specifying an ending timestamp for all partitions in topics being subscribed. Please refer the note on ending timestamp offset options below. (Batch)\n        endingOffsetsByTimestamp (JSON str): The end point when a batch query is ended, a json string specifying an ending timestamp for each TopicPartition. Please refer the note on ending timestamp offset options below. (Batch)\n        endingOffsets (latest or JSON str): The end point when a batch query is ended, either \"latest\" which is just referred to the latest, or a json string specifying an ending offset for each TopicPartition. In the json, -1 as an offset can be used to refer to latest, and -2 (earliest) as an offset is not allowed. (Batch)\n        maxOffsetsPerTrigger (long): Rate limit on maximum number of offsets processed per trigger interval. The specified total number of offsets will be proportionally split across topicPartitions of different volume. (Streaming)\n        minOffsetsPerTrigger (long): Minimum number of offsets to be processed per trigger interval. The specified total number of offsets will be proportionally split across topicPartitions of different volume. (Streaming)\n        failOnDataLoss (bool): Whether to fail the query when it's possible that data is lost (e.g., topics are deleted, or offsets are out of range). This may be a false alarm. You can disable it when it doesn't work as you expected.\n        minPartitions (int): Desired minimum number of partitions to read from Kafka. By default, Spark has a 1-1 mapping of topicPartitions to Spark partitions consuming from Kafka. (Streaming and Batch)\n        includeHeaders (bool): Whether to include the Kafka headers in the row. (Streaming and Batch)\n\n    !!! note \"Starting Timestamp Offset Note\"\n        If Kafka doesn't return the matched offset, the behavior will follow to the value of the option &lt;code&gt;startingOffsetsByTimestampStrategy&lt;/code&gt;.\n\n        &lt;code&gt;startingTimestamp&lt;/code&gt; takes precedence over &lt;code&gt;startingOffsetsByTimestamp&lt;/code&gt; and &lt;/code&gt;startingOffsets&lt;/code&gt;.\n\n        For streaming queries, this only applies when a new query is started, and that resuming will always pick up from where the query left off. Newly discovered partitions during a query will start at earliest.\n\n    !!! note \"Ending Timestamp Offset Note\"\n        If Kafka doesn't return the matched offset, the offset will be set to latest.\n\n        &lt;code&gt;endingOffsetsByTimestamp&lt;/code&gt; takes precedence over &lt;code&gt;endingOffsets&lt;/code&gt;.\n\n    \"\"\"\n\n    spark: SparkSession\n    options: dict\n\n    def __init__(self, spark: SparkSession, options: dict) -&gt; None:\n        self.spark = spark\n        self.options = options\n        self.schema = KAFKA_SCHEMA\n\n    @staticmethod\n    def system_type():\n\"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        spark_libraries = Libraries()\n        spark_libraries.add_maven_library(get_default_package(\"spark_sql_kafka\"))\n        return spark_libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_read_validation(self) -&gt; bool:\n        return True\n\n    def post_read_validation(self, df: DataFrame) -&gt; bool:\n        assert df.schema == self.schema\n        return True\n\n    def read_batch(self) -&gt; DataFrame:\n\"\"\"\n        Reads batch data from Kafka.\n        \"\"\"\n        try:\n            return self.spark.read.format(\"kafka\").options(**self.options).load()\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def read_stream(self) -&gt; DataFrame:\n\"\"\"\n        Reads streaming data from Kafka.\n        \"\"\"\n        try:\n            return self.spark.readStream.format(\"kafka\").options(**self.options).load()\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/kafka/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.kafka.SparkKafkaSource.read_batch","title":"<code>read_batch()</code>","text":"<p>Reads batch data from Kafka.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/kafka.py</code> <pre><code>def read_batch(self) -&gt; DataFrame:\n\"\"\"\n    Reads batch data from Kafka.\n    \"\"\"\n    try:\n        return self.spark.read.format(\"kafka\").options(**self.options).load()\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/kafka/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.kafka.SparkKafkaSource.read_stream","title":"<code>read_stream()</code>","text":"<p>Reads streaming data from Kafka.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/kafka.py</code> <pre><code>def read_stream(self) -&gt; DataFrame:\n\"\"\"\n    Reads streaming data from Kafka.\n    \"\"\"\n    try:\n        return self.spark.readStream.format(\"kafka\").options(**self.options).load()\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/kafka/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.kafka.SparkKafkaSource.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/kafka.py</code> <pre><code>@staticmethod\ndef system_type():\n\"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/kafka_eventhub/","title":"Read from Kafka","text":""},{"location":"sdk/code-reference/pipelines/sources/spark/kafka_eventhub/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.kafka_eventhub.SparkKafkaEventhubSource","title":"<code>SparkKafkaEventhubSource</code>","text":"<p>             Bases: <code>SourceInterface</code></p> <p>This Spark source class is used to read batch or streaming data from an Eventhub using the Kafka protocol. This enables Eventhubs to be used as a source in applications like Delta Live Tables or Databricks Serverless Jobs as the Spark Eventhubs JAR is not supported in these scenarios.</p> <p>The dataframe returned is transformed to ensure the schema is as close to the Eventhub Spark source as possible. There are some minor differences:</p> <ul> <li><code>offset</code> is dependent on <code>x-opt-offset</code> being populated in the headers provided. If this is not found in the headers, the value will be null</li> <li><code>publisher</code> is dependent on <code>x-opt-publisher</code> being populated in the headers provided. If this is not found in the headers, the value will be null</li> <li><code>partitionKey</code> is dependent on <code>x-opt-partition-key</code> being populated in the headers provided. If this is not found in the headers, the value will be null</li> <li><code>systemProperties</code> are identified according to the list provided in the Eventhub documentation and IoT Hub documentation</li> </ul> <p>Default settings will be specified if not provided in the <code>options</code> parameter:</p> <ul> <li><code>kafka.sasl.mechanism</code> will be set to <code>PLAIN</code></li> <li><code>kafka.security.protocol</code> will be set to <code>SASL_SSL</code></li> <li><code>kafka.request.timeout.ms</code> will be set to <code>60000</code></li> <li><code>kafka.session.timeout.ms</code> will be set to <code>60000</code></li> </ul>"},{"location":"sdk/code-reference/pipelines/sources/spark/kafka_eventhub/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.kafka_eventhub.SparkKafkaEventhubSource--examples","title":"Examples","text":"<p><pre><code>#Kafka Source for Streaming Queries\n\nfrom rtdip_sdk.pipelines.sources import SparkKafkaEventhubSource\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\nconnectionString = \"Endpoint=sb://{NAMESPACE}.servicebus.windows.net/;SharedAccessKeyName={ACCESS_KEY_NAME};SharedAccessKey={ACCESS_KEY}=;EntityPath={EVENT_HUB_NAME}\"\nconsumerGroup = \"{YOUR-CONSUMER-GROUP}\"\n\nkafka_eventhub_source = SparkKafkaEventhubSource(\n    spark=spark,\n    options={\n        \"startingOffsets\": \"earliest\",\n        \"maxOffsetsPerTrigger\": 10000,\n        \"failOnDataLoss\": \"false\",\n    },\n    connection_string=connectionString,\n    consumer_group=\"consumerGroup\"\n)\n\nkafka_eventhub_source.read_stream()\n</code></pre> <pre><code>#Kafka Source for Batch Queries\n\nfrom rtdip_sdk.pipelines.sources import SparkKafkaEventhubSource\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\nconnectionString = \"Endpoint=sb://{NAMESPACE}.servicebus.windows.net/;SharedAccessKeyName={ACCESS_KEY_NAME};SharedAccessKey={ACCESS_KEY}=;EntityPath={EVENT_HUB_NAME}\"\nconsumerGroup = \"{YOUR-CONSUMER-GROUP}\"\n\nkafka_eventhub_source = SparkKafkaEventhubSource(\n    spark=spark,\n    options={\n        \"startingOffsets\": \"earliest\",\n        \"endingOffsets\": \"latest\",\n        \"failOnDataLoss\": \"false\"\n    },\n    connection_string=connectionString,\n    consumer_group=\"consumerGroup\"\n)\n\nkafka_eventhub_source.read_batch()\n</code></pre></p> <p>Required and optional configurations can be found in the Attributes and Parameter tables below. Additionally, there are more optional configurations which can be found here.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session</p> required <code>options</code> <code>dict</code> <p>A dictionary of Kafka configurations (See Attributes tables below). For more information on configuration options see here</p> required <code>connection_string</code> <code>str</code> <p>Eventhubs connection string is required to connect to the Eventhubs service. This must include the Eventhub name as the <code>EntityPath</code> parameter. Example <code>\"Endpoint=sb://test.servicebus.windows.net/;SharedAccessKeyName=test;SharedAccessKey=test_key;EntityPath=test_eventhub\"</code></p> required <code>consumer_group</code> <code>str</code> <p>The Eventhub consumer group to use for the connection</p> required <p>The only configuration that must be set for the Kafka source for both batch and streaming queries is listed below.</p> <p>Attributes:</p> Name Type Description <code>kafka.bootstrap.servers</code> <code>A comma-separated list of host\ufe30port</code> <p>The Kafka \"bootstrap.servers\" configuration. (Streaming and Batch)</p> <p>There are multiple ways of specifying which topics to subscribe to. You should provide only one of these parameters:</p> <p>Attributes:</p> Name Type Description <code>assign</code> <code>json string {\"topicA\"\ufe30[0,1],\"topicB\"\ufe30[2,4]}</code> <p>Specific TopicPartitions to consume. Only one of \"assign\", \"subscribe\" or \"subscribePattern\" options can be specified for Kafka source. (Streaming and Batch)</p> <code>subscribe</code> <code>A comma-separated list of topics</code> <p>The topic list to subscribe. Only one of \"assign\", \"subscribe\" or \"subscribePattern\" options can be specified for Kafka source. (Streaming and Batch)</p> <code>subscribePattern</code> <code>Java regex string</code> <p>The pattern used to subscribe to topic(s). Only one of \"assign, \"subscribe\" or \"subscribePattern\" options can be specified for Kafka source. (Streaming and Batch)</p> <p>The following configurations are optional:</p> <p>Attributes:</p> Name Type Description <code>startingTimestamp</code> <code>timestamp str</code> <p>The start point of timestamp when a query is started, a string specifying a starting timestamp for all partitions in topics being subscribed. Please refer the note on starting timestamp offset options below. (Streaming and Batch)</p> <code>startingOffsetsByTimestamp</code> <code>JSON str</code> <p>The start point of timestamp when a query is started, a json string specifying a starting timestamp for each TopicPartition. Please refer the note on starting timestamp offset options below. (Streaming and Batch)</p> <code>startingOffsets</code> <code>\"earliest\", \"latest\" (streaming only), or JSON string</code> <p>The start point when a query is started, either \"earliest\" which is from the earliest offsets, \"latest\" which is just from the latest offsets, or a json string specifying a starting offset for each TopicPartition. In the json, -2 as an offset can be used to refer to earliest, -1 to latest.</p> <code>endingTimestamp</code> <code>timestamp str</code> <p>The end point when a batch query is ended, a json string specifying an ending timestamp for all partitions in topics being subscribed. Please refer the note on ending timestamp offset options below. (Batch)</p> <code>endingOffsetsByTimestamp</code> <code>JSON str</code> <p>The end point when a batch query is ended, a json string specifying an ending timestamp for each TopicPartition. Please refer the note on ending timestamp offset options below. (Batch)</p> <code>endingOffsets</code> <code>latest or JSON str</code> <p>The end point when a batch query is ended, either \"latest\" which is just referred to the latest, or a json string specifying an ending offset for each TopicPartition. In the json, -1 as an offset can be used to refer to latest, and -2 (earliest) as an offset is not allowed. (Batch)</p> <code>maxOffsetsPerTrigger</code> <code>long</code> <p>Rate limit on maximum number of offsets processed per trigger interval. The specified total number of offsets will be proportionally split across topicPartitions of different volume. (Streaming)</p> <code>minOffsetsPerTrigger</code> <code>long</code> <p>Minimum number of offsets to be processed per trigger interval. The specified total number of offsets will be proportionally split across topicPartitions of different volume. (Streaming)</p> <code>failOnDataLoss</code> <code>bool</code> <p>Whether to fail the query when it's possible that data is lost (e.g., topics are deleted, or offsets are out of range). This may be a false alarm. You can disable it when it doesn't work as you expected.</p> <code>minPartitions</code> <code>int</code> <p>Desired minimum number of partitions to read from Kafka. By default, Spark has a 1-1 mapping of topicPartitions to Spark partitions consuming from Kafka. (Streaming and Batch)</p> <code>includeHeaders</code> <code>bool</code> <p>Whether to include the Kafka headers in the row. (Streaming and Batch)</p> <p>Starting Timestamp Offset Note</p> <p>If Kafka doesn't return the matched offset, the behavior will follow to the value of the option <code>startingOffsetsByTimestampStrategy</code>.</p> <p><code>startingTimestamp</code> takes precedence over <code>startingOffsetsByTimestamp</code> and startingOffsets.</p> <p>For streaming queries, this only applies when a new query is started, and that resuming will always pick up from where the query left off. Newly discovered partitions during a query will start at earliest.</p> <p>Ending Timestamp Offset Note</p> <p>If Kafka doesn't return the matched offset, the offset will be set to latest.</p> <p><code>endingOffsetsByTimestamp</code> takes precedence over <code>endingOffsets</code>.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/kafka_eventhub.py</code> <pre><code>class SparkKafkaEventhubSource(SourceInterface):\n\"\"\"\n    This Spark source class is used to read batch or streaming data from an Eventhub using the Kafka protocol. This enables Eventhubs to be used as a source in applications like Delta Live Tables or Databricks Serverless Jobs as the Spark Eventhubs JAR is not supported in these scenarios.\n\n    The dataframe returned is transformed to ensure the schema is as close to the Eventhub Spark source as possible. There are some minor differences:\n\n    - `offset` is dependent on `x-opt-offset` being populated in the headers provided. If this is not found in the headers, the value will be null\n    - `publisher` is dependent on `x-opt-publisher` being populated in the headers provided. If this is not found in the headers, the value will be null\n    - `partitionKey` is dependent on `x-opt-partition-key` being populated in the headers provided. If this is not found in the headers, the value will be null\n    - `systemProperties` are identified according to the list provided in the [Eventhub documentation](https://learn.microsoft.com/en-us/azure/data-explorer/ingest-data-event-hub-overview#event-system-properties-mapping){ target=\"_blank\" } and [IoT Hub documentation](https://learn.microsoft.com/en-us/azure/data-explorer/ingest-data-iot-hub-overview#event-system-properties-mapping){ target=\"_blank\" }\n\n    Default settings will be specified if not provided in the `options` parameter:\n\n    - `kafka.sasl.mechanism` will be set to `PLAIN`\n    - `kafka.security.protocol` will be set to `SASL_SSL`\n    - `kafka.request.timeout.ms` will be set to `60000`\n    - `kafka.session.timeout.ms` will be set to `60000`\n\n    Examples\n    --------\n    ```python\n    #Kafka Source for Streaming Queries\n\n    from rtdip_sdk.pipelines.sources import SparkKafkaEventhubSource\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    connectionString = \"Endpoint=sb://{NAMESPACE}.servicebus.windows.net/;SharedAccessKeyName={ACCESS_KEY_NAME};SharedAccessKey={ACCESS_KEY}=;EntityPath={EVENT_HUB_NAME}\"\n    consumerGroup = \"{YOUR-CONSUMER-GROUP}\"\n\n    kafka_eventhub_source = SparkKafkaEventhubSource(\n        spark=spark,\n        options={\n            \"startingOffsets\": \"earliest\",\n            \"maxOffsetsPerTrigger\": 10000,\n            \"failOnDataLoss\": \"false\",\n        },\n        connection_string=connectionString,\n        consumer_group=\"consumerGroup\"\n    )\n\n    kafka_eventhub_source.read_stream()\n    ```\n    ```python\n    #Kafka Source for Batch Queries\n\n    from rtdip_sdk.pipelines.sources import SparkKafkaEventhubSource\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    connectionString = \"Endpoint=sb://{NAMESPACE}.servicebus.windows.net/;SharedAccessKeyName={ACCESS_KEY_NAME};SharedAccessKey={ACCESS_KEY}=;EntityPath={EVENT_HUB_NAME}\"\n    consumerGroup = \"{YOUR-CONSUMER-GROUP}\"\n\n    kafka_eventhub_source = SparkKafkaEventhubSource(\n        spark=spark,\n        options={\n            \"startingOffsets\": \"earliest\",\n            \"endingOffsets\": \"latest\",\n            \"failOnDataLoss\": \"false\"\n        },\n        connection_string=connectionString,\n        consumer_group=\"consumerGroup\"\n    )\n\n    kafka_eventhub_source.read_batch()\n    ```\n\n    Required and optional configurations can be found in the Attributes and Parameter tables below.\n    Additionally, there are more optional configurations which can be found [here.](https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html){ target=\"_blank\" }\n\n    Parameters:\n        spark (SparkSession): Spark Session\n        options (dict): A dictionary of Kafka configurations (See Attributes tables below). For more information on configuration options see [here](https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html){ target=\"_blank\" }\n        connection_string (str): Eventhubs connection string is required to connect to the Eventhubs service. This must include the Eventhub name as the `EntityPath` parameter. Example `\"Endpoint=sb://test.servicebus.windows.net/;SharedAccessKeyName=test;SharedAccessKey=test_key;EntityPath=test_eventhub\"`\n        consumer_group (str): The Eventhub consumer group to use for the connection\n\n    The only configuration that must be set for the Kafka source for both batch and streaming queries is listed below.\n\n    Attributes:\n        kafka.bootstrap.servers (A comma-separated list of host\ufe30port):  The Kafka \"bootstrap.servers\" configuration. (Streaming and Batch)\n\n    There are multiple ways of specifying which topics to subscribe to. You should provide only one of these parameters:\n\n    Attributes:\n        assign (json string {\"topicA\"\ufe30[0,1],\"topicB\"\ufe30[2,4]}):  Specific TopicPartitions to consume. Only one of \"assign\", \"subscribe\" or \"subscribePattern\" options can be specified for Kafka source. (Streaming and Batch)\n        subscribe (A comma-separated list of topics): The topic list to subscribe. Only one of \"assign\", \"subscribe\" or \"subscribePattern\" options can be specified for Kafka source. (Streaming and Batch)\n        subscribePattern (Java regex string): The pattern used to subscribe to topic(s). Only one of \"assign, \"subscribe\" or \"subscribePattern\" options can be specified for Kafka source. (Streaming and Batch)\n\n    The following configurations are optional:\n\n    Attributes:\n        startingTimestamp (timestamp str): The start point of timestamp when a query is started, a string specifying a starting timestamp for all partitions in topics being subscribed. Please refer the note on starting timestamp offset options below. (Streaming and Batch)\n        startingOffsetsByTimestamp (JSON str): The start point of timestamp when a query is started, a json string specifying a starting timestamp for each TopicPartition. Please refer the note on starting timestamp offset options below. (Streaming and Batch)\n        startingOffsets (\"earliest\", \"latest\" (streaming only), or JSON string): The start point when a query is started, either \"earliest\" which is from the earliest offsets, \"latest\" which is just from the latest offsets, or a json string specifying a starting offset for each TopicPartition. In the json, -2 as an offset can be used to refer to earliest, -1 to latest.\n        endingTimestamp (timestamp str): The end point when a batch query is ended, a json string specifying an ending timestamp for all partitions in topics being subscribed. Please refer the note on ending timestamp offset options below. (Batch)\n        endingOffsetsByTimestamp (JSON str): The end point when a batch query is ended, a json string specifying an ending timestamp for each TopicPartition. Please refer the note on ending timestamp offset options below. (Batch)\n        endingOffsets (latest or JSON str): The end point when a batch query is ended, either \"latest\" which is just referred to the latest, or a json string specifying an ending offset for each TopicPartition. In the json, -1 as an offset can be used to refer to latest, and -2 (earliest) as an offset is not allowed. (Batch)\n        maxOffsetsPerTrigger (long): Rate limit on maximum number of offsets processed per trigger interval. The specified total number of offsets will be proportionally split across topicPartitions of different volume. (Streaming)\n        minOffsetsPerTrigger (long): Minimum number of offsets to be processed per trigger interval. The specified total number of offsets will be proportionally split across topicPartitions of different volume. (Streaming)\n        failOnDataLoss (bool): Whether to fail the query when it's possible that data is lost (e.g., topics are deleted, or offsets are out of range). This may be a false alarm. You can disable it when it doesn't work as you expected.\n        minPartitions (int): Desired minimum number of partitions to read from Kafka. By default, Spark has a 1-1 mapping of topicPartitions to Spark partitions consuming from Kafka. (Streaming and Batch)\n        includeHeaders (bool): Whether to include the Kafka headers in the row. (Streaming and Batch)\n\n    !!! note \"Starting Timestamp Offset Note\"\n        If Kafka doesn't return the matched offset, the behavior will follow to the value of the option &lt;code&gt;startingOffsetsByTimestampStrategy&lt;/code&gt;.\n\n        &lt;code&gt;startingTimestamp&lt;/code&gt; takes precedence over &lt;code&gt;startingOffsetsByTimestamp&lt;/code&gt; and &lt;/code&gt;startingOffsets&lt;/code&gt;.\n\n        For streaming queries, this only applies when a new query is started, and that resuming will always pick up from where the query left off. Newly discovered partitions during a query will start at earliest.\n\n    !!! note \"Ending Timestamp Offset Note\"\n        If Kafka doesn't return the matched offset, the offset will be set to latest.\n\n        &lt;code&gt;endingOffsetsByTimestamp&lt;/code&gt; takes precedence over &lt;code&gt;endingOffsets&lt;/code&gt;.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        options: dict,\n        connection_string: str,\n        consumer_group: str,\n    ) -&gt; None:\n        self.spark = spark\n        self.options = options\n        self.connection_string = connection_string\n        self.consumer_group = consumer_group\n        self.connection_string_properties = self._parse_connection_string(\n            connection_string\n        )\n        self.schema = KAFKA_EVENTHUB_SCHEMA\n        self.options = self._configure_options(options)\n\n    @staticmethod\n    def system_type():\n\"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        spark_libraries = Libraries()\n        spark_libraries.add_maven_library(get_default_package(\"spark_sql_kafka\"))\n        return spark_libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_read_validation(self) -&gt; bool:\n        return True\n\n    def post_read_validation(self, df: DataFrame) -&gt; bool:\n        assert df.schema == self.schema\n        return True\n\n    # Code is from Azure Eventhub Python SDK. Will import the package if possible with Conda in the  conda-forge channel in the future\n    def _parse_connection_string(self, connection_string: str):\n        conn_settings = [s.split(\"=\", 1) for s in connection_string.split(\";\")]\n        if any(len(tup) != 2 for tup in conn_settings):\n            raise ValueError(\"Connection string is either blank or malformed.\")\n        conn_settings = dict(conn_settings)\n        shared_access_signature = None\n        for key, value in conn_settings.items():\n            if key.lower() == \"sharedaccesssignature\":\n                shared_access_signature = value\n        shared_access_key = conn_settings.get(\"SharedAccessKey\")\n        shared_access_key_name = conn_settings.get(\"SharedAccessKeyName\")\n        if any([shared_access_key, shared_access_key_name]) and not all(\n            [shared_access_key, shared_access_key_name]\n        ):\n            raise ValueError(\n                \"Connection string must have both SharedAccessKeyName and SharedAccessKey.\"\n            )\n        if shared_access_signature is not None and shared_access_key is not None:\n            raise ValueError(\n                \"Only one of the SharedAccessKey or SharedAccessSignature must be present.\"\n            )\n        endpoint = conn_settings.get(\"Endpoint\")\n        if not endpoint:\n            raise ValueError(\"Connection string is either blank or malformed.\")\n        parsed = urlparse(endpoint.rstrip(\"/\"))\n        if not parsed.netloc:\n            raise ValueError(\"Invalid Endpoint on the Connection String.\")\n        namespace = parsed.netloc.strip()\n        properties = {\n            \"fully_qualified_namespace\": namespace,\n            \"endpoint\": endpoint,\n            \"eventhub_name\": conn_settings.get(\"EntityPath\"),\n            \"shared_access_signature\": shared_access_signature,\n            \"shared_access_key_name\": shared_access_key_name,\n            \"shared_access_key\": shared_access_key,\n        }\n        return properties\n\n    def _connection_string_builder(self, properties: dict) -&gt; str:\n        connection_string = \"Endpoint=\" + properties.get(\"endpoint\") + \";\"\n\n        if properties.get(\"shared_access_key\"):\n            connection_string += (\n                \"SharedAccessKey=\" + properties.get(\"shared_access_key\") + \";\"\n            )\n\n        if properties.get(\"shared_access_key_name\"):\n            connection_string += (\n                \"SharedAccessKeyName=\" + properties.get(\"shared_access_key_name\") + \";\"\n            )\n\n        if properties.get(\"shared_access_signature\"):\n            connection_string += (\n                \"SharedAccessSignature=\"\n                + properties.get(\"shared_access_signature\")\n                + \";\"\n            )\n        return connection_string\n\n    def _configure_options(self, options: dict) -&gt; dict:\n        if \"subscribe\" not in options:\n            options[\"subscribe\"] = self.connection_string_properties.get(\n                \"eventhub_name\"\n            )\n\n        if \"kafka.bootstrap.servers\" not in options:\n            options[\"kafka.bootstrap.servers\"] = (\n                self.connection_string_properties.get(\"fully_qualified_namespace\")\n                + \":9093\"\n            )\n\n        if \"kafka.sasl.mechanism\" not in options:\n            options[\"kafka.sasl.mechanism\"] = \"PLAIN\"\n\n        if \"kafka.security.protocol\" not in options:\n            options[\"kafka.security.protocol\"] = \"SASL_SSL\"\n\n        if \"kafka.sasl.jaas.config\" not in options:\n            kafka_package = \"org.apache.kafka.common.security.plain.PlainLoginModule\"\n            if \"DATABRICKS_RUNTIME_VERSION\" in os.environ:\n                kafka_package = \"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule\"\n            connection_string = self._connection_string_builder(\n                self.connection_string_properties\n            )\n            options[\n                \"kafka.sasl.jaas.config\"\n            ] = '{} required username=\"$ConnectionString\" password=\"{}\";'.format(\n                kafka_package, connection_string\n            )  # NOSONAR\n\n        if \"kafka.request.timeout.ms\" not in options:\n            options[\"kafka.request.timeout.ms\"] = \"60000\"\n\n        if \"kafka.session.timeout.ms\" not in options:\n            options[\"kafka.session.timeout.ms\"] = \"60000\"\n\n        if \"kafka.group.id\" not in options:\n            options[\"kafka.group.id\"] = self.consumer_group\n\n        options[\"includeHeaders\"] = \"true\"\n\n        return options\n\n    def _transform_to_eventhub_schema(self, df: DataFrame) -&gt; DataFrame:\n        return (\n            df.withColumn(\"headers\", map_from_entries(col(\"headers\")))\n            .select(\n                col(\"value\").alias(\"body\"),\n                col(\"partition\").cast(\"string\"),\n                col(\"offset\").alias(\"sequenceNumber\"),\n                col(\"timestamp\").alias(\"enqueuedTime\"),\n                decode_kafka_headers_to_amqp_properties(col(\"headers\")).alias(\n                    \"properties\"\n                ),\n            )\n            .withColumn(\"offset\", col(\"properties\").getItem(\"x-opt-offset\"))\n            .withColumn(\"publisher\", col(\"properties\").getItem(\"x-opt-publisher\"))\n            .withColumn(\n                \"partitionKey\", col(\"properties\").getItem(\"x-opt-partition-key\")\n            )\n            .withColumn(\n                \"systemProperties\",\n                map_filter(\n                    col(\"properties\"), lambda k, _: k.isin(eventhub_system_properties)\n                ),\n            )\n            .withColumn(\n                \"properties\",\n                map_filter(\n                    col(\"properties\"), lambda k, _: ~k.isin(eventhub_system_properties)\n                ),\n            )\n            .select(\n                col(\"body\"),\n                col(\"partition\"),\n                col(\"offset\"),\n                col(\"sequenceNumber\"),\n                col(\"enqueuedTime\"),\n                col(\"publisher\"),\n                col(\"partitionKey\"),\n                col(\"properties\"),\n                col(\"systemProperties\"),\n            )\n        )\n\n    def read_batch(self) -&gt; DataFrame:\n\"\"\"\n        Reads batch data from Kafka.\n        \"\"\"\n        try:\n            df = self.spark.read.format(\"kafka\").options(**self.options).load()\n            return self._transform_to_eventhub_schema(df)\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def read_stream(self) -&gt; DataFrame:\n\"\"\"\n        Reads streaming data from Kafka.\n        \"\"\"\n        try:\n            df = self.spark.readStream.format(\"kafka\").options(**self.options).load()\n            return self._transform_to_eventhub_schema(df)\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/kafka_eventhub/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.kafka_eventhub.SparkKafkaEventhubSource.read_batch","title":"<code>read_batch()</code>","text":"<p>Reads batch data from Kafka.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/kafka_eventhub.py</code> <pre><code>def read_batch(self) -&gt; DataFrame:\n\"\"\"\n    Reads batch data from Kafka.\n    \"\"\"\n    try:\n        df = self.spark.read.format(\"kafka\").options(**self.options).load()\n        return self._transform_to_eventhub_schema(df)\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/kafka_eventhub/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.kafka_eventhub.SparkKafkaEventhubSource.read_stream","title":"<code>read_stream()</code>","text":"<p>Reads streaming data from Kafka.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/kafka_eventhub.py</code> <pre><code>def read_stream(self) -&gt; DataFrame:\n\"\"\"\n    Reads streaming data from Kafka.\n    \"\"\"\n    try:\n        df = self.spark.readStream.format(\"kafka\").options(**self.options).load()\n        return self._transform_to_eventhub_schema(df)\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/kafka_eventhub/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.kafka_eventhub.SparkKafkaEventhubSource.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/kafka_eventhub.py</code> <pre><code>@staticmethod\ndef system_type():\n\"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/kinesis/","title":"Read from Amazon Kinesis Data Streams","text":""},{"location":"sdk/code-reference/pipelines/sources/spark/kinesis/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.kinesis.SparkKinesisSource","title":"<code>SparkKinesisSource</code>","text":"<p>             Bases: <code>SourceInterface</code></p> <p>The Spark Kinesis Source is used to read data from Kinesis in a Databricks environment. Structured streaming from Kinesis is not supported in open source Spark.</p>"},{"location":"sdk/code-reference/pipelines/sources/spark/kinesis/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.kinesis.SparkKinesisSource--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.sources import SparkKinesisSource\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\nkinesis_source = SparkKinesisSource(\n    spark=spark,\n    options={\n        \"awsAccessKey\": \"{AWS-ACCESS-KEY}\",\n        \"awsSecretKey\": \"{AWS-SECRET-KEY}\",\n        \"streamName\": \"{STREAM-NAME}\",\n        \"region\": \"{REGION}\",\n        \"endpoint\": \"https://kinesis.{REGION}.amazonaws.com\",\n        \"initialPosition\": \"earliest\"\n    }\n)\n\nkinesis_source.read_stream()\n\nOR\n\nkinesis_source.read_batch()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session required to read data from Kinesis</p> required <code>options</code> <code>dict</code> <p>Options that can be specified for a Kinesis read operation (See Attributes table below). Further information on the options is available here</p> required <p>Attributes:</p> Name Type Description <code>awsAccessKey</code> <code>str</code> <p>AWS access key.</p> <code>awsSecretKey</code> <code>str</code> <p>AWS secret access key corresponding to the access key.</p> <code>streamName</code> <code>List[str]</code> <p>The stream names to subscribe to.</p> <code>region</code> <code>str</code> <p>The region the streams are defined in.</p> <code>endpoint</code> <code>str</code> <p>The regional endpoint for Kinesis Data Streams.</p> <code>initialPosition</code> <code>str</code> <p>The point to start reading from; earliest, latest, or at_timestamp.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/kinesis.py</code> <pre><code>class SparkKinesisSource(SourceInterface):\n\"\"\"\n    The Spark Kinesis Source is used to read data from Kinesis in a Databricks environment.\n    Structured streaming from Kinesis is **not** supported in open source Spark.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.sources import SparkKinesisSource\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    kinesis_source = SparkKinesisSource(\n        spark=spark,\n        options={\n            \"awsAccessKey\": \"{AWS-ACCESS-KEY}\",\n            \"awsSecretKey\": \"{AWS-SECRET-KEY}\",\n            \"streamName\": \"{STREAM-NAME}\",\n            \"region\": \"{REGION}\",\n            \"endpoint\": \"https://kinesis.{REGION}.amazonaws.com\",\n            \"initialPosition\": \"earliest\"\n        }\n    )\n\n    kinesis_source.read_stream()\n\n    OR\n\n    kinesis_source.read_batch()\n    ```\n\n    Parameters:\n        spark (SparkSession): Spark Session required to read data from Kinesis\n        options (dict): Options that can be specified for a Kinesis read operation (See Attributes table below). Further information on the options is available [here](https://docs.databricks.com/structured-streaming/kinesis.html#configuration){ target=\"_blank\" }\n\n    Attributes:\n        awsAccessKey (str): AWS access key.\n        awsSecretKey (str): AWS secret access key corresponding to the access key.\n        streamName (List[str]): The stream names to subscribe to.\n        region (str): The region the streams are defined in.\n        endpoint (str): The regional endpoint for Kinesis Data Streams.\n        initialPosition (str): The point to start reading from; earliest, latest, or at_timestamp.\n    \"\"\"\n\n    spark: SparkSession\n    options: dict\n\n    def __init__(self, spark: SparkSession, options: dict) -&gt; None:\n        self.spark = spark\n        self.options = options\n        self.schema = KINESIS_SCHEMA\n\n    @staticmethod\n    def system_type():\n\"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK_DATABRICKS\n        \"\"\"\n        return SystemType.PYSPARK_DATABRICKS\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_read_validation(self):\n        return True\n\n    def post_read_validation(self, df: DataFrame) -&gt; bool:\n        assert df.schema == self.schema\n        return True\n\n    def read_batch(self):\n\"\"\"\n        Raises:\n            NotImplementedError: Kinesis only supports streaming reads. To perform a batch read, use the read_stream method of this component and specify the Trigger on the write_stream to be `availableNow=True` to perform batch-like reads of cloud storage files.\n        \"\"\"\n        raise NotImplementedError(\n            \"Kinesis only supports streaming reads. To perform a batch read, use the read_stream method and specify Trigger on the write_stream as `availableNow=True`\"\n        )\n\n    def read_stream(self) -&gt; DataFrame:\n\"\"\"\n        Reads streaming data from Kinesis. All of the data in the table is processed as well as any new data that arrives after the stream started.\n        \"\"\"\n        try:\n            return (\n                self.spark.readStream.format(\"kinesis\").options(**self.options).load()\n            )\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/kinesis/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.kinesis.SparkKinesisSource.read_batch","title":"<code>read_batch()</code>","text":"<p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Kinesis only supports streaming reads. To perform a batch read, use the read_stream method of this component and specify the Trigger on the write_stream to be <code>availableNow=True</code> to perform batch-like reads of cloud storage files.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/kinesis.py</code> <pre><code>def read_batch(self):\n\"\"\"\n    Raises:\n        NotImplementedError: Kinesis only supports streaming reads. To perform a batch read, use the read_stream method of this component and specify the Trigger on the write_stream to be `availableNow=True` to perform batch-like reads of cloud storage files.\n    \"\"\"\n    raise NotImplementedError(\n        \"Kinesis only supports streaming reads. To perform a batch read, use the read_stream method and specify Trigger on the write_stream as `availableNow=True`\"\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/kinesis/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.kinesis.SparkKinesisSource.read_stream","title":"<code>read_stream()</code>","text":"<p>Reads streaming data from Kinesis. All of the data in the table is processed as well as any new data that arrives after the stream started.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/kinesis.py</code> <pre><code>def read_stream(self) -&gt; DataFrame:\n\"\"\"\n    Reads streaming data from Kinesis. All of the data in the table is processed as well as any new data that arrives after the stream started.\n    \"\"\"\n    try:\n        return (\n            self.spark.readStream.format(\"kinesis\").options(**self.options).load()\n        )\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/kinesis/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.kinesis.SparkKinesisSource.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK_DATABRICKS</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/kinesis.py</code> <pre><code>@staticmethod\ndef system_type():\n\"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK_DATABRICKS\n    \"\"\"\n    return SystemType.PYSPARK_DATABRICKS\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/ecmwf/base_mars/","title":"Base MARS","text":""},{"location":"sdk/code-reference/pipelines/sources/spark/ecmwf/base_mars/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.ecmwf.base_mars.SparkECMWFBaseMarsSource","title":"<code>SparkECMWFBaseMarsSource</code>","text":"<p>Download nc files from ECMWF MARS server using the ECMWF python API. Data is downloaded in parallel using joblib from ECMWF MARS server using the ECMWF python API.</p> <p>Parameters:</p> Name Type Description Default <code>save_path</code> <code>str</code> <p>Path to local directory where the nc files will be stored, in format \"yyyy-mm-dd_HH.nc\"</p> required <code>date_start</code> <code>str</code> <p>Start date of extraction in \"YYYY-MM-DD HH:MM:SS\" format</p> required <code>date_end</code> <code>str</code> <p>End date of extraction in \"YYYY-MM-DD HH:MM:SS\" format</p> required <code>ecmwf_api_key</code> <code>str</code> <p>API key for ECMWF MARS server</p> required <code>ecmwf_api_email</code> <code>str</code> <p>Email for ECMWF MARS server</p> required <code>ecmwf_api_url</code> <code>str</code> <p>URL for ECMWF MARS server</p> <code>'https://api.ecmwf.int/v1'</code> <code>run_frequency</code> <code>str</code> <p>Frequency format of runs to download, e.g. \"H\"</p> <code>'H'</code> <code>run_interval</code> <code>str</code> <p>Interval of runs, e.g. a run_frequency of \"H\" and run_interval of \"12\" will extract the data of the 00 and 12 run for each day.</p> <code>'12'</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/ecmwf/base_mars.py</code> <pre><code>class SparkECMWFBaseMarsSource:\n\"\"\"\n    Download nc files from ECMWF MARS server using the ECMWF python API.\n    Data is downloaded in parallel using joblib from ECMWF MARS server using the ECMWF python API.\n\n    Parameters:\n        save_path (str): Path to local directory where the nc files will be stored, in format \"yyyy-mm-dd_HH.nc\"\n        date_start (str): Start date of extraction in \"YYYY-MM-DD HH:MM:SS\" format\n        date_end (str): End date of extraction in \"YYYY-MM-DD HH:MM:SS\" format\n        ecmwf_api_key (str): API key for ECMWF MARS server\n        ecmwf_api_email (str): Email for ECMWF MARS server\n        ecmwf_api_url (str): URL for ECMWF MARS server\n        run_frequency (str):Frequency format of runs to download, e.g. \"H\"\n        run_interval (str): Interval of runs, e.g. a run_frequency of \"H\" and run_interval of \"12\" will extract the data of the 00 and 12 run for each day.\n    \"\"\"\n\n    def __init__(\n        self,\n        date_start: str,\n        date_end: str,\n        save_path: str,\n        ecmwf_api_key: str,\n        ecmwf_api_email: str,\n        ecmwf_api_url: str = \"https://api.ecmwf.int/v1\",\n        run_interval: str = \"12\",\n        run_frequency: str = \"H\",\n    ):\n        self.retrieve_ran = False\n        self.date_start = date_start\n        self.date_end = date_end\n        self.save_path = save_path\n        self.format = format\n        self.run_interval = run_interval\n        self.run_frequency = run_frequency\n        self.ecmwf_api_key = ecmwf_api_key\n        self.ecmwf_api_url = ecmwf_api_url\n        self.ecmwf_api_email = ecmwf_api_email\n\n        # Pandas date_list (info best retrieved per forecast day)\n        self.dates = pd.date_range(\n            start=date_start, end=date_end, freq=run_interval + run_frequency\n        )\n\n    def retrieve(\n        self,\n        mars_dict: dict,\n        n_jobs=None,\n        backend=\"loky\",\n        tries=5,\n        cost=False,\n    ):\n\"\"\"Retrieve the data from the server.\n\n        Function will use the ecmwf api to download the data from the server.\n        Note that mars has a max of two active requests per user and 20 queued\n        requests.\n        Data is downloaded in parallel using joblib from ECMWF MARS server using the ECMWF python API.\n\n\n        Parameters:\n            mars_dict (dict): Dictionary of mars parameters.\n            n_jobs (int, optional): Download in parallel? by default None, i.e. no parallelization\n            backend (str, optional) : Specify the parallelization backend implementation in joblib, by default \"loky\"\n            tries (int, optional): Number of tries for each request if it fails, by default 5\n            cost (bool, optional):  Pass a cost request to mars to estimate the size and efficiency of your request,\n                but not actually download the data. Can be useful for defining requests,\n                by default False.\n        \"\"\"\n        chk = [\"date\", \"target\", \"time\", \"format\", \"output\"]\n        for i in chk:\n            if i in mars_dict.keys():\n                raise ValueError(f\"don't include {i} in the mars_dict\")\n\n        parallel = Parallel(n_jobs=n_jobs, backend=backend)\n\n        def _retrieve_datetime(i, j, cost=cost):\n            i_dict = {\"date\": i, \"time\": j}\n\n            if cost:\n                filename = f\"{i}_{j}.txt\"  # NOSONAR\n            else:\n                filename = f\"{i}_{j}.nc\"\n                i_dict[\"format\"] = \"netcdf\"  # NOSONAR\n\n            target = os.path.join(self.save_path, filename)\n            msg = f\"retrieving mars data --- {filename}\"\n\n            req_dict = {**i_dict, **mars_dict}\n            for k, v in req_dict.items():\n                if isinstance(v, (list, tuple)):\n                    req_dict[k] = \"/\".join([str(x) for x in v])  # NOSONAR\n\n            req_dict = [\"{}={}\".format(k, v) for k, v in req_dict.items()]\n            if cost:\n                req_dict = \"list,output=cost,{}\".format(\",\".join(req_dict))  # NOSONAR\n            else:\n                req_dict = \"retrieve,{}\".format(\",\".join(req_dict))  # NOSONAR\n\n            for j in range(tries):\n                try:\n                    print(msg)\n                    server = ECMWFService(\n                        \"mars\",\n                        url=self.ecmwf_api_url,\n                        email=self.ecmwf_api_email,\n                        key=self.ecmwf_api_key,\n                    )\n                    server.execute(req_dict, target)\n                    return 1  # NOSONAR\n                except:  # NOSONAR\n                    if j &lt; tries - 1:\n                        continue  # NOSONAR\n                    else:\n                        return 0  # NOSONAR\n\n        self.success = parallel(\n            delayed(_retrieve_datetime)(str(k.date()), f\"{k.hour:02}\")\n            for k in self.dates\n        )\n        self.retrieve_ran = True\n\n        return self\n\n    def info(self) -&gt; pd.Series:\n\"\"\"\n        Return info on each ECMWF request.\n\n        Returns:\n            pd.Series: Successful request for each run == 1.\n        \"\"\"\n        if not self.retrieve_ran:\n            raise ValueError(\n                \"Before using self.info(), prepare the request using \"\n                + \"self.retrieve()\"\n            )\n        y = pd.Series(self.success, index=self.dates, name=\"success\", dtype=bool)\n\n        return y\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/ecmwf/base_mars/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.ecmwf.base_mars.SparkECMWFBaseMarsSource.info","title":"<code>info()</code>","text":"<p>Return info on each ECMWF request.</p> <p>Returns:</p> Type Description <code>Series</code> <p>pd.Series: Successful request for each run == 1.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/ecmwf/base_mars.py</code> <pre><code>def info(self) -&gt; pd.Series:\n\"\"\"\n    Return info on each ECMWF request.\n\n    Returns:\n        pd.Series: Successful request for each run == 1.\n    \"\"\"\n    if not self.retrieve_ran:\n        raise ValueError(\n            \"Before using self.info(), prepare the request using \"\n            + \"self.retrieve()\"\n        )\n    y = pd.Series(self.success, index=self.dates, name=\"success\", dtype=bool)\n\n    return y\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/ecmwf/base_mars/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.ecmwf.base_mars.SparkECMWFBaseMarsSource.retrieve","title":"<code>retrieve(mars_dict, n_jobs=None, backend='loky', tries=5, cost=False)</code>","text":"<p>Retrieve the data from the server.</p> <p>Function will use the ecmwf api to download the data from the server. Note that mars has a max of two active requests per user and 20 queued requests. Data is downloaded in parallel using joblib from ECMWF MARS server using the ECMWF python API.</p> <p>Parameters:</p> Name Type Description Default <code>mars_dict</code> <code>dict</code> <p>Dictionary of mars parameters.</p> required <code>n_jobs</code> <code>int</code> <p>Download in parallel? by default None, i.e. no parallelization</p> <code>None</code> <code>backend</code> <code>str, optional) </code> <p>Specify the parallelization backend implementation in joblib, by default \"loky\"</p> <code>'loky'</code> <code>tries</code> <code>int</code> <p>Number of tries for each request if it fails, by default 5</p> <code>5</code> <code>cost</code> <code>bool</code> <p>Pass a cost request to mars to estimate the size and efficiency of your request, but not actually download the data. Can be useful for defining requests, by default False.</p> <code>False</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/ecmwf/base_mars.py</code> <pre><code>def retrieve(\n    self,\n    mars_dict: dict,\n    n_jobs=None,\n    backend=\"loky\",\n    tries=5,\n    cost=False,\n):\n\"\"\"Retrieve the data from the server.\n\n    Function will use the ecmwf api to download the data from the server.\n    Note that mars has a max of two active requests per user and 20 queued\n    requests.\n    Data is downloaded in parallel using joblib from ECMWF MARS server using the ECMWF python API.\n\n\n    Parameters:\n        mars_dict (dict): Dictionary of mars parameters.\n        n_jobs (int, optional): Download in parallel? by default None, i.e. no parallelization\n        backend (str, optional) : Specify the parallelization backend implementation in joblib, by default \"loky\"\n        tries (int, optional): Number of tries for each request if it fails, by default 5\n        cost (bool, optional):  Pass a cost request to mars to estimate the size and efficiency of your request,\n            but not actually download the data. Can be useful for defining requests,\n            by default False.\n    \"\"\"\n    chk = [\"date\", \"target\", \"time\", \"format\", \"output\"]\n    for i in chk:\n        if i in mars_dict.keys():\n            raise ValueError(f\"don't include {i} in the mars_dict\")\n\n    parallel = Parallel(n_jobs=n_jobs, backend=backend)\n\n    def _retrieve_datetime(i, j, cost=cost):\n        i_dict = {\"date\": i, \"time\": j}\n\n        if cost:\n            filename = f\"{i}_{j}.txt\"  # NOSONAR\n        else:\n            filename = f\"{i}_{j}.nc\"\n            i_dict[\"format\"] = \"netcdf\"  # NOSONAR\n\n        target = os.path.join(self.save_path, filename)\n        msg = f\"retrieving mars data --- {filename}\"\n\n        req_dict = {**i_dict, **mars_dict}\n        for k, v in req_dict.items():\n            if isinstance(v, (list, tuple)):\n                req_dict[k] = \"/\".join([str(x) for x in v])  # NOSONAR\n\n        req_dict = [\"{}={}\".format(k, v) for k, v in req_dict.items()]\n        if cost:\n            req_dict = \"list,output=cost,{}\".format(\",\".join(req_dict))  # NOSONAR\n        else:\n            req_dict = \"retrieve,{}\".format(\",\".join(req_dict))  # NOSONAR\n\n        for j in range(tries):\n            try:\n                print(msg)\n                server = ECMWFService(\n                    \"mars\",\n                    url=self.ecmwf_api_url,\n                    email=self.ecmwf_api_email,\n                    key=self.ecmwf_api_key,\n                )\n                server.execute(req_dict, target)\n                return 1  # NOSONAR\n            except:  # NOSONAR\n                if j &lt; tries - 1:\n                    continue  # NOSONAR\n                else:\n                    return 0  # NOSONAR\n\n    self.success = parallel(\n        delayed(_retrieve_datetime)(str(k.date()), f\"{k.hour:02}\")\n        for k in self.dates\n    )\n    self.retrieve_ran = True\n\n    return self\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/ecmwf/weather_forecast/","title":"Weather Forecast","text":""},{"location":"sdk/code-reference/pipelines/sources/spark/ecmwf/weather_forecast/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.ecmwf.weather_forecast.SparkECMWFWeatherForecastSource","title":"<code>SparkECMWFWeatherForecastSource</code>","text":"<p>             Bases: <code>SourceInterface</code></p> <p>The Weather Forecast API V1 Source class to doownload nc files from ECMWF MARS server using the ECMWF python API.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance</p> required <code>save_path</code> <code>str</code> <p>Path to local directory where the nc files will be stored, in format \"yyyy-mm-dd_HH.nc\"</p> required <code>date_start</code> <code>str</code> <p>Start date of extraction in \"YYYY-MM-DD HH:MM:SS\" format    date_end:str,</p> required <code>date_end</code> <code>str</code> <p>End date of extraction in \"YYYY-MM-DD HH:MM:SS\" format</p> required <code>ecmwf_class</code> <code>str</code> <p>ecmwf classification of data</p> required <code>stream</code> <code>str</code> <p>Operational model stream</p> required <code>expver</code> <code>str</code> <p>Version of data</p> required <code>leveltype</code> <code>str</code> <p>Surface level forecasts</p> required <code>ec_vars</code> <code>list</code> <p>Variables of forecast measurements.</p> required <code>forecast_area</code> <code>list</code> <p>N/W/S/E coordinates of the forecast area</p> required <code>ecmwf_api_key</code> <code>str</code> <p>API key for ECMWF API</p> required <code>ecmwf_api_email</code> <code>str</code> <p>Email for ECMWF API</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/ecmwf/weather_forecast.py</code> <pre><code>class SparkECMWFWeatherForecastSource(SourceInterface):\n\"\"\"\n    The Weather Forecast API V1 Source class to doownload nc files from ECMWF MARS server using the ECMWF python API.\n\n    Parameters:\n        spark (SparkSession): Spark Session instance\n        save_path (str): Path to local directory where the nc files will be stored, in format \"yyyy-mm-dd_HH.nc\"\n        date_start (str): Start date of extraction in \"YYYY-MM-DD HH:MM:SS\" format    date_end:str,\n        date_end (str): End date of extraction in \"YYYY-MM-DD HH:MM:SS\" format\n        ecmwf_class (str): ecmwf classification of data\n        stream (str): Operational model stream\n        expver (str): Version of data\n        leveltype (str): Surface level forecasts\n        ec_vars (list): Variables of forecast measurements.\n        forecast_area (list): N/W/S/E coordinates of the forecast area\n        ecmwf_api_key (str): API key for ECMWF API\n        ecmwf_api_email (str): Email for ECMWF API\n    \"\"\"\n\n    spark: SparkSession\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        save_path: str,\n        date_start: str,\n        date_end: str,\n        ecmwf_class: str,\n        stream: str,\n        expver: str,\n        leveltype: str,\n        ec_vars: list,\n        forecast_area: list,\n        ecmwf_api_key: str,\n        ecmwf_api_email: str,\n    ) -&gt; None:\n        self.spark = spark\n        self.save_path = save_path\n        self.date_start = date_start\n        self.date_end = date_end\n        self.ecmwf_class = ecmwf_class\n        self.stream = stream  # operational model\n        self.expver = expver  # experiment version of data\n        self.leveltype = leveltype  # surface level forecasts\n        self.ec_vars = ec_vars  # variables\n        self.forecast_area = forecast_area  # N/W/S/E\n        self.ecmwf_api_key = ecmwf_api_key\n        self.ecmwf_api_email = ecmwf_api_email\n\n    @staticmethod\n    def system_type():\n\"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_read_validation(self):\n        return True\n\n    def post_read_validation(self):\n        return True\n\n    def read_stream(self):\n        return True\n\n    @classmethod\n    def _get_lead_time(cls):\n\"\"\"\n        Lead time for the forecast data.\n        90 hours - 1 Hour Interval\n        90-146 - 3 Hour interval\n        146 -246 - 6 Hour interval\n\n        Returns:\n            lead_times: Lead times in an array format.\n        \"\"\"\n        lead_times = [*range(91), *range(93, 146, 3), *range(150, 246, 6)]\n        np.array(lead_times)\n\n        return lead_times\n\n    def _get_api_params(self, lead_times):\n\"\"\"\n        API parameters for the forecast data.\n\n        Returns:\n            params (dict): API parameters for the forecast data.\n        \"\"\"\n\n        params = {\n            \"class\": self.ecmwf_class,  # ecmwf classification of data\n            \"stream\": self.stream,  # operational model\n            \"expver\": self.expver,  # experiment version of data\n            \"levtype\": self.leveltype,  # surface level forecasts\n            \"type\": \"fc\",  # forecasts\n            \"param\": self.ec_vars,  # variables\n            \"step\": lead_times,  # which lead times to download\n            \"area\": self.forecast_area,  # N/W/S/E\n            \"grid\": [0.1, 0.1],  # grid res of output\n        }\n\n        return params\n\n    def read_batch(self):\n\"\"\"\n        Pulls data from the Weather API and returns as .nc files.\n\n        \"\"\"\n        lead_times = self._get_lead_time()\n        para = self._get_api_params(lead_times=lead_times)\n\n        ec_conn = SparkECMWFBaseMarsSource(\n            date_start=self.date_start,\n            date_end=self.date_end,\n            save_path=self.save_path,\n            run_interval=\"12\",\n            run_frequency=\"H\",\n            ecmwf_api_key=self.ecmwf_api_key,\n            ecmwf_api_email=self.ecmwf_api_email,\n            ecmwf_api_url=\"https://api.ecmwf.int/v1\",\n        )\n\n        ec_conn.retrieve(\n            mars_dict=para,\n            tries=5,\n            n_jobs=-1,  # maximum of 20 queued requests per user (only two allowed active)\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/ecmwf/weather_forecast/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.ecmwf.weather_forecast.SparkECMWFWeatherForecastSource.read_batch","title":"<code>read_batch()</code>","text":"<p>Pulls data from the Weather API and returns as .nc files.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/ecmwf/weather_forecast.py</code> <pre><code>def read_batch(self):\n\"\"\"\n    Pulls data from the Weather API and returns as .nc files.\n\n    \"\"\"\n    lead_times = self._get_lead_time()\n    para = self._get_api_params(lead_times=lead_times)\n\n    ec_conn = SparkECMWFBaseMarsSource(\n        date_start=self.date_start,\n        date_end=self.date_end,\n        save_path=self.save_path,\n        run_interval=\"12\",\n        run_frequency=\"H\",\n        ecmwf_api_key=self.ecmwf_api_key,\n        ecmwf_api_email=self.ecmwf_api_email,\n        ecmwf_api_url=\"https://api.ecmwf.int/v1\",\n    )\n\n    ec_conn.retrieve(\n        mars_dict=para,\n        tries=5,\n        n_jobs=-1,  # maximum of 20 queued requests per user (only two allowed active)\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/ecmwf/weather_forecast/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.ecmwf.weather_forecast.SparkECMWFWeatherForecastSource.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/ecmwf/weather_forecast.py</code> <pre><code>@staticmethod\ndef system_type():\n\"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/base_iso/","title":"Base iso","text":""},{"location":"sdk/code-reference/pipelines/sources/spark/iso/base_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource","title":"<code>BaseISOSource</code>","text":"<p>             Bases: <code>SourceInterface</code></p> <p>Base class for all the ISO Sources. It provides common functionality and helps in reducing the code redundancy.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance</p> required <code>options</code> <code>dict</code> <p>A dictionary of ISO Source specific configurations</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>class BaseISOSource(SourceInterface):\n\"\"\"\n    Base class for all the ISO Sources. It provides common functionality and helps in reducing the code redundancy.\n\n    Parameters:\n        spark (SparkSession): Spark Session instance\n        options (dict): A dictionary of ISO Source specific configurations\n    \"\"\"\n\n    spark: SparkSession\n    options: dict\n    iso_url: str = \"https://\"\n    query_datetime_format: str = \"%Y%m%d\"\n    required_options: list = []\n    spark_schema = StructType([StructField(\"id\", IntegerType(), True)])\n    default_query_timezone: str = \"UTC\"\n\n    def __init__(self, spark: SparkSession, options: dict) -&gt; None:\n        self.spark = spark\n        self.options = options\n        self.query_timezone = pytz.timezone(\n            self.options.get(\"query_timezone\", self.default_query_timezone)\n        )\n        self.current_date = datetime.now(timezone.utc).astimezone(self.query_timezone)\n\n    def _fetch_from_url(self, url_suffix: str) -&gt; bytes:\n\"\"\"\n        Gets data from external ISO API.\n\n        Args:\n            url_suffix: String to be used as suffix to iso url.\n\n        Returns:\n            Raw content of the data received.\n\n        \"\"\"\n        url = f\"{self.iso_url}{url_suffix}\"\n        logging.info(f\"Requesting URL - {url}\")\n\n        response = requests.get(url)\n        code = response.status_code\n\n        if code != 200:\n            raise HTTPError(\n                f\"Unable to access URL `{url}`.\"\n                f\" Received status code {code} with message {response.content}\"\n            )\n\n        return response.content\n\n    def _get_localized_datetime(self, datetime_str: str) -&gt; datetime:\n\"\"\"\n        Converts string datetime into Python datetime object with configured format and timezone.\n        Args:\n            datetime_str: String to be converted into datetime.\n\n        Returns: Timezone aware datetime object.\n\n        \"\"\"\n        parsed_dt = datetime.strptime(datetime_str, self.query_datetime_format)\n        parsed_dt = parsed_dt.replace(tzinfo=self.query_timezone)\n        return parsed_dt\n\n    def _pull_data(self) -&gt; pd.DataFrame:\n\"\"\"\n        Hits the fetch_from_url method with certain parameters to get raw data from API.\n\n        All the children ISO classes must override this method and call the fetch_url method\n        in it.\n\n        Returns:\n             Raw DataFrame from API.\n        \"\"\"\n\n        return pd.read_csv(BytesIO(self._fetch_from_url(\"\")))\n\n    def _prepare_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n        Performs all the basic transformations to prepare data for further processing.\n        All the children ISO classes must override this method.\n\n        Args:\n            df: Raw DataFrame, received from the API.\n\n        Returns:\n             Modified DataFrame, ready for basic use.\n\n        \"\"\"\n        return df\n\n    def _sanitize_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n        Another data transformation helper method to be called after prepare data.\n        Used for advance data processing such as cleaning, filtering, restructuring.\n        All the children ISO classes must override this method if there is any post-processing required.\n\n        Args:\n            df: Initial modified version of DataFrame, received after preparing the data.\n\n        Returns:\n             Final version of data after all the fixes and modifications.\n\n        \"\"\"\n        return df\n\n    def _get_data(self) -&gt; pd.DataFrame:\n\"\"\"\n        Entrypoint method to return the final version of DataFrame.\n\n        Returns:\n            Modified form of data for specific use case.\n\n        \"\"\"\n        df = self._pull_data()\n        df = self._prepare_data(df)\n        df = self._sanitize_data(df)\n\n        # Reorder columns to keep the data consistent\n        df = df[self.spark_schema.names]\n\n        return df\n\n    @staticmethod\n    def system_type():\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def _validate_options(self) -&gt; bool:\n\"\"\"\n        Performs all the options checks. Raises exception in case of any invalid value.\n        Returns:\n             True if all checks are passed.\n\n        \"\"\"\n        return True\n\n    def pre_read_validation(self) -&gt; bool:\n\"\"\"\n        Ensures all the required options are provided and performs other validations.\n        Returns:\n             True if all checks are passed.\n\n        \"\"\"\n        for key in self.required_options:\n            if key not in self.options:\n                raise ValueError(f\"Required option `{key}` is missing.\")\n\n        return self._validate_options()\n\n    def post_read_validation(self) -&gt; bool:\n        return True\n\n    def read_batch(self) -&gt; DataFrame:\n\"\"\"\n        Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data.\n        Returns:\n             Final Spark DataFrame converted from Pandas DataFrame post-execution.\n\n        \"\"\"\n\n        try:\n            self.pre_read_validation()\n            pdf = self._get_data()\n            pdf = _prepare_pandas_to_convert_to_spark(pdf)\n\n            # The below is to fix the compatibility issues between Pandas 2.0 and PySpark.\n            pd.DataFrame.iteritems = pd.DataFrame.items\n            df = self.spark.createDataFrame(data=pdf, schema=self.spark_schema)\n            return df\n\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def read_stream(self) -&gt; DataFrame:\n\"\"\"\n        By default, the streaming operation is not supported but child classes can override if ISO supports streaming.\n\n        Returns:\n             Final Spark DataFrame after all the processing.\n\n        \"\"\"\n\n        raise NotImplementedError(\n            f\"{self.__class__.__name__} connector doesn't support stream operation.\"\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/base_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.pre_read_validation","title":"<code>pre_read_validation()</code>","text":"<p>Ensures all the required options are provided and performs other validations. Returns:      True if all checks are passed.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def pre_read_validation(self) -&gt; bool:\n\"\"\"\n    Ensures all the required options are provided and performs other validations.\n    Returns:\n         True if all checks are passed.\n\n    \"\"\"\n    for key in self.required_options:\n        if key not in self.options:\n            raise ValueError(f\"Required option `{key}` is missing.\")\n\n    return self._validate_options()\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/base_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.read_batch","title":"<code>read_batch()</code>","text":"<p>Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data. Returns:      Final Spark DataFrame converted from Pandas DataFrame post-execution.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def read_batch(self) -&gt; DataFrame:\n\"\"\"\n    Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data.\n    Returns:\n         Final Spark DataFrame converted from Pandas DataFrame post-execution.\n\n    \"\"\"\n\n    try:\n        self.pre_read_validation()\n        pdf = self._get_data()\n        pdf = _prepare_pandas_to_convert_to_spark(pdf)\n\n        # The below is to fix the compatibility issues between Pandas 2.0 and PySpark.\n        pd.DataFrame.iteritems = pd.DataFrame.items\n        df = self.spark.createDataFrame(data=pdf, schema=self.spark_schema)\n        return df\n\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/base_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.read_stream","title":"<code>read_stream()</code>","text":"<p>By default, the streaming operation is not supported but child classes can override if ISO supports streaming.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Final Spark DataFrame after all the processing.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def read_stream(self) -&gt; DataFrame:\n\"\"\"\n    By default, the streaming operation is not supported but child classes can override if ISO supports streaming.\n\n    Returns:\n         Final Spark DataFrame after all the processing.\n\n    \"\"\"\n\n    raise NotImplementedError(\n        f\"{self.__class__.__name__} connector doesn't support stream operation.\"\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/caiso_daily_load_iso/","title":"CAISO Daily Load","text":""},{"location":"sdk/code-reference/pipelines/sources/spark/iso/caiso_daily_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.caiso_daily_load_iso.CAISODailyLoadISOSource","title":"<code>CAISODailyLoadISOSource</code>","text":"<p>             Bases: <code>BaseISOSource</code></p> <p>The CAISO Daily Load ISO Source is used to read daily load data from CAISO API. It supports multiple types of data. Check the <code>load_types</code> attribute. API: http://oasis.caiso.com/oasisapi</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance</p> required <code>options</code> <code>dict</code> <p>A dictionary of ISO Source specific configurations (See Attributes table below)</p> required <p>Attributes:</p> Name Type Description <code>load_types</code> <code>list</code> <p>Must be a subset of [<code>Demand Forecast 7-Day Ahead</code>, <code>Demand Forecast 2-Day Ahead</code>, <code>Demand Forecast Day Ahead</code>, <code>RTM 15Min Load Forecast</code>, <code>RTM 5Min Load Forecast</code>, <code>Total Actual Hourly Integrated Load</code>].  Default Value - <code>[Total Actual Hourly Integrated Load]</code>.</p> <code>date</code> <code>str</code> <p>Must be in <code>YYYY-MM-DD</code> format.</p> <p>Please check the BaseISOSource for available methods.</p> BaseISOSource Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/caiso_daily_load_iso.py</code> <pre><code>class CAISODailyLoadISOSource(BaseISOSource):\n\"\"\"\n    The CAISO Daily Load ISO Source is used to read daily load data from CAISO API.\n    It supports multiple types of data. Check the `load_types` attribute.\n    &lt;br&gt;API: &lt;a href=\"http://oasis.caiso.com/oasisapi\"&gt;http://oasis.caiso.com/oasisapi&lt;/a&gt;\n\n\n    Parameters:\n        spark (SparkSession): Spark Session instance\n        options (dict): A dictionary of ISO Source specific configurations (See Attributes table below)\n\n    Attributes:\n        load_types (list): Must be a subset of [`Demand Forecast 7-Day Ahead`, `Demand Forecast 2-Day Ahead`, `Demand Forecast Day Ahead`, `RTM 15Min Load Forecast`, `RTM 5Min Load Forecast`, `Total Actual Hourly Integrated Load`]. &lt;br&gt; Default Value - `[Total Actual Hourly Integrated Load]`.\n        date (str): Must be in `YYYY-MM-DD` format.\n\n    Please check the BaseISOSource for available methods.\n\n    BaseISOSource:\n        ::: src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso\n    \"\"\"\n\n    spark: SparkSession\n    options: dict\n    iso_url: str = \"https://oasis.caiso.com/oasisapi/SingleZip\"\n    query_datetime_format: str = \"%Y%m%dT00:00-0000\"\n    required_options = [\"load_types\", \"date\"]\n    spark_schema = CAISO_SCHEMA\n    default_query_timezone = \"UTC\"\n\n    def __init__(self, spark: SparkSession, options: dict) -&gt; None:\n        super().__init__(spark, options)\n        self.spark = spark\n        self.options = options\n        self.load_types = self.options.get(\n            \"load_types\", [\"Total Actual Hourly Integrated Load\"]\n        )\n        self.date = self.options.get(\"date\", \"\").strip()\n        self.user_datetime_format = \"%Y-%m-%d\"\n\n        # The following to fix the Security Check Error as the CAISO API is timing out with HTTPS protocol.\n        self.iso_url = self.iso_url.replace(\"s://\", \"://\")\n\n    def _pull_data(self) -&gt; pd.DataFrame:\n\"\"\"\n        Pulls data from the CAISO API and parses the zip files for CSV data.\n\n        Returns:\n            Raw form of data.\n        \"\"\"\n\n        logging.info(f\"Getting {self.load_types} data for date {self.date}\")\n        start_date = datetime.strptime(self.date, self.user_datetime_format)\n        end_date = start_date + timedelta(days=1)\n        return self._fetch_and_parse_zip(start_date, end_date)\n\n    def _fetch_and_parse_zip(\n        self, start_date: datetime, end_date: datetime\n    ) -&gt; pd.DataFrame:\n        suffix = (\n            f\"?resultformat=6&amp;\"\n            f\"queryname=SLD_FCST&amp;\"\n            \"version=1&amp;\"\n            f\"startdatetime={start_date.strftime(self.query_datetime_format)}&amp;\"\n            f\"enddatetime={end_date.strftime(self.query_datetime_format)}\"\n        )\n\n        content = self._fetch_from_url(suffix)\n        if not content:\n            raise HTTPError(\"Empty Response was returned\")\n        logging.info(\"Unzipping the file\")\n\n        zf = ZipFile(BytesIO(content))\n\n        csvs = list(filter(lambda name: \".csv\" in name, zf.namelist()))\n        if len(csvs) == 0:\n            raise ValueError(\"No data was found in the specified interval\")\n\n        df = pd.read_csv(zf.open(csvs[0]))\n        return df\n\n    def _prepare_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        date_cols = [\"INTERVALSTARTTIME_GMT\", \"INTERVALENDTIME_GMT\"]\n        for date_col in date_cols:\n            df[date_col] = df[date_col].apply(\n                lambda data: datetime.strptime(str(data)[:19], \"%Y-%m-%dT%H:%M:%S\")\n            )\n\n        df = df.rename(\n            columns={\n                \"INTERVALSTARTTIME_GMT\": \"StartTime\",\n                \"INTERVALENDTIME_GMT\": \"EndTime\",\n                \"LOAD_TYPE\": \"LoadType\",\n                \"OPR_DT\": \"OprDt\",\n                \"OPR_HR\": \"OprHr\",\n                \"OPR_INTERVAL\": \"OprInterval\",\n                \"MARKET_RUN_ID\": \"MarketRunId\",\n                \"TAC_AREA_NAME\": \"TacAreaName\",\n                \"LABEL\": \"Label\",\n                \"XML_DATA_ITEM\": \"XmlDataItem\",\n                \"POS\": \"Pos\",\n                \"MW\": \"Load\",\n                \"EXECUTION_TYPE\": \"ExecutionType\",\n                \"GROUP\": \"Group\",\n            }\n        )\n\n        return df\n\n    def _sanitize_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        df = df[df[\"Label\"].isin(self.load_types)]\n        return df\n\n    def _validate_options(self) -&gt; bool:\n        try:\n            datetime.strptime(self.date, self.user_datetime_format)\n        except ValueError:\n            raise ValueError(\n                f\"Unable to parse date. Please specify in {self.user_datetime_format} format.\"\n            )\n        return True\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/caiso_daily_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource","title":"<code>BaseISOSource</code>","text":"<p>             Bases: <code>SourceInterface</code></p> <p>Base class for all the ISO Sources. It provides common functionality and helps in reducing the code redundancy.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance</p> required <code>options</code> <code>dict</code> <p>A dictionary of ISO Source specific configurations</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>class BaseISOSource(SourceInterface):\n\"\"\"\n    Base class for all the ISO Sources. It provides common functionality and helps in reducing the code redundancy.\n\n    Parameters:\n        spark (SparkSession): Spark Session instance\n        options (dict): A dictionary of ISO Source specific configurations\n    \"\"\"\n\n    spark: SparkSession\n    options: dict\n    iso_url: str = \"https://\"\n    query_datetime_format: str = \"%Y%m%d\"\n    required_options: list = []\n    spark_schema = StructType([StructField(\"id\", IntegerType(), True)])\n    default_query_timezone: str = \"UTC\"\n\n    def __init__(self, spark: SparkSession, options: dict) -&gt; None:\n        self.spark = spark\n        self.options = options\n        self.query_timezone = pytz.timezone(\n            self.options.get(\"query_timezone\", self.default_query_timezone)\n        )\n        self.current_date = datetime.now(timezone.utc).astimezone(self.query_timezone)\n\n    def _fetch_from_url(self, url_suffix: str) -&gt; bytes:\n\"\"\"\n        Gets data from external ISO API.\n\n        Args:\n            url_suffix: String to be used as suffix to iso url.\n\n        Returns:\n            Raw content of the data received.\n\n        \"\"\"\n        url = f\"{self.iso_url}{url_suffix}\"\n        logging.info(f\"Requesting URL - {url}\")\n\n        response = requests.get(url)\n        code = response.status_code\n\n        if code != 200:\n            raise HTTPError(\n                f\"Unable to access URL `{url}`.\"\n                f\" Received status code {code} with message {response.content}\"\n            )\n\n        return response.content\n\n    def _get_localized_datetime(self, datetime_str: str) -&gt; datetime:\n\"\"\"\n        Converts string datetime into Python datetime object with configured format and timezone.\n        Args:\n            datetime_str: String to be converted into datetime.\n\n        Returns: Timezone aware datetime object.\n\n        \"\"\"\n        parsed_dt = datetime.strptime(datetime_str, self.query_datetime_format)\n        parsed_dt = parsed_dt.replace(tzinfo=self.query_timezone)\n        return parsed_dt\n\n    def _pull_data(self) -&gt; pd.DataFrame:\n\"\"\"\n        Hits the fetch_from_url method with certain parameters to get raw data from API.\n\n        All the children ISO classes must override this method and call the fetch_url method\n        in it.\n\n        Returns:\n             Raw DataFrame from API.\n        \"\"\"\n\n        return pd.read_csv(BytesIO(self._fetch_from_url(\"\")))\n\n    def _prepare_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n        Performs all the basic transformations to prepare data for further processing.\n        All the children ISO classes must override this method.\n\n        Args:\n            df: Raw DataFrame, received from the API.\n\n        Returns:\n             Modified DataFrame, ready for basic use.\n\n        \"\"\"\n        return df\n\n    def _sanitize_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n        Another data transformation helper method to be called after prepare data.\n        Used for advance data processing such as cleaning, filtering, restructuring.\n        All the children ISO classes must override this method if there is any post-processing required.\n\n        Args:\n            df: Initial modified version of DataFrame, received after preparing the data.\n\n        Returns:\n             Final version of data after all the fixes and modifications.\n\n        \"\"\"\n        return df\n\n    def _get_data(self) -&gt; pd.DataFrame:\n\"\"\"\n        Entrypoint method to return the final version of DataFrame.\n\n        Returns:\n            Modified form of data for specific use case.\n\n        \"\"\"\n        df = self._pull_data()\n        df = self._prepare_data(df)\n        df = self._sanitize_data(df)\n\n        # Reorder columns to keep the data consistent\n        df = df[self.spark_schema.names]\n\n        return df\n\n    @staticmethod\n    def system_type():\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def _validate_options(self) -&gt; bool:\n\"\"\"\n        Performs all the options checks. Raises exception in case of any invalid value.\n        Returns:\n             True if all checks are passed.\n\n        \"\"\"\n        return True\n\n    def pre_read_validation(self) -&gt; bool:\n\"\"\"\n        Ensures all the required options are provided and performs other validations.\n        Returns:\n             True if all checks are passed.\n\n        \"\"\"\n        for key in self.required_options:\n            if key not in self.options:\n                raise ValueError(f\"Required option `{key}` is missing.\")\n\n        return self._validate_options()\n\n    def post_read_validation(self) -&gt; bool:\n        return True\n\n    def read_batch(self) -&gt; DataFrame:\n\"\"\"\n        Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data.\n        Returns:\n             Final Spark DataFrame converted from Pandas DataFrame post-execution.\n\n        \"\"\"\n\n        try:\n            self.pre_read_validation()\n            pdf = self._get_data()\n            pdf = _prepare_pandas_to_convert_to_spark(pdf)\n\n            # The below is to fix the compatibility issues between Pandas 2.0 and PySpark.\n            pd.DataFrame.iteritems = pd.DataFrame.items\n            df = self.spark.createDataFrame(data=pdf, schema=self.spark_schema)\n            return df\n\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def read_stream(self) -&gt; DataFrame:\n\"\"\"\n        By default, the streaming operation is not supported but child classes can override if ISO supports streaming.\n\n        Returns:\n             Final Spark DataFrame after all the processing.\n\n        \"\"\"\n\n        raise NotImplementedError(\n            f\"{self.__class__.__name__} connector doesn't support stream operation.\"\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/caiso_daily_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.pre_read_validation","title":"<code>pre_read_validation()</code>","text":"<p>Ensures all the required options are provided and performs other validations. Returns:      True if all checks are passed.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def pre_read_validation(self) -&gt; bool:\n\"\"\"\n    Ensures all the required options are provided and performs other validations.\n    Returns:\n         True if all checks are passed.\n\n    \"\"\"\n    for key in self.required_options:\n        if key not in self.options:\n            raise ValueError(f\"Required option `{key}` is missing.\")\n\n    return self._validate_options()\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/caiso_daily_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.read_batch","title":"<code>read_batch()</code>","text":"<p>Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data. Returns:      Final Spark DataFrame converted from Pandas DataFrame post-execution.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def read_batch(self) -&gt; DataFrame:\n\"\"\"\n    Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data.\n    Returns:\n         Final Spark DataFrame converted from Pandas DataFrame post-execution.\n\n    \"\"\"\n\n    try:\n        self.pre_read_validation()\n        pdf = self._get_data()\n        pdf = _prepare_pandas_to_convert_to_spark(pdf)\n\n        # The below is to fix the compatibility issues between Pandas 2.0 and PySpark.\n        pd.DataFrame.iteritems = pd.DataFrame.items\n        df = self.spark.createDataFrame(data=pdf, schema=self.spark_schema)\n        return df\n\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/caiso_daily_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.read_stream","title":"<code>read_stream()</code>","text":"<p>By default, the streaming operation is not supported but child classes can override if ISO supports streaming.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Final Spark DataFrame after all the processing.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def read_stream(self) -&gt; DataFrame:\n\"\"\"\n    By default, the streaming operation is not supported but child classes can override if ISO supports streaming.\n\n    Returns:\n         Final Spark DataFrame after all the processing.\n\n    \"\"\"\n\n    raise NotImplementedError(\n        f\"{self.__class__.__name__} connector doesn't support stream operation.\"\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/caiso_historical_load_iso/","title":"CAISO Historical Load","text":""},{"location":"sdk/code-reference/pipelines/sources/spark/iso/caiso_historical_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.caiso_historical_load_iso.CAISOHistoricalLoadISOSource","title":"<code>CAISOHistoricalLoadISOSource</code>","text":"<p>             Bases: <code>CAISODailyLoadISOSource</code></p> <p>The CAISO Historical Load ISO Source is used to read load data for an interval of dates  between start_date and end_date inclusive from CAISO API. It supports multiple types of data. Check the <code>load_types</code> attribute. API: http://oasis.caiso.com/oasisapi  It creates batches of interval of 30 days and queries the CAISO API sequentially.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance</p> required <code>options</code> <code>dict</code> <p>A dictionary of ISO Source specific configurations (See Attributes table below)</p> required <p>Attributes:</p> Name Type Description <code>load_types</code> <code>list</code> <p>Must be a subset of [<code>Demand Forecast 7-Day Ahead</code>, <code>Demand Forecast 2-Day Ahead</code>, <code>Demand Forecast Day Ahead</code>, <code>RTM 15Min Load Forecast</code>, <code>RTM 5Min Load Forecast</code>, <code>Total Actual Hourly Integrated Load</code>].  Default Value - <code>[Total Actual Hourly Integrated Load]</code>.</p> <code>start_date</code> <code>str</code> <p>Must be in <code>YYYY-MM-DD</code> format.</p> <code>end_date</code> <code>str</code> <p>Must be in <code>YYYY-MM-DD</code> format.</p> <p>Please check the BaseISOSource for available methods.</p> BaseISOSource Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/caiso_historical_load_iso.py</code> <pre><code>class CAISOHistoricalLoadISOSource(CAISODailyLoadISOSource):\n\"\"\"\n    The CAISO Historical Load ISO Source is used to read load data for an interval of dates\n     between start_date and end_date inclusive from CAISO API.\n    It supports multiple types of data. Check the `load_types` attribute.\n    &lt;br&gt;API: &lt;a href=\"http://oasis.caiso.com/oasisapi\"&gt;http://oasis.caiso.com/oasisapi&lt;/a&gt;\n    &lt;br&gt; It creates batches of interval of 30 days and queries the CAISO API sequentially.\n\n    Parameters:\n        spark (SparkSession): Spark Session instance\n        options (dict): A dictionary of ISO Source specific configurations (See Attributes table below)\n\n    Attributes:\n        load_types (list): Must be a subset of [`Demand Forecast 7-Day Ahead`, `Demand Forecast 2-Day Ahead`, `Demand Forecast Day Ahead`, `RTM 15Min Load Forecast`, `RTM 5Min Load Forecast`, `Total Actual Hourly Integrated Load`]. &lt;br&gt; Default Value - `[Total Actual Hourly Integrated Load]`.\n        start_date (str): Must be in `YYYY-MM-DD` format.\n        end_date (str): Must be in `YYYY-MM-DD` format.\n\n    Please check the BaseISOSource for available methods.\n\n    BaseISOSource:\n        ::: src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso\n    \"\"\"\n\n    spark: SparkSession\n    options: dict\n    required_options = [\"load_types\", \"start_date\", \"end_date\"]\n\n    def __init__(self, spark: SparkSession, options: dict) -&gt; None:\n        super().__init__(spark, options)\n        self.spark = spark\n        self.options = options\n        self.load_types = self.options.get(\n            \"load_types\", [\"Total Actual Hourly Integrated Load\"]\n        )\n        self.start_date = self.options.get(\"start_date\", \"\").strip()\n        self.end_date = self.options.get(\"end_date\", \"\").strip()\n        self.user_datetime_format = \"%Y-%m-%d\"\n\n    def _pull_data(self) -&gt; pd.DataFrame:\n\"\"\"\n        Pulls data from the CAISO API and parses the zip files for CSV data.\n\n        Returns:\n            Raw form of data.\n        \"\"\"\n\n        logging.info(\n            f\"Getting {self.load_types} data from {self.start_date} to {self.end_date}\"\n        )\n        start_date = datetime.strptime(self.start_date, self.user_datetime_format)\n        end_date = datetime.strptime(self.end_date, self.user_datetime_format)\n        end_date = end_date + timedelta(days=1)\n        generated_days_ranges = []\n        dates = pd.date_range(start_date, end_date, freq=\"30D\", inclusive=\"left\")\n\n        for date in dates:\n            py_date = date.to_pydatetime()\n            date_last = py_date + timedelta(days=30)\n            date_last = min(date_last, end_date)\n            generated_days_ranges.append((py_date, date_last))\n\n        logging.info(f\"Generated date ranges are {generated_days_ranges}\")\n\n        dfs = []\n        for idx, date_range in enumerate(generated_days_ranges):\n            start_date_str, end_date_str = date_range\n            df = self._fetch_and_parse_zip(start_date_str, end_date_str)\n\n            dfs.append(df)\n\n        return pd.concat(dfs)\n\n    def _validate_options(self) -&gt; bool:\n        try:\n            datetime.strptime(self.start_date, self.user_datetime_format)\n        except ValueError:\n            raise ValueError(\n                f\"Unable to parse start_date. Please specify in {self.user_datetime_format} format.\"\n            )\n\n        try:\n            datetime.strptime(self.end_date, self.user_datetime_format)\n        except ValueError:\n            raise ValueError(\n                f\"Unable to parse end_date. Please specify in {self.user_datetime_format} format.\"\n            )\n\n        return True\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/caiso_historical_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource","title":"<code>BaseISOSource</code>","text":"<p>             Bases: <code>SourceInterface</code></p> <p>Base class for all the ISO Sources. It provides common functionality and helps in reducing the code redundancy.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance</p> required <code>options</code> <code>dict</code> <p>A dictionary of ISO Source specific configurations</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>class BaseISOSource(SourceInterface):\n\"\"\"\n    Base class for all the ISO Sources. It provides common functionality and helps in reducing the code redundancy.\n\n    Parameters:\n        spark (SparkSession): Spark Session instance\n        options (dict): A dictionary of ISO Source specific configurations\n    \"\"\"\n\n    spark: SparkSession\n    options: dict\n    iso_url: str = \"https://\"\n    query_datetime_format: str = \"%Y%m%d\"\n    required_options: list = []\n    spark_schema = StructType([StructField(\"id\", IntegerType(), True)])\n    default_query_timezone: str = \"UTC\"\n\n    def __init__(self, spark: SparkSession, options: dict) -&gt; None:\n        self.spark = spark\n        self.options = options\n        self.query_timezone = pytz.timezone(\n            self.options.get(\"query_timezone\", self.default_query_timezone)\n        )\n        self.current_date = datetime.now(timezone.utc).astimezone(self.query_timezone)\n\n    def _fetch_from_url(self, url_suffix: str) -&gt; bytes:\n\"\"\"\n        Gets data from external ISO API.\n\n        Args:\n            url_suffix: String to be used as suffix to iso url.\n\n        Returns:\n            Raw content of the data received.\n\n        \"\"\"\n        url = f\"{self.iso_url}{url_suffix}\"\n        logging.info(f\"Requesting URL - {url}\")\n\n        response = requests.get(url)\n        code = response.status_code\n\n        if code != 200:\n            raise HTTPError(\n                f\"Unable to access URL `{url}`.\"\n                f\" Received status code {code} with message {response.content}\"\n            )\n\n        return response.content\n\n    def _get_localized_datetime(self, datetime_str: str) -&gt; datetime:\n\"\"\"\n        Converts string datetime into Python datetime object with configured format and timezone.\n        Args:\n            datetime_str: String to be converted into datetime.\n\n        Returns: Timezone aware datetime object.\n\n        \"\"\"\n        parsed_dt = datetime.strptime(datetime_str, self.query_datetime_format)\n        parsed_dt = parsed_dt.replace(tzinfo=self.query_timezone)\n        return parsed_dt\n\n    def _pull_data(self) -&gt; pd.DataFrame:\n\"\"\"\n        Hits the fetch_from_url method with certain parameters to get raw data from API.\n\n        All the children ISO classes must override this method and call the fetch_url method\n        in it.\n\n        Returns:\n             Raw DataFrame from API.\n        \"\"\"\n\n        return pd.read_csv(BytesIO(self._fetch_from_url(\"\")))\n\n    def _prepare_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n        Performs all the basic transformations to prepare data for further processing.\n        All the children ISO classes must override this method.\n\n        Args:\n            df: Raw DataFrame, received from the API.\n\n        Returns:\n             Modified DataFrame, ready for basic use.\n\n        \"\"\"\n        return df\n\n    def _sanitize_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n        Another data transformation helper method to be called after prepare data.\n        Used for advance data processing such as cleaning, filtering, restructuring.\n        All the children ISO classes must override this method if there is any post-processing required.\n\n        Args:\n            df: Initial modified version of DataFrame, received after preparing the data.\n\n        Returns:\n             Final version of data after all the fixes and modifications.\n\n        \"\"\"\n        return df\n\n    def _get_data(self) -&gt; pd.DataFrame:\n\"\"\"\n        Entrypoint method to return the final version of DataFrame.\n\n        Returns:\n            Modified form of data for specific use case.\n\n        \"\"\"\n        df = self._pull_data()\n        df = self._prepare_data(df)\n        df = self._sanitize_data(df)\n\n        # Reorder columns to keep the data consistent\n        df = df[self.spark_schema.names]\n\n        return df\n\n    @staticmethod\n    def system_type():\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def _validate_options(self) -&gt; bool:\n\"\"\"\n        Performs all the options checks. Raises exception in case of any invalid value.\n        Returns:\n             True if all checks are passed.\n\n        \"\"\"\n        return True\n\n    def pre_read_validation(self) -&gt; bool:\n\"\"\"\n        Ensures all the required options are provided and performs other validations.\n        Returns:\n             True if all checks are passed.\n\n        \"\"\"\n        for key in self.required_options:\n            if key not in self.options:\n                raise ValueError(f\"Required option `{key}` is missing.\")\n\n        return self._validate_options()\n\n    def post_read_validation(self) -&gt; bool:\n        return True\n\n    def read_batch(self) -&gt; DataFrame:\n\"\"\"\n        Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data.\n        Returns:\n             Final Spark DataFrame converted from Pandas DataFrame post-execution.\n\n        \"\"\"\n\n        try:\n            self.pre_read_validation()\n            pdf = self._get_data()\n            pdf = _prepare_pandas_to_convert_to_spark(pdf)\n\n            # The below is to fix the compatibility issues between Pandas 2.0 and PySpark.\n            pd.DataFrame.iteritems = pd.DataFrame.items\n            df = self.spark.createDataFrame(data=pdf, schema=self.spark_schema)\n            return df\n\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def read_stream(self) -&gt; DataFrame:\n\"\"\"\n        By default, the streaming operation is not supported but child classes can override if ISO supports streaming.\n\n        Returns:\n             Final Spark DataFrame after all the processing.\n\n        \"\"\"\n\n        raise NotImplementedError(\n            f\"{self.__class__.__name__} connector doesn't support stream operation.\"\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/caiso_historical_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.pre_read_validation","title":"<code>pre_read_validation()</code>","text":"<p>Ensures all the required options are provided and performs other validations. Returns:      True if all checks are passed.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def pre_read_validation(self) -&gt; bool:\n\"\"\"\n    Ensures all the required options are provided and performs other validations.\n    Returns:\n         True if all checks are passed.\n\n    \"\"\"\n    for key in self.required_options:\n        if key not in self.options:\n            raise ValueError(f\"Required option `{key}` is missing.\")\n\n    return self._validate_options()\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/caiso_historical_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.read_batch","title":"<code>read_batch()</code>","text":"<p>Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data. Returns:      Final Spark DataFrame converted from Pandas DataFrame post-execution.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def read_batch(self) -&gt; DataFrame:\n\"\"\"\n    Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data.\n    Returns:\n         Final Spark DataFrame converted from Pandas DataFrame post-execution.\n\n    \"\"\"\n\n    try:\n        self.pre_read_validation()\n        pdf = self._get_data()\n        pdf = _prepare_pandas_to_convert_to_spark(pdf)\n\n        # The below is to fix the compatibility issues between Pandas 2.0 and PySpark.\n        pd.DataFrame.iteritems = pd.DataFrame.items\n        df = self.spark.createDataFrame(data=pdf, schema=self.spark_schema)\n        return df\n\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/caiso_historical_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.read_stream","title":"<code>read_stream()</code>","text":"<p>By default, the streaming operation is not supported but child classes can override if ISO supports streaming.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Final Spark DataFrame after all the processing.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def read_stream(self) -&gt; DataFrame:\n\"\"\"\n    By default, the streaming operation is not supported but child classes can override if ISO supports streaming.\n\n    Returns:\n         Final Spark DataFrame after all the processing.\n\n    \"\"\"\n\n    raise NotImplementedError(\n        f\"{self.__class__.__name__} connector doesn't support stream operation.\"\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/ercot_daily_load_iso/","title":"ERCOT Daily Load","text":""},{"location":"sdk/code-reference/pipelines/sources/spark/iso/ercot_daily_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.ercot_daily_load_iso.ERCOTDailyLoadISOSource","title":"<code>ERCOTDailyLoadISOSource</code>","text":"<p>             Bases: <code>BaseISOSource</code></p> <p>The ERCOT Daily Load ISO Source is used to read daily load data from ERCOT using WebScrapping. It supports actual and forecast data. API: https://mis.ercot.com</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance</p> required <code>options</code> <code>dict</code> <p>A dictionary of ISO Source specific configurations (See Attributes table below)</p> required <p>Attributes:</p> Name Type Description <code>load_type</code> <code>list</code> <p>Must be one of <code>actual</code> or <code>forecast</code>.</p> <code>date</code> <code>str</code> <p>Must be in <code>YYYY-MM-DD</code> format.</p> <code>certificate_pfx_key</code> <code>str</code> <p>The certificate key data or password received from ERCOT.</p> <code>certificate_pfx_key_contents</code> <code>str</code> <p>The certificate data received from ERCOT, it could be base64 encoded.</p> <p>Please check the BaseISOSource for available methods.</p> BaseISOSource Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/ercot_daily_load_iso.py</code> <pre><code>class ERCOTDailyLoadISOSource(BaseISOSource):\n\"\"\"\n    The ERCOT Daily Load ISO Source is used to read daily load data from ERCOT using WebScrapping.\n    It supports actual and forecast data.\n    &lt;br&gt;API: &lt;a href=\"https://mis.ercot.com\"&gt;https://mis.ercot.com&lt;/a&gt;\n\n\n    Parameters:\n        spark (SparkSession): Spark Session instance\n        options (dict): A dictionary of ISO Source specific configurations (See Attributes table below)\n\n    Attributes:\n        load_type (list): Must be one of `actual` or `forecast`.\n        date (str): Must be in `YYYY-MM-DD` format.\n        certificate_pfx_key (str): The certificate key data or password received from ERCOT.\n        certificate_pfx_key_contents (str): The certificate data received from ERCOT, it could be base64 encoded.\n\n    Please check the BaseISOSource for available methods.\n\n    BaseISOSource:\n        ::: src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso\n    \"\"\"\n\n    spark: SparkSession\n    options: dict\n    url_forecast: str = \"https://mis.ercot.com/misapp/GetReports.do?reportTypeId=12312\"\n    url_actual: str = \"https://mis.ercot.com/misapp/GetReports.do?reportTypeId=13101\"\n    url_prefix: str = \"https://mis.ercot.com\"\n    query_datetime_format: str = \"%Y-%m-%d\"\n    required_options = [\n        \"load_type\",\n        \"date\",\n        \"certificate_pfx_key\",\n        \"certificate_pfx_key_contents\",\n    ]\n    spark_schema = ERCOT_SCHEMA\n    default_query_timezone = \"UTC\"\n\n    def __init__(self, spark: SparkSession, options: dict) -&gt; None:\n        super().__init__(spark, options)\n        self.spark = spark\n        self.options = options\n        self.load_type = self.options.get(\"load_type\", \"actual\")\n        self.date = self.options.get(\"date\", \"\").strip()\n        self.certificate_pfx_key = self.options.get(\"certificate_pfx_key\", \"\").strip()\n        self.certificate_pfx_key_contents = self.options.get(\n            \"certificate_pfx_key_contents\", \"\"\n        ).strip()\n\n    def generate_temp_client_cert_files_from_pfx(self):\n        password = self.certificate_pfx_key.encode()\n        pfx: bytes = base64.b64decode(self.certificate_pfx_key_contents)\n\n        if base64.b64encode(pfx) != self.certificate_pfx_key_contents.encode():\n            pfx = self.certificate_pfx_key_contents\n\n        key, cert, _ = pkcs12.load_key_and_certificates(data=pfx, password=password)\n        key_bytes = key.private_bytes(\n            encoding=serialization.Encoding.PEM,\n            format=serialization.PrivateFormat.TraditionalOpenSSL,\n            encryption_algorithm=serialization.NoEncryption(),\n        )\n\n        cert_bytes = cert.public_bytes(encoding=serialization.Encoding.PEM)\n        return TempCertFiles(cert_bytes, key_bytes)\n\n    def _pull_data(self) -&gt; pd.DataFrame:\n\"\"\"\n        Pulls data from the ERCOT API and parses the zip files for CSV data.\n\n        Returns:\n            Raw form of data.\n        \"\"\"\n\n        logging.info(f\"Getting {self.load_type} data for date {self.date}\")\n        url = self.url_forecast\n        req_date = datetime.strptime(self.date, self.query_datetime_format)\n\n        if self.load_type == \"actual\":\n            req_date = req_date + timedelta(days=1)\n            url = self.url_actual\n\n        url_lists, files = self.generate_urls_for_zip(url, req_date)\n        dfs = []\n        logging.info(f\"Generated {len(url_lists)} URLs - {url_lists}\")\n        logging.info(f\"Requesting files - {files}\")\n\n        for url in url_lists:\n            df = self.download_zip(url)\n            dfs.append(df)\n        final_df = pd.concat(dfs)\n        return final_df\n\n    def download_zip(self, url) -&gt; pd.DataFrame:\n        logging.info(f\"Downloading zip using {url}\")\n        with self.generate_temp_client_cert_files_from_pfx() as cert:\n            response = requests.get(url, cert=cert)\n\n        if not response.content:\n            raise HTTPError(\"Empty Response was returned\")\n\n        logging.info(\"Unzipping the file\")\n        zf = ZipFile(BytesIO(response.content))\n        csvs = [s for s in zf.namelist() if \".csv\" in s]\n\n        if len(csvs) == 0:\n            raise ValueError(\"No data was found in the specified interval\")\n\n        df = pd.read_csv(zf.open(csvs[0]))\n        return df\n\n    def generate_urls_for_zip(self, url: str, date: datetime) -&gt; (List[str], List[str]):\n        logging.info(f\"Finding urls list for date {date}\")\n        with self.generate_temp_client_cert_files_from_pfx() as cert:\n            page_response = requests.get(url, timeout=5, cert=cert)\n\n        page_content = BeautifulSoup(page_response.content, \"html.parser\")\n        zip_info = []\n        length = len(page_content.find_all(\"td\", {\"class\": \"labelOptional_ind\"}))\n\n        for i in range(0, length):\n            zip_name = page_content.find_all(\"td\", {\"class\": \"labelOptional_ind\"})[\n                i\n            ].text\n            zip_link = page_content.find_all(\"a\")[i].get(\"href\")\n            zip_info.append((zip_name, zip_link))\n\n        date_str = date.strftime(\"%Y%m%d\")\n        zip_info = list(\n            filter(\n                lambda f_info: f_info[0].endswith(\"csv.zip\") and date_str in f_info[0],\n                zip_info,\n            )\n        )\n\n        urls = []\n        files = []\n\n        if len(zip_info) == 0:\n            raise ValueError(f\"No file was found for date - {date_str}\")\n\n        # As Forecast is generated every hour, pick the latest one.\n        zip_info = sorted(zip_info, key=lambda item: item[0], reverse=True)\n        zip_info_item = zip_info[0]\n\n        file_name, file_url = zip_info_item\n        urls.append(self.url_prefix + file_url)\n        files.append(file_name)\n\n        return urls, files\n\n    def _prepare_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        if self.load_type == \"actual\":\n            df[\"Date\"] = pd.to_datetime(df[\"OperDay\"], format=\"%m/%d/%Y\")\n\n            df = df.rename(\n                columns={\n                    \"COAST\": \"Coast\",\n                    \"EAST\": \"East\",\n                    \"FAR_WEST\": \"FarWest\",\n                    \"NORTH\": \"North\",\n                    \"NORTH_C\": \"NorthCentral\",\n                    \"SOUTH_C\": \"SouthCentral\",\n                    \"SOUTHERN\": \"Southern\",\n                    \"WEST\": \"West\",\n                    \"TOTAL\": \"SystemTotal\",\n                    \"DSTFlag\": \"DstFlag\",\n                }\n            )\n\n        else:\n            df = df.rename(columns={\"DSTFlag\": \"DstFlag\"})\n\n            df[\"Date\"] = pd.to_datetime(df[\"DeliveryDate\"], format=\"%m/%d/%Y\")\n\n        return df\n\n    def _validate_options(self) -&gt; bool:\n        try:\n            datetime.strptime(self.date, self.query_datetime_format)\n        except ValueError:\n            raise ValueError(\n                f\"Unable to parse date. Please specify in {self.query_datetime_format} format.\"\n            )\n        return True\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/ercot_daily_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource","title":"<code>BaseISOSource</code>","text":"<p>             Bases: <code>SourceInterface</code></p> <p>Base class for all the ISO Sources. It provides common functionality and helps in reducing the code redundancy.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance</p> required <code>options</code> <code>dict</code> <p>A dictionary of ISO Source specific configurations</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>class BaseISOSource(SourceInterface):\n\"\"\"\n    Base class for all the ISO Sources. It provides common functionality and helps in reducing the code redundancy.\n\n    Parameters:\n        spark (SparkSession): Spark Session instance\n        options (dict): A dictionary of ISO Source specific configurations\n    \"\"\"\n\n    spark: SparkSession\n    options: dict\n    iso_url: str = \"https://\"\n    query_datetime_format: str = \"%Y%m%d\"\n    required_options: list = []\n    spark_schema = StructType([StructField(\"id\", IntegerType(), True)])\n    default_query_timezone: str = \"UTC\"\n\n    def __init__(self, spark: SparkSession, options: dict) -&gt; None:\n        self.spark = spark\n        self.options = options\n        self.query_timezone = pytz.timezone(\n            self.options.get(\"query_timezone\", self.default_query_timezone)\n        )\n        self.current_date = datetime.now(timezone.utc).astimezone(self.query_timezone)\n\n    def _fetch_from_url(self, url_suffix: str) -&gt; bytes:\n\"\"\"\n        Gets data from external ISO API.\n\n        Args:\n            url_suffix: String to be used as suffix to iso url.\n\n        Returns:\n            Raw content of the data received.\n\n        \"\"\"\n        url = f\"{self.iso_url}{url_suffix}\"\n        logging.info(f\"Requesting URL - {url}\")\n\n        response = requests.get(url)\n        code = response.status_code\n\n        if code != 200:\n            raise HTTPError(\n                f\"Unable to access URL `{url}`.\"\n                f\" Received status code {code} with message {response.content}\"\n            )\n\n        return response.content\n\n    def _get_localized_datetime(self, datetime_str: str) -&gt; datetime:\n\"\"\"\n        Converts string datetime into Python datetime object with configured format and timezone.\n        Args:\n            datetime_str: String to be converted into datetime.\n\n        Returns: Timezone aware datetime object.\n\n        \"\"\"\n        parsed_dt = datetime.strptime(datetime_str, self.query_datetime_format)\n        parsed_dt = parsed_dt.replace(tzinfo=self.query_timezone)\n        return parsed_dt\n\n    def _pull_data(self) -&gt; pd.DataFrame:\n\"\"\"\n        Hits the fetch_from_url method with certain parameters to get raw data from API.\n\n        All the children ISO classes must override this method and call the fetch_url method\n        in it.\n\n        Returns:\n             Raw DataFrame from API.\n        \"\"\"\n\n        return pd.read_csv(BytesIO(self._fetch_from_url(\"\")))\n\n    def _prepare_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n        Performs all the basic transformations to prepare data for further processing.\n        All the children ISO classes must override this method.\n\n        Args:\n            df: Raw DataFrame, received from the API.\n\n        Returns:\n             Modified DataFrame, ready for basic use.\n\n        \"\"\"\n        return df\n\n    def _sanitize_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n        Another data transformation helper method to be called after prepare data.\n        Used for advance data processing such as cleaning, filtering, restructuring.\n        All the children ISO classes must override this method if there is any post-processing required.\n\n        Args:\n            df: Initial modified version of DataFrame, received after preparing the data.\n\n        Returns:\n             Final version of data after all the fixes and modifications.\n\n        \"\"\"\n        return df\n\n    def _get_data(self) -&gt; pd.DataFrame:\n\"\"\"\n        Entrypoint method to return the final version of DataFrame.\n\n        Returns:\n            Modified form of data for specific use case.\n\n        \"\"\"\n        df = self._pull_data()\n        df = self._prepare_data(df)\n        df = self._sanitize_data(df)\n\n        # Reorder columns to keep the data consistent\n        df = df[self.spark_schema.names]\n\n        return df\n\n    @staticmethod\n    def system_type():\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def _validate_options(self) -&gt; bool:\n\"\"\"\n        Performs all the options checks. Raises exception in case of any invalid value.\n        Returns:\n             True if all checks are passed.\n\n        \"\"\"\n        return True\n\n    def pre_read_validation(self) -&gt; bool:\n\"\"\"\n        Ensures all the required options are provided and performs other validations.\n        Returns:\n             True if all checks are passed.\n\n        \"\"\"\n        for key in self.required_options:\n            if key not in self.options:\n                raise ValueError(f\"Required option `{key}` is missing.\")\n\n        return self._validate_options()\n\n    def post_read_validation(self) -&gt; bool:\n        return True\n\n    def read_batch(self) -&gt; DataFrame:\n\"\"\"\n        Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data.\n        Returns:\n             Final Spark DataFrame converted from Pandas DataFrame post-execution.\n\n        \"\"\"\n\n        try:\n            self.pre_read_validation()\n            pdf = self._get_data()\n            pdf = _prepare_pandas_to_convert_to_spark(pdf)\n\n            # The below is to fix the compatibility issues between Pandas 2.0 and PySpark.\n            pd.DataFrame.iteritems = pd.DataFrame.items\n            df = self.spark.createDataFrame(data=pdf, schema=self.spark_schema)\n            return df\n\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def read_stream(self) -&gt; DataFrame:\n\"\"\"\n        By default, the streaming operation is not supported but child classes can override if ISO supports streaming.\n\n        Returns:\n             Final Spark DataFrame after all the processing.\n\n        \"\"\"\n\n        raise NotImplementedError(\n            f\"{self.__class__.__name__} connector doesn't support stream operation.\"\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/ercot_daily_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.pre_read_validation","title":"<code>pre_read_validation()</code>","text":"<p>Ensures all the required options are provided and performs other validations. Returns:      True if all checks are passed.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def pre_read_validation(self) -&gt; bool:\n\"\"\"\n    Ensures all the required options are provided and performs other validations.\n    Returns:\n         True if all checks are passed.\n\n    \"\"\"\n    for key in self.required_options:\n        if key not in self.options:\n            raise ValueError(f\"Required option `{key}` is missing.\")\n\n    return self._validate_options()\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/ercot_daily_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.read_batch","title":"<code>read_batch()</code>","text":"<p>Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data. Returns:      Final Spark DataFrame converted from Pandas DataFrame post-execution.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def read_batch(self) -&gt; DataFrame:\n\"\"\"\n    Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data.\n    Returns:\n         Final Spark DataFrame converted from Pandas DataFrame post-execution.\n\n    \"\"\"\n\n    try:\n        self.pre_read_validation()\n        pdf = self._get_data()\n        pdf = _prepare_pandas_to_convert_to_spark(pdf)\n\n        # The below is to fix the compatibility issues between Pandas 2.0 and PySpark.\n        pd.DataFrame.iteritems = pd.DataFrame.items\n        df = self.spark.createDataFrame(data=pdf, schema=self.spark_schema)\n        return df\n\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/ercot_daily_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.read_stream","title":"<code>read_stream()</code>","text":"<p>By default, the streaming operation is not supported but child classes can override if ISO supports streaming.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Final Spark DataFrame after all the processing.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def read_stream(self) -&gt; DataFrame:\n\"\"\"\n    By default, the streaming operation is not supported but child classes can override if ISO supports streaming.\n\n    Returns:\n         Final Spark DataFrame after all the processing.\n\n    \"\"\"\n\n    raise NotImplementedError(\n        f\"{self.__class__.__name__} connector doesn't support stream operation.\"\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/miso_daily_load_iso/","title":"MISO Daily Load","text":""},{"location":"sdk/code-reference/pipelines/sources/spark/iso/miso_daily_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.miso_daily_load_iso.MISODailyLoadISOSource","title":"<code>MISODailyLoadISOSource</code>","text":"<p>             Bases: <code>BaseISOSource</code></p> <p>The MISO Daily Load ISO Source is used to read daily load data from MISO API. It supports both Actual and Forecast data.</p> <p>API: https://docs.misoenergy.org/marketreports/</p> <p>Actual data is available for one day minus from the given date.</p> <p>Forecast data is available for next 6 day (inclusive of given date).</p>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/miso_daily_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.miso_daily_load_iso.MISODailyLoadISOSource--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.sources import MISODailyLoadISOSource\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\nmiso_source = MISODailyLoadISOSource(\n    spark=spark,\n    options={\n        \"load_type\": \"actual\",\n        \"date\": \"20230520\",\n    }\n)\n\nmiso_source.read_batch()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance</p> required <code>options</code> <code>dict</code> <p>A dictionary of ISO Source specific configurations (See Attributes table below)</p> required <p>Attributes:</p> Name Type Description <code>load_type</code> <code>str</code> <p>Must be one of <code>actual</code> or <code>forecast</code></p> <code>date</code> <code>str</code> <p>Must be in <code>YYYYMMDD</code> format.</p> <p>Please check the BaseISOSource for available methods.</p> BaseISOSource Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/miso_daily_load_iso.py</code> <pre><code>class MISODailyLoadISOSource(BaseISOSource):\n\"\"\"\n    The MISO Daily Load ISO Source is used to read daily load data from MISO API. It supports both Actual and Forecast data.\n\n    API: &lt;a href=\"https://docs.misoenergy.org/marketreports/\"&gt;https://docs.misoenergy.org/marketreports/&lt;/a&gt;\n\n    Actual data is available for one day minus from the given date.\n\n    Forecast data is available for next 6 day (inclusive of given date).\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.sources import MISODailyLoadISOSource\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    miso_source = MISODailyLoadISOSource(\n        spark=spark,\n        options={\n            \"load_type\": \"actual\",\n            \"date\": \"20230520\",\n        }\n    )\n\n    miso_source.read_batch()\n    ```\n\n    Parameters:\n        spark (SparkSession): Spark Session instance\n        options (dict): A dictionary of ISO Source specific configurations (See Attributes table below)\n\n    Attributes:\n        load_type (str): Must be one of `actual` or `forecast`\n        date (str): Must be in `YYYYMMDD` format.\n\n    Please check the BaseISOSource for available methods.\n\n    BaseISOSource:\n        ::: src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso\n    \"\"\"\n\n    spark: SparkSession\n    options: dict\n    iso_url: str = \"https://docs.misoenergy.org/marketreports/\"\n    query_datetime_format: str = \"%Y%m%d\"\n    required_options = [\"load_type\", \"date\"]\n    spark_schema = MISO_SCHEMA\n    default_query_timezone = \"US/Central\"\n\n    def __init__(self, spark: SparkSession, options: dict) -&gt; None:\n        super().__init__(spark, options)\n        self.spark = spark\n        self.options = options\n        self.load_type = self.options.get(\"load_type\", \"actual\")\n        self.date = self.options.get(\"date\", \"\").strip()\n\n    def _pull_data(self) -&gt; pd.DataFrame:\n\"\"\"\n        Pulls data from the MISO API and parses the Excel file.\n\n        Returns:\n            Raw form of data.\n        \"\"\"\n\n        logging.info(f\"Getting {self.load_type} data for date {self.date}\")\n        df = pd.read_excel(self._fetch_from_url(f\"{self.date}_df_al.xls\"), skiprows=4)\n\n        return df\n\n    def _prepare_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n        Creates a new `date_time` column and removes null values.\n\n        Args:\n            df: Raw form of data received from the API.\n\n        Returns:\n            Data after basic transformations.\n\n        \"\"\"\n\n        df.drop(\n            df.index[(df[\"HourEnding\"] == \"HourEnding\") | df[\"MISO MTLF (MWh)\"].isna()],\n            inplace=True,\n        )\n        df.rename(columns={\"Market Day\": \"date\"}, inplace=True)\n\n        df[\"date_time\"] = pd.to_datetime(df[\"date\"]) + pd.to_timedelta(\n            df[\"HourEnding\"].astype(int) - 1, \"h\"\n        )\n        df.drop([\"HourEnding\", \"date\"], axis=1, inplace=True)\n\n        data_cols = df.columns[df.columns != \"date_time\"]\n        df[data_cols] = df[data_cols].astype(float)\n\n        df.reset_index(inplace=True, drop=True)\n\n        return df\n\n    def _sanitize_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n        Filter outs Actual or Forecast data based on `load_type`.\n        Args:\n            df: Data received after preparation.\n\n        Returns:\n            Final data either containing Actual or Forecast values.\n\n        \"\"\"\n\n        skip_col_suffix = \"\"\n\n        if self.load_type == \"actual\":\n            skip_col_suffix = \"MTLF (MWh)\"\n\n        elif self.load_type == \"forecast\":\n            skip_col_suffix = \"ActualLoad (MWh)\"\n\n        df = df[[x for x in df.columns if not x.endswith(skip_col_suffix)]]\n        df = df.dropna()\n        df.columns = [str(x.split(\" \")[0]).upper() for x in df.columns]\n\n        rename_cols = {\n            \"LRZ1\": \"Lrz1\",\n            \"LRZ2_7\": \"Lrz2_7\",\n            \"LRZ3_5\": \"Lrz3_5\",\n            \"LRZ4\": \"Lrz4\",\n            \"LRZ6\": \"Lrz6\",\n            \"LRZ8_9_10\": \"Lrz8_9_10\",\n            \"MISO\": \"Miso\",\n            \"DATE_TIME\": \"Datetime\",\n        }\n\n        df = df.rename(columns=rename_cols)\n\n        return df\n\n    def _validate_options(self) -&gt; bool:\n\"\"\"\n        Validates the following options:\n            - `date` must be in the correct format.\n            - `load_type` must be valid.\n\n        Returns:\n            True if all looks good otherwise raises Exception.\n\n        \"\"\"\n\n        try:\n            date = self._get_localized_datetime(self.date)\n        except ValueError:\n            raise ValueError(\"Unable to parse Date. Please specify in YYYYMMDD format.\")\n\n        if date &gt; self.current_date:\n            raise ValueError(\"Query date can't be in future.\")\n\n        valid_load_types = [\"actual\", \"forecast\"]\n\n        if self.load_type not in valid_load_types:\n            raise ValueError(\n                f\"Invalid load_type `{self.load_type}` given. Supported values are {valid_load_types}.\"\n            )\n\n        return True\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/miso_daily_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource","title":"<code>BaseISOSource</code>","text":"<p>             Bases: <code>SourceInterface</code></p> <p>Base class for all the ISO Sources. It provides common functionality and helps in reducing the code redundancy.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance</p> required <code>options</code> <code>dict</code> <p>A dictionary of ISO Source specific configurations</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>class BaseISOSource(SourceInterface):\n\"\"\"\n    Base class for all the ISO Sources. It provides common functionality and helps in reducing the code redundancy.\n\n    Parameters:\n        spark (SparkSession): Spark Session instance\n        options (dict): A dictionary of ISO Source specific configurations\n    \"\"\"\n\n    spark: SparkSession\n    options: dict\n    iso_url: str = \"https://\"\n    query_datetime_format: str = \"%Y%m%d\"\n    required_options: list = []\n    spark_schema = StructType([StructField(\"id\", IntegerType(), True)])\n    default_query_timezone: str = \"UTC\"\n\n    def __init__(self, spark: SparkSession, options: dict) -&gt; None:\n        self.spark = spark\n        self.options = options\n        self.query_timezone = pytz.timezone(\n            self.options.get(\"query_timezone\", self.default_query_timezone)\n        )\n        self.current_date = datetime.now(timezone.utc).astimezone(self.query_timezone)\n\n    def _fetch_from_url(self, url_suffix: str) -&gt; bytes:\n\"\"\"\n        Gets data from external ISO API.\n\n        Args:\n            url_suffix: String to be used as suffix to iso url.\n\n        Returns:\n            Raw content of the data received.\n\n        \"\"\"\n        url = f\"{self.iso_url}{url_suffix}\"\n        logging.info(f\"Requesting URL - {url}\")\n\n        response = requests.get(url)\n        code = response.status_code\n\n        if code != 200:\n            raise HTTPError(\n                f\"Unable to access URL `{url}`.\"\n                f\" Received status code {code} with message {response.content}\"\n            )\n\n        return response.content\n\n    def _get_localized_datetime(self, datetime_str: str) -&gt; datetime:\n\"\"\"\n        Converts string datetime into Python datetime object with configured format and timezone.\n        Args:\n            datetime_str: String to be converted into datetime.\n\n        Returns: Timezone aware datetime object.\n\n        \"\"\"\n        parsed_dt = datetime.strptime(datetime_str, self.query_datetime_format)\n        parsed_dt = parsed_dt.replace(tzinfo=self.query_timezone)\n        return parsed_dt\n\n    def _pull_data(self) -&gt; pd.DataFrame:\n\"\"\"\n        Hits the fetch_from_url method with certain parameters to get raw data from API.\n\n        All the children ISO classes must override this method and call the fetch_url method\n        in it.\n\n        Returns:\n             Raw DataFrame from API.\n        \"\"\"\n\n        return pd.read_csv(BytesIO(self._fetch_from_url(\"\")))\n\n    def _prepare_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n        Performs all the basic transformations to prepare data for further processing.\n        All the children ISO classes must override this method.\n\n        Args:\n            df: Raw DataFrame, received from the API.\n\n        Returns:\n             Modified DataFrame, ready for basic use.\n\n        \"\"\"\n        return df\n\n    def _sanitize_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n        Another data transformation helper method to be called after prepare data.\n        Used for advance data processing such as cleaning, filtering, restructuring.\n        All the children ISO classes must override this method if there is any post-processing required.\n\n        Args:\n            df: Initial modified version of DataFrame, received after preparing the data.\n\n        Returns:\n             Final version of data after all the fixes and modifications.\n\n        \"\"\"\n        return df\n\n    def _get_data(self) -&gt; pd.DataFrame:\n\"\"\"\n        Entrypoint method to return the final version of DataFrame.\n\n        Returns:\n            Modified form of data for specific use case.\n\n        \"\"\"\n        df = self._pull_data()\n        df = self._prepare_data(df)\n        df = self._sanitize_data(df)\n\n        # Reorder columns to keep the data consistent\n        df = df[self.spark_schema.names]\n\n        return df\n\n    @staticmethod\n    def system_type():\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def _validate_options(self) -&gt; bool:\n\"\"\"\n        Performs all the options checks. Raises exception in case of any invalid value.\n        Returns:\n             True if all checks are passed.\n\n        \"\"\"\n        return True\n\n    def pre_read_validation(self) -&gt; bool:\n\"\"\"\n        Ensures all the required options are provided and performs other validations.\n        Returns:\n             True if all checks are passed.\n\n        \"\"\"\n        for key in self.required_options:\n            if key not in self.options:\n                raise ValueError(f\"Required option `{key}` is missing.\")\n\n        return self._validate_options()\n\n    def post_read_validation(self) -&gt; bool:\n        return True\n\n    def read_batch(self) -&gt; DataFrame:\n\"\"\"\n        Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data.\n        Returns:\n             Final Spark DataFrame converted from Pandas DataFrame post-execution.\n\n        \"\"\"\n\n        try:\n            self.pre_read_validation()\n            pdf = self._get_data()\n            pdf = _prepare_pandas_to_convert_to_spark(pdf)\n\n            # The below is to fix the compatibility issues between Pandas 2.0 and PySpark.\n            pd.DataFrame.iteritems = pd.DataFrame.items\n            df = self.spark.createDataFrame(data=pdf, schema=self.spark_schema)\n            return df\n\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def read_stream(self) -&gt; DataFrame:\n\"\"\"\n        By default, the streaming operation is not supported but child classes can override if ISO supports streaming.\n\n        Returns:\n             Final Spark DataFrame after all the processing.\n\n        \"\"\"\n\n        raise NotImplementedError(\n            f\"{self.__class__.__name__} connector doesn't support stream operation.\"\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/miso_daily_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.pre_read_validation","title":"<code>pre_read_validation()</code>","text":"<p>Ensures all the required options are provided and performs other validations. Returns:      True if all checks are passed.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def pre_read_validation(self) -&gt; bool:\n\"\"\"\n    Ensures all the required options are provided and performs other validations.\n    Returns:\n         True if all checks are passed.\n\n    \"\"\"\n    for key in self.required_options:\n        if key not in self.options:\n            raise ValueError(f\"Required option `{key}` is missing.\")\n\n    return self._validate_options()\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/miso_daily_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.read_batch","title":"<code>read_batch()</code>","text":"<p>Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data. Returns:      Final Spark DataFrame converted from Pandas DataFrame post-execution.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def read_batch(self) -&gt; DataFrame:\n\"\"\"\n    Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data.\n    Returns:\n         Final Spark DataFrame converted from Pandas DataFrame post-execution.\n\n    \"\"\"\n\n    try:\n        self.pre_read_validation()\n        pdf = self._get_data()\n        pdf = _prepare_pandas_to_convert_to_spark(pdf)\n\n        # The below is to fix the compatibility issues between Pandas 2.0 and PySpark.\n        pd.DataFrame.iteritems = pd.DataFrame.items\n        df = self.spark.createDataFrame(data=pdf, schema=self.spark_schema)\n        return df\n\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/miso_daily_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.read_stream","title":"<code>read_stream()</code>","text":"<p>By default, the streaming operation is not supported but child classes can override if ISO supports streaming.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Final Spark DataFrame after all the processing.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def read_stream(self) -&gt; DataFrame:\n\"\"\"\n    By default, the streaming operation is not supported but child classes can override if ISO supports streaming.\n\n    Returns:\n         Final Spark DataFrame after all the processing.\n\n    \"\"\"\n\n    raise NotImplementedError(\n        f\"{self.__class__.__name__} connector doesn't support stream operation.\"\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/miso_historical_load_iso/","title":"MISO Historical Load","text":""},{"location":"sdk/code-reference/pipelines/sources/spark/iso/miso_historical_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.miso_historical_load_iso.MISOHistoricalLoadISOSource","title":"<code>MISOHistoricalLoadISOSource</code>","text":"<p>             Bases: <code>MISODailyLoadISOSource</code></p> <p>The MISO Historical Load ISO Source is used to read historical load data from MISO API.</p> <p>API: https://docs.misoenergy.org/marketreports/</p>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/miso_historical_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.miso_historical_load_iso.MISOHistoricalLoadISOSource--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.sources import MISOHistoricalLoadISOSource\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\nmiso_source = MISOHistoricalLoadISOSource(\n    spark=spark,\n    options={\n        \"start_date\": \"20230510\",\n        \"end_date\": \"20230520\",\n    }\n)\n\nmiso_source.read_batch()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance</p> required <code>options</code> <code>dict</code> <p>A dictionary of ISO Source specific configurations (See Attributes table below)</p> required <p>Attributes:</p> Name Type Description <code>start_date</code> <code>str</code> <p>Must be in <code>YYYYMMDD</code> format.</p> <code>end_date</code> <code>str</code> <p>Must be in <code>YYYYMMDD</code> format.</p> <code>fill_missing</code> <code>str</code> <p>Set to <code>\"true\"</code> to fill missing Actual load with Forecast load. Default - <code>true</code>.</p> <p>Please check the BaseISOSource for available methods.</p> BaseISOSource Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/miso_historical_load_iso.py</code> <pre><code>class MISOHistoricalLoadISOSource(MISODailyLoadISOSource):\n\"\"\"\n    The MISO Historical Load ISO Source is used to read historical load data from MISO API.\n\n    API: &lt;a href=\"https://docs.misoenergy.org/marketreports/\"&gt;https://docs.misoenergy.org/marketreports/&lt;/a&gt;\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.sources import MISOHistoricalLoadISOSource\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    miso_source = MISOHistoricalLoadISOSource(\n        spark=spark,\n        options={\n            \"start_date\": \"20230510\",\n            \"end_date\": \"20230520\",\n        }\n    )\n\n    miso_source.read_batch()\n    ```\n\n    Parameters:\n        spark (SparkSession): Spark Session instance\n        options (dict): A dictionary of ISO Source specific configurations (See Attributes table below)\n\n    Attributes:\n        start_date (str): Must be in `YYYYMMDD` format.\n        end_date (str): Must be in `YYYYMMDD` format.\n        fill_missing (str): Set to `\"true\"` to fill missing Actual load with Forecast load. Default - `true`.\n\n    Please check the BaseISOSource for available methods.\n\n    BaseISOSource:\n        ::: src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso\n    \"\"\"\n\n    spark: SparkSession\n    options: dict\n    required_options = [\"start_date\", \"end_date\"]\n\n    def __init__(self, spark: SparkSession, options: dict):\n        super().__init__(spark, options)\n        self.start_date = self.options.get(\"start_date\", \"\")\n        self.end_date = self.options.get(\"end_date\", \"\")\n        self.fill_missing = bool(self.options.get(\"fill_missing\", \"true\") == \"true\")\n\n    def _get_historical_data_for_date(self, date: datetime) -&gt; pd.DataFrame:\n        logging.info(f\"Getting historical data for date {date}\")\n        df = pd.read_excel(\n            self._fetch_from_url(\n                f\"{date.strftime(self.query_datetime_format)}_dfal_HIST.xls\"\n            ),\n            skiprows=5,\n        )\n\n        if date.month == 12 and date.day == 31:\n            expected_year_rows = (\n                pd.Timestamp(date.year, 12, 31).dayofyear * 24 * 7\n            )  # Every hour has 7 zones.\n            received_year_rows = (\n                len(df[df[\"MarketDay\"] != \"MarketDay\"]) - 2\n            )  # Last 2 rows are invalid.\n\n            if expected_year_rows != received_year_rows:\n                logging.warning(\n                    f\"Didn't receive full year historical data for year {date.year}.\"\n                    f\" Expected {expected_year_rows} but Received {received_year_rows}\"\n                )\n\n        return df\n\n    def _pull_data(self) -&gt; pd.DataFrame:\n\"\"\"\n        Pulls data from the MISO API and parses the Excel file.\n\n        Returns:\n            Raw form of data.\n        \"\"\"\n\n        logging.info(\n            f\"Historical load requested from {self.start_date} to {self.end_date}\"\n        )\n\n        start_date = self._get_localized_datetime(self.start_date)\n        end_date = self._get_localized_datetime(self.end_date)\n\n        dates = pd.date_range(\n            start_date, end_date + timedelta(days=365), freq=\"Y\", inclusive=\"left\"\n        )\n        logging.info(f\"Generated date ranges are - {dates}\")\n\n        # Collect all historical data on yearly basis.\n        df = pd.concat(\n            [\n                self._get_historical_data_for_date(min(date, self.current_date))\n                for date in dates\n            ],\n            sort=False,\n        )\n\n        return df\n\n    def _prepare_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n        Creates a new `Datetime` column, removes null values and pivots the data.\n\n        Args:\n            df: Raw form of data received from the API.\n\n        Returns:\n            Data after basic transformations and pivoting.\n\n        \"\"\"\n\n        df = df[df[\"MarketDay\"] != \"MarketDay\"]\n\n        # Fill missing actual values with the forecast values to avoid gaps.\n        if self.fill_missing:\n            df = df.fillna({\"ActualLoad (MWh)\": df[\"MTLF (MWh)\"]})\n\n        df = df.rename(\n            columns={\n                \"MarketDay\": \"date\",\n                \"HourEnding\": \"hour\",\n                \"ActualLoad (MWh)\": \"load\",\n                \"LoadResource Zone\": \"zone\",\n            }\n        )\n        df = df.dropna()\n\n        df[\"date_time\"] = pd.to_datetime(df[\"date\"]) + pd.to_timedelta(\n            df[\"hour\"].astype(int) - 1, \"h\"\n        )\n\n        df.drop([\"hour\", \"date\"], axis=1, inplace=True)\n        df[\"load\"] = df[\"load\"].astype(float)\n\n        df = df.pivot_table(\n            index=\"date_time\", values=\"load\", columns=\"zone\"\n        ).reset_index()\n\n        df.columns = [str(x.split(\" \")[0]).upper() for x in df.columns]\n\n        rename_cols = {\n            \"LRZ1\": \"Lrz1\",\n            \"LRZ2_7\": \"Lrz2_7\",\n            \"LRZ3_5\": \"Lrz3_5\",\n            \"LRZ4\": \"Lrz4\",\n            \"LRZ6\": \"Lrz6\",\n            \"LRZ8_9_10\": \"Lrz8_9_10\",\n            \"MISO\": \"Miso\",\n            \"DATE_TIME\": \"Datetime\",\n        }\n\n        df = df.rename(columns=rename_cols)\n\n        return df\n\n    def _sanitize_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n        Filter outs data outside the requested date range.\n\n        Args:\n            df: Data received after preparation.\n\n        Returns:\n            Final data after all the transformations.\n\n        \"\"\"\n\n        start_date = self._get_localized_datetime(self.start_date)\n        end_date = self._get_localized_datetime(self.end_date).replace(\n            hour=23, minute=59, second=59\n        )\n\n        df = df[\n            (df[\"Datetime\"] &gt;= start_date.replace(tzinfo=None))\n            &amp; (df[\"Datetime\"] &lt;= end_date.replace(tzinfo=None))\n        ]\n\n        df = df.sort_values(by=\"Datetime\", ascending=True).reset_index(drop=True)\n\n        expected_rows = ((min(end_date, self.current_date) - start_date).days + 1) * 24\n\n        actual_rows = len(df)\n\n        logging.info(f\"Rows Expected = {expected_rows}, Rows Found = {actual_rows}\")\n\n        return df\n\n    def _validate_options(self) -&gt; bool:\n\"\"\"\n        Validates the following options:\n            - `start_date` &amp; `end_data` must be in the correct format.\n            - `start_date` must be behind `end_data`.\n            - `start_date` must not be in the future (UTC).\n\n        Returns:\n            True if all looks good otherwise raises Exception.\n\n        \"\"\"\n\n        try:\n            start_date = self._get_localized_datetime(self.start_date)\n        except ValueError:\n            raise ValueError(\n                \"Unable to parse Start date. Please specify in YYYYMMDD format.\"\n            )\n\n        try:\n            end_date = self._get_localized_datetime(self.end_date)\n        except ValueError:\n            raise ValueError(\n                \"Unable to parse End date. Please specify in YYYYMMDD format.\"\n            )\n\n        if start_date &gt; self.current_date:\n            raise ValueError(\"Start date can't be in future.\")\n\n        if start_date &gt; end_date:\n            raise ValueError(\"Start date can't be ahead of End date.\")\n\n        return True\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/miso_historical_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource","title":"<code>BaseISOSource</code>","text":"<p>             Bases: <code>SourceInterface</code></p> <p>Base class for all the ISO Sources. It provides common functionality and helps in reducing the code redundancy.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance</p> required <code>options</code> <code>dict</code> <p>A dictionary of ISO Source specific configurations</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>class BaseISOSource(SourceInterface):\n\"\"\"\n    Base class for all the ISO Sources. It provides common functionality and helps in reducing the code redundancy.\n\n    Parameters:\n        spark (SparkSession): Spark Session instance\n        options (dict): A dictionary of ISO Source specific configurations\n    \"\"\"\n\n    spark: SparkSession\n    options: dict\n    iso_url: str = \"https://\"\n    query_datetime_format: str = \"%Y%m%d\"\n    required_options: list = []\n    spark_schema = StructType([StructField(\"id\", IntegerType(), True)])\n    default_query_timezone: str = \"UTC\"\n\n    def __init__(self, spark: SparkSession, options: dict) -&gt; None:\n        self.spark = spark\n        self.options = options\n        self.query_timezone = pytz.timezone(\n            self.options.get(\"query_timezone\", self.default_query_timezone)\n        )\n        self.current_date = datetime.now(timezone.utc).astimezone(self.query_timezone)\n\n    def _fetch_from_url(self, url_suffix: str) -&gt; bytes:\n\"\"\"\n        Gets data from external ISO API.\n\n        Args:\n            url_suffix: String to be used as suffix to iso url.\n\n        Returns:\n            Raw content of the data received.\n\n        \"\"\"\n        url = f\"{self.iso_url}{url_suffix}\"\n        logging.info(f\"Requesting URL - {url}\")\n\n        response = requests.get(url)\n        code = response.status_code\n\n        if code != 200:\n            raise HTTPError(\n                f\"Unable to access URL `{url}`.\"\n                f\" Received status code {code} with message {response.content}\"\n            )\n\n        return response.content\n\n    def _get_localized_datetime(self, datetime_str: str) -&gt; datetime:\n\"\"\"\n        Converts string datetime into Python datetime object with configured format and timezone.\n        Args:\n            datetime_str: String to be converted into datetime.\n\n        Returns: Timezone aware datetime object.\n\n        \"\"\"\n        parsed_dt = datetime.strptime(datetime_str, self.query_datetime_format)\n        parsed_dt = parsed_dt.replace(tzinfo=self.query_timezone)\n        return parsed_dt\n\n    def _pull_data(self) -&gt; pd.DataFrame:\n\"\"\"\n        Hits the fetch_from_url method with certain parameters to get raw data from API.\n\n        All the children ISO classes must override this method and call the fetch_url method\n        in it.\n\n        Returns:\n             Raw DataFrame from API.\n        \"\"\"\n\n        return pd.read_csv(BytesIO(self._fetch_from_url(\"\")))\n\n    def _prepare_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n        Performs all the basic transformations to prepare data for further processing.\n        All the children ISO classes must override this method.\n\n        Args:\n            df: Raw DataFrame, received from the API.\n\n        Returns:\n             Modified DataFrame, ready for basic use.\n\n        \"\"\"\n        return df\n\n    def _sanitize_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n        Another data transformation helper method to be called after prepare data.\n        Used for advance data processing such as cleaning, filtering, restructuring.\n        All the children ISO classes must override this method if there is any post-processing required.\n\n        Args:\n            df: Initial modified version of DataFrame, received after preparing the data.\n\n        Returns:\n             Final version of data after all the fixes and modifications.\n\n        \"\"\"\n        return df\n\n    def _get_data(self) -&gt; pd.DataFrame:\n\"\"\"\n        Entrypoint method to return the final version of DataFrame.\n\n        Returns:\n            Modified form of data for specific use case.\n\n        \"\"\"\n        df = self._pull_data()\n        df = self._prepare_data(df)\n        df = self._sanitize_data(df)\n\n        # Reorder columns to keep the data consistent\n        df = df[self.spark_schema.names]\n\n        return df\n\n    @staticmethod\n    def system_type():\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def _validate_options(self) -&gt; bool:\n\"\"\"\n        Performs all the options checks. Raises exception in case of any invalid value.\n        Returns:\n             True if all checks are passed.\n\n        \"\"\"\n        return True\n\n    def pre_read_validation(self) -&gt; bool:\n\"\"\"\n        Ensures all the required options are provided and performs other validations.\n        Returns:\n             True if all checks are passed.\n\n        \"\"\"\n        for key in self.required_options:\n            if key not in self.options:\n                raise ValueError(f\"Required option `{key}` is missing.\")\n\n        return self._validate_options()\n\n    def post_read_validation(self) -&gt; bool:\n        return True\n\n    def read_batch(self) -&gt; DataFrame:\n\"\"\"\n        Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data.\n        Returns:\n             Final Spark DataFrame converted from Pandas DataFrame post-execution.\n\n        \"\"\"\n\n        try:\n            self.pre_read_validation()\n            pdf = self._get_data()\n            pdf = _prepare_pandas_to_convert_to_spark(pdf)\n\n            # The below is to fix the compatibility issues between Pandas 2.0 and PySpark.\n            pd.DataFrame.iteritems = pd.DataFrame.items\n            df = self.spark.createDataFrame(data=pdf, schema=self.spark_schema)\n            return df\n\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def read_stream(self) -&gt; DataFrame:\n\"\"\"\n        By default, the streaming operation is not supported but child classes can override if ISO supports streaming.\n\n        Returns:\n             Final Spark DataFrame after all the processing.\n\n        \"\"\"\n\n        raise NotImplementedError(\n            f\"{self.__class__.__name__} connector doesn't support stream operation.\"\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/miso_historical_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.pre_read_validation","title":"<code>pre_read_validation()</code>","text":"<p>Ensures all the required options are provided and performs other validations. Returns:      True if all checks are passed.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def pre_read_validation(self) -&gt; bool:\n\"\"\"\n    Ensures all the required options are provided and performs other validations.\n    Returns:\n         True if all checks are passed.\n\n    \"\"\"\n    for key in self.required_options:\n        if key not in self.options:\n            raise ValueError(f\"Required option `{key}` is missing.\")\n\n    return self._validate_options()\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/miso_historical_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.read_batch","title":"<code>read_batch()</code>","text":"<p>Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data. Returns:      Final Spark DataFrame converted from Pandas DataFrame post-execution.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def read_batch(self) -&gt; DataFrame:\n\"\"\"\n    Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data.\n    Returns:\n         Final Spark DataFrame converted from Pandas DataFrame post-execution.\n\n    \"\"\"\n\n    try:\n        self.pre_read_validation()\n        pdf = self._get_data()\n        pdf = _prepare_pandas_to_convert_to_spark(pdf)\n\n        # The below is to fix the compatibility issues between Pandas 2.0 and PySpark.\n        pd.DataFrame.iteritems = pd.DataFrame.items\n        df = self.spark.createDataFrame(data=pdf, schema=self.spark_schema)\n        return df\n\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/miso_historical_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.read_stream","title":"<code>read_stream()</code>","text":"<p>By default, the streaming operation is not supported but child classes can override if ISO supports streaming.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Final Spark DataFrame after all the processing.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def read_stream(self) -&gt; DataFrame:\n\"\"\"\n    By default, the streaming operation is not supported but child classes can override if ISO supports streaming.\n\n    Returns:\n         Final Spark DataFrame after all the processing.\n\n    \"\"\"\n\n    raise NotImplementedError(\n        f\"{self.__class__.__name__} connector doesn't support stream operation.\"\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/pjm_daily_load_iso/","title":"PJM Daily Load","text":""},{"location":"sdk/code-reference/pipelines/sources/spark/iso/pjm_daily_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.pjm_daily_load_iso.PJMDailyLoadISOSource","title":"<code>PJMDailyLoadISOSource</code>","text":"<p>             Bases: <code>BaseISOSource</code></p> <p>The PJM Daily Load ISO Source is used to read daily load data from PJM API. It supports both Actual and Forecast data. Actual will return 1 day, Forecast will return 7 days</p> <p>API:           https://api.pjm.com/api/v1/  (must be a valid apy key from PJM)</p> <p>Actual doc:    https://dataminer2.pjm.com/feed/ops_sum_prev_period/definition</p> <p>Forecast doc:  https://dataminer2.pjm.com/feed/load_frcstd_7_day/definition</p>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/pjm_daily_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.pjm_daily_load_iso.PJMDailyLoadISOSource--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.sources import PJMDailyLoadISOSource\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\npjm_source = PJMDailyLoadISOSource(\n    spark=spark,\n    options={\n        \"api_key\": \"{api_key}\",\n        \"load_type\": \"actual\"\n    }\n)\n\npjm_source.read_batch()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance</p> required <code>options</code> <code>dict</code> <p>A dictionary of ISO Source specific configurations (See Attributes table below)</p> required <p>Attributes:</p> Name Type Description <code>api_key</code> <code>str</code> <p>Must be a valid key from PJM, see api url</p> <code>load_type</code> <code>str</code> <p>Must be one of <code>actual</code> or <code>forecast</code></p> <p>Please check the BaseISOSource for available methods.</p> BaseISOSource Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/pjm_daily_load_iso.py</code> <pre><code>class PJMDailyLoadISOSource(BaseISOSource):\n\"\"\"\n    The PJM Daily Load ISO Source is used to read daily load data from PJM API. It supports both Actual and Forecast data. Actual will return 1 day, Forecast will return 7 days\n\n    API:           &lt;a href=\"https://api.pjm.com/api/v1/\"&gt;https://api.pjm.com/api/v1/&lt;/a&gt;  (must be a valid apy key from PJM)\n\n    Actual doc:    &lt;a href=\"https://dataminer2.pjm.com/feed/ops_sum_prev_period/definition\"&gt;https://dataminer2.pjm.com/feed/ops_sum_prev_period/definition&lt;/a&gt;\n\n    Forecast doc:  &lt;a href=\"https://dataminer2.pjm.com/feed/load_frcstd_7_day/definition\"&gt;https://dataminer2.pjm.com/feed/load_frcstd_7_day/definition&lt;/a&gt;\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.sources import PJMDailyLoadISOSource\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    pjm_source = PJMDailyLoadISOSource(\n        spark=spark,\n        options={\n            \"api_key\": \"{api_key}\",\n            \"load_type\": \"actual\"\n        }\n    )\n\n    pjm_source.read_batch()\n    ```\n\n    Parameters:\n        spark (SparkSession): Spark Session instance\n        options (dict): A dictionary of ISO Source specific configurations (See Attributes table below)\n\n    Attributes:\n        api_key (str): Must be a valid key from PJM, see api url\n        load_type (str): Must be one of `actual` or `forecast`\n\n    Please check the BaseISOSource for available methods.\n\n    BaseISOSource:\n        ::: src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso\n    \"\"\"\n\n    spark: SparkSession\n    spark_schema = PJM_SCHEMA\n    options: dict\n    iso_url: str = \"https://api.pjm.com/api/v1/\"\n    query_datetime_format: str = \"%Y-%m-%d %H:%M\"\n    required_options = [\"api_key\", \"load_type\"]\n    default_query_timezone = \"US/Eastern\"\n\n    def __init__(self, spark: SparkSession, options: dict) -&gt; None:\n        super().__init__(spark, options)\n        self.spark: SparkSession = spark\n        self.options: dict = options\n        self.load_type: str = self.options.get(\"load_type\", \"\").strip()\n        self.api_key: str = self.options.get(\"api_key\", \"\").strip()\n        self.days: int = self.options.get(\"days\", 7)\n\n    def _fetch_from_url(self, url_suffix: str, start_date: str, end_date: str) -&gt; bytes:\n\"\"\"\n        Gets data from external ISO API.\n\n        Args:\n            url_suffix: String to be used as suffix to iso url.\n\n        Returns:\n            Raw content of the data received.\n        \"\"\"\n\n        url = f\"{self.iso_url}{url_suffix}\"\n        headers = {\"Ocp-Apim-Subscription-Key\": self.api_key}\n        logging.info(\n            f\"Requesting URL - {url}, start_date={start_date}, end_date={end_date}, load_type={self.load_type}\"\n        )\n        load_key = (\n            \"datetime_beginning_ept\"\n            if self.load_type != \"forecast\"\n            else \"forecast_datetime_beginning_ept\"\n        )\n        feed = (\n            \"ops_sum_prev_period\"\n            if self.load_type != \"forecast\"\n            else \"load_frcstd_7_day\"\n        )\n        query = {\n            \"startRow\": \"1\",\n            load_key: f\"{start_date}to{end_date}\",\n            \"format\": \"csv\",\n            \"download\": \"true\",\n        }\n        query_s = \"&amp;\".join([\"=\".join([k, v]) for k, v in query.items()])\n        new_url = f\"{url}{feed}?{query_s}\"\n        response = requests.get(new_url, headers=headers)\n        code = response.status_code\n\n        if code != 200:\n            raise requests.HTTPError(\n                f\"Unable to access URL `{url}`.\"\n                f\" Received status code {code} with message {response.content}\"\n            )\n        return response.content\n\n    def _pull_data(self) -&gt; pd.DataFrame:\n\"\"\"\n        Pulls data from the PJM API and parses the return.\n\n        Returns:\n            Raw form of data.\n        \"\"\"\n        start_date = self.current_date - timedelta(days=1)\n        start_date = start_date.replace(hour=0, minute=0)\n        end_date = (start_date + timedelta(days=self.days)).replace(hour=23)\n        start_date_str = start_date.strftime(self.query_datetime_format)\n        end_date_str = end_date.strftime(self.query_datetime_format)\n        df = pd.read_csv(\n            BytesIO(self._fetch_from_url(\"\", start_date_str, end_date_str))\n        )\n\n        return df\n\n    def _prepare_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n        Creates a new date time column and removes null values. Renames columns\n\n        Args:\n            df: Raw form of data received from the API.\n\n        Returns:\n            Data after basic transformations.\n\n        \"\"\"\n\n        if self.load_type == \"forecast\":\n            df = df.rename(\n                columns={\n                    \"forecast_datetime_beginning_utc\": \"start_time\",\n                    \"forecast_area\": \"zone\",\n                    \"forecast_datetime_ending_utc\": \"end_time\",\n                    \"forecast_load_mw\": \"load\",\n                }\n            )\n        else:\n            df = df.rename(\n                columns={\n                    \"datetime_beginning_utc\": \"start_time\",\n                    \"area\": \"zone\",\n                    \"datetime_ending_utc\": \"end_time\",\n                    \"actual_load\": \"load\",\n                }\n            )\n\n        df = df[[\"start_time\", \"end_time\", \"zone\", \"load\"]]\n        df = df.replace({np.nan: None, \"\": None})\n\n        date_cols = [\"start_time\", \"end_time\"]\n        for col in date_cols:\n            df[col] = pd.to_datetime(df[col], format=\"%m/%d/%Y %I:%M:%S %p\")\n\n        df[\"load\"] = df[\"load\"].astype(float)\n        df = df.replace({np.nan: None, \"\": None})\n        df.columns = list(map(lambda x: x.upper(), df.columns))\n\n        rename_cols = {\n            \"START_TIME\": \"StartTime\",\n            \"END_TIME\": \"EndTime\",\n            \"ZONE\": \"Zone\",\n            \"LOAD\": \"Load\",\n        }\n\n        df = df.rename(columns=rename_cols)\n\n        df.reset_index(inplace=True, drop=True)\n\n        return df\n\n    def _validate_options(self) -&gt; bool:\n\"\"\"\n        Validates the following options:\n            - `load_type` must be valid.\n\n        Returns:\n            True if all looks good otherwise raises Exception.\n        \"\"\"\n\n        valid_load_types = [\"actual\", \"forecast\"]\n\n        if self.load_type not in valid_load_types:\n            raise ValueError(\n                f\"Invalid load_type `{self.load_type}` given. Supported values are {valid_load_types}.\"\n            )\n\n        return True\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/pjm_daily_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource","title":"<code>BaseISOSource</code>","text":"<p>             Bases: <code>SourceInterface</code></p> <p>Base class for all the ISO Sources. It provides common functionality and helps in reducing the code redundancy.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance</p> required <code>options</code> <code>dict</code> <p>A dictionary of ISO Source specific configurations</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>class BaseISOSource(SourceInterface):\n\"\"\"\n    Base class for all the ISO Sources. It provides common functionality and helps in reducing the code redundancy.\n\n    Parameters:\n        spark (SparkSession): Spark Session instance\n        options (dict): A dictionary of ISO Source specific configurations\n    \"\"\"\n\n    spark: SparkSession\n    options: dict\n    iso_url: str = \"https://\"\n    query_datetime_format: str = \"%Y%m%d\"\n    required_options: list = []\n    spark_schema = StructType([StructField(\"id\", IntegerType(), True)])\n    default_query_timezone: str = \"UTC\"\n\n    def __init__(self, spark: SparkSession, options: dict) -&gt; None:\n        self.spark = spark\n        self.options = options\n        self.query_timezone = pytz.timezone(\n            self.options.get(\"query_timezone\", self.default_query_timezone)\n        )\n        self.current_date = datetime.now(timezone.utc).astimezone(self.query_timezone)\n\n    def _fetch_from_url(self, url_suffix: str) -&gt; bytes:\n\"\"\"\n        Gets data from external ISO API.\n\n        Args:\n            url_suffix: String to be used as suffix to iso url.\n\n        Returns:\n            Raw content of the data received.\n\n        \"\"\"\n        url = f\"{self.iso_url}{url_suffix}\"\n        logging.info(f\"Requesting URL - {url}\")\n\n        response = requests.get(url)\n        code = response.status_code\n\n        if code != 200:\n            raise HTTPError(\n                f\"Unable to access URL `{url}`.\"\n                f\" Received status code {code} with message {response.content}\"\n            )\n\n        return response.content\n\n    def _get_localized_datetime(self, datetime_str: str) -&gt; datetime:\n\"\"\"\n        Converts string datetime into Python datetime object with configured format and timezone.\n        Args:\n            datetime_str: String to be converted into datetime.\n\n        Returns: Timezone aware datetime object.\n\n        \"\"\"\n        parsed_dt = datetime.strptime(datetime_str, self.query_datetime_format)\n        parsed_dt = parsed_dt.replace(tzinfo=self.query_timezone)\n        return parsed_dt\n\n    def _pull_data(self) -&gt; pd.DataFrame:\n\"\"\"\n        Hits the fetch_from_url method with certain parameters to get raw data from API.\n\n        All the children ISO classes must override this method and call the fetch_url method\n        in it.\n\n        Returns:\n             Raw DataFrame from API.\n        \"\"\"\n\n        return pd.read_csv(BytesIO(self._fetch_from_url(\"\")))\n\n    def _prepare_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n        Performs all the basic transformations to prepare data for further processing.\n        All the children ISO classes must override this method.\n\n        Args:\n            df: Raw DataFrame, received from the API.\n\n        Returns:\n             Modified DataFrame, ready for basic use.\n\n        \"\"\"\n        return df\n\n    def _sanitize_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n        Another data transformation helper method to be called after prepare data.\n        Used for advance data processing such as cleaning, filtering, restructuring.\n        All the children ISO classes must override this method if there is any post-processing required.\n\n        Args:\n            df: Initial modified version of DataFrame, received after preparing the data.\n\n        Returns:\n             Final version of data after all the fixes and modifications.\n\n        \"\"\"\n        return df\n\n    def _get_data(self) -&gt; pd.DataFrame:\n\"\"\"\n        Entrypoint method to return the final version of DataFrame.\n\n        Returns:\n            Modified form of data for specific use case.\n\n        \"\"\"\n        df = self._pull_data()\n        df = self._prepare_data(df)\n        df = self._sanitize_data(df)\n\n        # Reorder columns to keep the data consistent\n        df = df[self.spark_schema.names]\n\n        return df\n\n    @staticmethod\n    def system_type():\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def _validate_options(self) -&gt; bool:\n\"\"\"\n        Performs all the options checks. Raises exception in case of any invalid value.\n        Returns:\n             True if all checks are passed.\n\n        \"\"\"\n        return True\n\n    def pre_read_validation(self) -&gt; bool:\n\"\"\"\n        Ensures all the required options are provided and performs other validations.\n        Returns:\n             True if all checks are passed.\n\n        \"\"\"\n        for key in self.required_options:\n            if key not in self.options:\n                raise ValueError(f\"Required option `{key}` is missing.\")\n\n        return self._validate_options()\n\n    def post_read_validation(self) -&gt; bool:\n        return True\n\n    def read_batch(self) -&gt; DataFrame:\n\"\"\"\n        Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data.\n        Returns:\n             Final Spark DataFrame converted from Pandas DataFrame post-execution.\n\n        \"\"\"\n\n        try:\n            self.pre_read_validation()\n            pdf = self._get_data()\n            pdf = _prepare_pandas_to_convert_to_spark(pdf)\n\n            # The below is to fix the compatibility issues between Pandas 2.0 and PySpark.\n            pd.DataFrame.iteritems = pd.DataFrame.items\n            df = self.spark.createDataFrame(data=pdf, schema=self.spark_schema)\n            return df\n\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def read_stream(self) -&gt; DataFrame:\n\"\"\"\n        By default, the streaming operation is not supported but child classes can override if ISO supports streaming.\n\n        Returns:\n             Final Spark DataFrame after all the processing.\n\n        \"\"\"\n\n        raise NotImplementedError(\n            f\"{self.__class__.__name__} connector doesn't support stream operation.\"\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/pjm_daily_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.pre_read_validation","title":"<code>pre_read_validation()</code>","text":"<p>Ensures all the required options are provided and performs other validations. Returns:      True if all checks are passed.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def pre_read_validation(self) -&gt; bool:\n\"\"\"\n    Ensures all the required options are provided and performs other validations.\n    Returns:\n         True if all checks are passed.\n\n    \"\"\"\n    for key in self.required_options:\n        if key not in self.options:\n            raise ValueError(f\"Required option `{key}` is missing.\")\n\n    return self._validate_options()\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/pjm_daily_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.read_batch","title":"<code>read_batch()</code>","text":"<p>Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data. Returns:      Final Spark DataFrame converted from Pandas DataFrame post-execution.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def read_batch(self) -&gt; DataFrame:\n\"\"\"\n    Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data.\n    Returns:\n         Final Spark DataFrame converted from Pandas DataFrame post-execution.\n\n    \"\"\"\n\n    try:\n        self.pre_read_validation()\n        pdf = self._get_data()\n        pdf = _prepare_pandas_to_convert_to_spark(pdf)\n\n        # The below is to fix the compatibility issues between Pandas 2.0 and PySpark.\n        pd.DataFrame.iteritems = pd.DataFrame.items\n        df = self.spark.createDataFrame(data=pdf, schema=self.spark_schema)\n        return df\n\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/pjm_daily_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.read_stream","title":"<code>read_stream()</code>","text":"<p>By default, the streaming operation is not supported but child classes can override if ISO supports streaming.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Final Spark DataFrame after all the processing.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def read_stream(self) -&gt; DataFrame:\n\"\"\"\n    By default, the streaming operation is not supported but child classes can override if ISO supports streaming.\n\n    Returns:\n         Final Spark DataFrame after all the processing.\n\n    \"\"\"\n\n    raise NotImplementedError(\n        f\"{self.__class__.__name__} connector doesn't support stream operation.\"\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/pjm_historical_load_iso/","title":"PJM Historical Load","text":""},{"location":"sdk/code-reference/pipelines/sources/spark/iso/pjm_historical_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.pjm_historical_load_iso.PJMHistoricalLoadISOSource","title":"<code>PJMHistoricalLoadISOSource</code>","text":"<p>             Bases: <code>PJMDailyLoadISOSource</code></p> <p>The PJM Historical Load ISO Source is used to read historical load data from PJM API.</p> <p>API:               https://api.pjm.com/api/v1/  (must be a valid apy key from PJM)</p> <p>Historical doc:    https://dataminer2.pjm.com/feed/ops_sum_prev_period/definition</p> <p>Historical is the same PJM endpoint as Actual, but is called repeatedly within a range established by the start_date &amp; end_date attributes</p>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/pjm_historical_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.pjm_historical_load_iso.PJMHistoricalLoadISOSource--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.sources import PJMHistoricalLoadISOSource\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\npjm_source = PJMHistoricalLoadISOSource(\n    spark=spark,\n    options={\n        \"api_key\": \"{api_key}\",\n        \"start_date\": \"20230510\",\n        \"end_date\": \"20230520\",\n    }\n)\n\npjm_source.read_batch()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance</p> required <code>options</code> <code>dict</code> <p>A dictionary of ISO Source specific configurations (See Attributes table below)</p> required <p>Attributes:</p> Name Type Description <code>api_key</code> <code>str</code> <p>Must be a valid key from PJM, see PJM documentation</p> <code>start_date</code> <code>str</code> <p>Must be in <code>YYYY-MM-DD</code> format.</p> <code>end_date</code> <code>str</code> <p>Must be in <code>YYYY-MM-DD</code> format.</p> <code>query_batch_days</code> <code>int</code> <p>(optional) Number of days must be &lt; 160 as per PJM &amp; is defaulted to <code>120</code></p> <code>sleep_duration</code> <code>int</code> <p>(optional) Number of seconds to sleep between request, defaulted to <code>5</code> seconds, used to manage requests to PJM endpoint</p> <code>request_count</code> <code>int</code> <p>(optional) Number of requests made to PJM endpoint before sleep_duration, currently defaulted to <code>1</code></p> <p>Please check the BaseISOSource for available methods.</p> BaseISOSource Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/pjm_historical_load_iso.py</code> <pre><code>class PJMHistoricalLoadISOSource(PJMDailyLoadISOSource):\n\"\"\"\n    The PJM Historical Load ISO Source is used to read historical load data from PJM API.\n\n    API:               &lt;a href=\"https://api.pjm.com/api/v1/\"&gt;https://api.pjm.com/api/v1/&lt;/a&gt;  (must be a valid apy key from PJM)\n\n    Historical doc:    &lt;a href=\"https://dataminer2.pjm.com/feed/ops_sum_prev_period/definition\"&gt;https://dataminer2.pjm.com/feed/ops_sum_prev_period/definition&lt;/a&gt;\n\n    Historical is the same PJM endpoint as Actual, but is called repeatedly within a range established by the start_date &amp; end_date attributes\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.sources import PJMHistoricalLoadISOSource\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    pjm_source = PJMHistoricalLoadISOSource(\n        spark=spark,\n        options={\n            \"api_key\": \"{api_key}\",\n            \"start_date\": \"20230510\",\n            \"end_date\": \"20230520\",\n        }\n    )\n\n    pjm_source.read_batch()\n    ```\n\n    Parameters:\n        spark (SparkSession): Spark Session instance\n        options (dict): A dictionary of ISO Source specific configurations (See Attributes table below)\n\n    Attributes:\n        api_key (str): Must be a valid key from PJM, see PJM documentation\n        start_date (str): Must be in `YYYY-MM-DD` format.\n        end_date (str): Must be in `YYYY-MM-DD` format.\n\n        query_batch_days (int): (optional) Number of days must be &lt; 160 as per PJM &amp; is defaulted to `120`\n        sleep_duration (int): (optional) Number of seconds to sleep between request, defaulted to `5` seconds, used to manage requests to PJM endpoint\n        request_count (int): (optional) Number of requests made to PJM endpoint before sleep_duration, currently defaulted to `1`\n\n    Please check the BaseISOSource for available methods.\n\n    BaseISOSource:\n        ::: src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso\"\"\"\n\n    spark: SparkSession\n    options: dict\n    required_options = [\"api_key\", \"start_date\", \"end_date\"]\n\n    def __init__(self, spark: SparkSession, options: dict) -&gt; None:\n        super().__init__(spark, options)\n        self.spark: SparkSession = spark\n        self.options: dict = options\n        self.api_key: str = self.options.get(\"api_key\", \"\").strip()\n        self.start_date: str = self.options.get(\"start_date\", \"\")\n        self.end_date: str = self.options.get(\"end_date\", \"\")\n        self.query_batch_days: int = self.options.get(\"query_batch_days\", 120)\n        self.sleep_duration: int = self.options.get(\"sleep_duration\", 5)\n        self.request_count: int = self.options.get(\"request_count\", 1)\n        self.load_type: str = \"actual\"\n        self.user_datetime_format = \"%Y-%m-%d\"\n\n    def _pull_data(self) -&gt; pd.DataFrame:\n\"\"\"\n        Pulls data from the PJM API and parses the return including date ranges.\n\n        Returns:\n            Raw form of data.\n        \"\"\"\n\n        logging.info(\n            f\"Historical load requested from {self.start_date} to {self.end_date}\"\n        )\n        start_date = datetime.strptime(self.start_date, self.user_datetime_format)\n        end_date = datetime.strptime(self.end_date, self.user_datetime_format).replace(\n            hour=23\n        )\n\n        days_diff = (end_date - start_date).days\n        logging.info(f\"Expected hours for a single zone = {(days_diff + 1) * 24}\")\n        generated_days_ranges = []\n        dates = pd.date_range(\n            start_date, end_date, freq=pd.DateOffset(days=self.query_batch_days)\n        )\n\n        for date in dates:\n            py_date = date.to_pydatetime()\n            date_last = (py_date + timedelta(days=self.query_batch_days - 1)).replace(\n                hour=23\n            )\n            date_last = min(date_last, end_date)\n            generated_days_ranges.append((py_date, date_last))\n\n        logging.info(\n            f\"Generated date ranges for batch days {self.query_batch_days} are {generated_days_ranges}\"\n        )\n\n        # Collect all historical data on yearly basis.\n        dfs = []\n        for idx, date_range in enumerate(generated_days_ranges):\n            start_date_str = date_range[0].strftime(self.query_datetime_format)\n            end_date_str = date_range[1].strftime(self.query_datetime_format)\n\n            df = pd.read_csv(\n                BytesIO(self._fetch_from_url(\"\", start_date_str, end_date_str))\n            )\n            dfs.append(df)\n\n            if idx &gt; 0 and idx % self.request_count == 0:\n                logging.info(f\"Going to sleep for {self.sleep_duration} seconds\")\n                time.sleep(self.sleep_duration)\n\n        df = pd.concat(dfs, sort=False)\n        df = df.reset_index(drop=True)\n        return df\n\n    def _validate_options(self) -&gt; bool:\n\"\"\"\n        Validates all parameters including the following examples:\n            - `start_date` &amp; `end_data` must be in the correct format.\n            - `start_date` must be behind `end_data`.\n            - `start_date` must not be in the future (UTC).\n\n        Returns:\n            True if all looks good otherwise raises Exception.\n\n        \"\"\"\n\n        try:\n            start_date = datetime.strptime(self.start_date, self.user_datetime_format)\n        except ValueError:\n            raise ValueError(\n                f\"Unable to parse Start date. Please specify in {self.user_datetime_format} format.\"\n            )\n\n        try:\n            end_date = datetime.strptime(self.end_date, self.user_datetime_format)\n        except ValueError:\n            raise ValueError(\n                f\"Unable to parse End date. Please specify in {self.user_datetime_format} format.\"\n            )\n\n        if start_date &gt; datetime.utcnow() - timedelta(days=1):\n            raise ValueError(\"Start date can't be in future.\")\n\n        if start_date &gt; end_date:\n            raise ValueError(\"Start date can't be ahead of End date.\")\n\n        if end_date &gt; datetime.utcnow() - timedelta(days=1):\n            raise ValueError(\"End date can't be in future.\")\n\n        if self.sleep_duration &lt; 0:\n            raise ValueError(\"Sleep duration can't be negative.\")\n\n        if self.request_count &lt; 0:\n            raise ValueError(\"Request count can't be negative.\")\n\n        if self.query_batch_days &lt; 0:\n            raise ValueError(\"Query batch days count can't be negative.\")\n\n        return True\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/pjm_historical_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource","title":"<code>BaseISOSource</code>","text":"<p>             Bases: <code>SourceInterface</code></p> <p>Base class for all the ISO Sources. It provides common functionality and helps in reducing the code redundancy.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance</p> required <code>options</code> <code>dict</code> <p>A dictionary of ISO Source specific configurations</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>class BaseISOSource(SourceInterface):\n\"\"\"\n    Base class for all the ISO Sources. It provides common functionality and helps in reducing the code redundancy.\n\n    Parameters:\n        spark (SparkSession): Spark Session instance\n        options (dict): A dictionary of ISO Source specific configurations\n    \"\"\"\n\n    spark: SparkSession\n    options: dict\n    iso_url: str = \"https://\"\n    query_datetime_format: str = \"%Y%m%d\"\n    required_options: list = []\n    spark_schema = StructType([StructField(\"id\", IntegerType(), True)])\n    default_query_timezone: str = \"UTC\"\n\n    def __init__(self, spark: SparkSession, options: dict) -&gt; None:\n        self.spark = spark\n        self.options = options\n        self.query_timezone = pytz.timezone(\n            self.options.get(\"query_timezone\", self.default_query_timezone)\n        )\n        self.current_date = datetime.now(timezone.utc).astimezone(self.query_timezone)\n\n    def _fetch_from_url(self, url_suffix: str) -&gt; bytes:\n\"\"\"\n        Gets data from external ISO API.\n\n        Args:\n            url_suffix: String to be used as suffix to iso url.\n\n        Returns:\n            Raw content of the data received.\n\n        \"\"\"\n        url = f\"{self.iso_url}{url_suffix}\"\n        logging.info(f\"Requesting URL - {url}\")\n\n        response = requests.get(url)\n        code = response.status_code\n\n        if code != 200:\n            raise HTTPError(\n                f\"Unable to access URL `{url}`.\"\n                f\" Received status code {code} with message {response.content}\"\n            )\n\n        return response.content\n\n    def _get_localized_datetime(self, datetime_str: str) -&gt; datetime:\n\"\"\"\n        Converts string datetime into Python datetime object with configured format and timezone.\n        Args:\n            datetime_str: String to be converted into datetime.\n\n        Returns: Timezone aware datetime object.\n\n        \"\"\"\n        parsed_dt = datetime.strptime(datetime_str, self.query_datetime_format)\n        parsed_dt = parsed_dt.replace(tzinfo=self.query_timezone)\n        return parsed_dt\n\n    def _pull_data(self) -&gt; pd.DataFrame:\n\"\"\"\n        Hits the fetch_from_url method with certain parameters to get raw data from API.\n\n        All the children ISO classes must override this method and call the fetch_url method\n        in it.\n\n        Returns:\n             Raw DataFrame from API.\n        \"\"\"\n\n        return pd.read_csv(BytesIO(self._fetch_from_url(\"\")))\n\n    def _prepare_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n        Performs all the basic transformations to prepare data for further processing.\n        All the children ISO classes must override this method.\n\n        Args:\n            df: Raw DataFrame, received from the API.\n\n        Returns:\n             Modified DataFrame, ready for basic use.\n\n        \"\"\"\n        return df\n\n    def _sanitize_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n        Another data transformation helper method to be called after prepare data.\n        Used for advance data processing such as cleaning, filtering, restructuring.\n        All the children ISO classes must override this method if there is any post-processing required.\n\n        Args:\n            df: Initial modified version of DataFrame, received after preparing the data.\n\n        Returns:\n             Final version of data after all the fixes and modifications.\n\n        \"\"\"\n        return df\n\n    def _get_data(self) -&gt; pd.DataFrame:\n\"\"\"\n        Entrypoint method to return the final version of DataFrame.\n\n        Returns:\n            Modified form of data for specific use case.\n\n        \"\"\"\n        df = self._pull_data()\n        df = self._prepare_data(df)\n        df = self._sanitize_data(df)\n\n        # Reorder columns to keep the data consistent\n        df = df[self.spark_schema.names]\n\n        return df\n\n    @staticmethod\n    def system_type():\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def _validate_options(self) -&gt; bool:\n\"\"\"\n        Performs all the options checks. Raises exception in case of any invalid value.\n        Returns:\n             True if all checks are passed.\n\n        \"\"\"\n        return True\n\n    def pre_read_validation(self) -&gt; bool:\n\"\"\"\n        Ensures all the required options are provided and performs other validations.\n        Returns:\n             True if all checks are passed.\n\n        \"\"\"\n        for key in self.required_options:\n            if key not in self.options:\n                raise ValueError(f\"Required option `{key}` is missing.\")\n\n        return self._validate_options()\n\n    def post_read_validation(self) -&gt; bool:\n        return True\n\n    def read_batch(self) -&gt; DataFrame:\n\"\"\"\n        Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data.\n        Returns:\n             Final Spark DataFrame converted from Pandas DataFrame post-execution.\n\n        \"\"\"\n\n        try:\n            self.pre_read_validation()\n            pdf = self._get_data()\n            pdf = _prepare_pandas_to_convert_to_spark(pdf)\n\n            # The below is to fix the compatibility issues between Pandas 2.0 and PySpark.\n            pd.DataFrame.iteritems = pd.DataFrame.items\n            df = self.spark.createDataFrame(data=pdf, schema=self.spark_schema)\n            return df\n\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def read_stream(self) -&gt; DataFrame:\n\"\"\"\n        By default, the streaming operation is not supported but child classes can override if ISO supports streaming.\n\n        Returns:\n             Final Spark DataFrame after all the processing.\n\n        \"\"\"\n\n        raise NotImplementedError(\n            f\"{self.__class__.__name__} connector doesn't support stream operation.\"\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/pjm_historical_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.pre_read_validation","title":"<code>pre_read_validation()</code>","text":"<p>Ensures all the required options are provided and performs other validations. Returns:      True if all checks are passed.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def pre_read_validation(self) -&gt; bool:\n\"\"\"\n    Ensures all the required options are provided and performs other validations.\n    Returns:\n         True if all checks are passed.\n\n    \"\"\"\n    for key in self.required_options:\n        if key not in self.options:\n            raise ValueError(f\"Required option `{key}` is missing.\")\n\n    return self._validate_options()\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/pjm_historical_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.read_batch","title":"<code>read_batch()</code>","text":"<p>Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data. Returns:      Final Spark DataFrame converted from Pandas DataFrame post-execution.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def read_batch(self) -&gt; DataFrame:\n\"\"\"\n    Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data.\n    Returns:\n         Final Spark DataFrame converted from Pandas DataFrame post-execution.\n\n    \"\"\"\n\n    try:\n        self.pre_read_validation()\n        pdf = self._get_data()\n        pdf = _prepare_pandas_to_convert_to_spark(pdf)\n\n        # The below is to fix the compatibility issues between Pandas 2.0 and PySpark.\n        pd.DataFrame.iteritems = pd.DataFrame.items\n        df = self.spark.createDataFrame(data=pdf, schema=self.spark_schema)\n        return df\n\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/pjm_historical_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.read_stream","title":"<code>read_stream()</code>","text":"<p>By default, the streaming operation is not supported but child classes can override if ISO supports streaming.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Final Spark DataFrame after all the processing.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def read_stream(self) -&gt; DataFrame:\n\"\"\"\n    By default, the streaming operation is not supported but child classes can override if ISO supports streaming.\n\n    Returns:\n         Final Spark DataFrame after all the processing.\n\n    \"\"\"\n\n    raise NotImplementedError(\n        f\"{self.__class__.__name__} connector doesn't support stream operation.\"\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/the_weather_company/base_weather/","title":"Base Weather","text":""},{"location":"sdk/code-reference/pipelines/sources/spark/the_weather_company/base_weather/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.the_weather_company.base_weather.SparkWeatherCompanyBaseWeatherSource","title":"<code>SparkWeatherCompanyBaseWeatherSource</code>","text":"<p>             Bases: <code>BaseISOSource</code></p> <p>Base class for all the Weather related sources. Provides common functionality.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance</p> required <code>options</code> <code>dict</code> <p>A dictionary of Weather Source specific configurations.</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/the_weather_company/base_weather.py</code> <pre><code>class SparkWeatherCompanyBaseWeatherSource(BaseISOSource):\n\"\"\"\n    Base class for all the Weather related sources. Provides common functionality.\n\n    Parameters:\n        spark (SparkSession): Spark Session instance\n        options (dict): A dictionary of Weather Source specific configurations.\n\n    \"\"\"\n\n    spark: SparkSession\n    options: dict\n    weather_url: str = \"https://\"\n    api_params: dict = {}\n\n    def __init__(self, spark: SparkSession, options: dict) -&gt; None:\n        super(SparkWeatherCompanyBaseWeatherSource, self).__init__(spark, options)\n        self.spark = spark\n        self.options = options\n\n    def _get_api_params(self) -&gt; dict:\n        return self.api_params\n\n    def _fetch_weather_from_url(self, url_suffix: str, params: dict) -&gt; bytes:\n\"\"\"\n        Gets data from external Weather Forecast API.\n\n        Args:\n            url_suffix: String to be used as suffix to weather url.\n\n        Returns:\n            Raw content of the data received.\n\n        \"\"\"\n        url = f\"{self.weather_url}{url_suffix}\"\n        logging.info(f\"Requesting URL - `{url}` with params - {params}\")\n\n        response = requests.get(url, params)\n        code = response.status_code\n\n        if code != 200:\n            raise HTTPError(\n                f\"Unable to access URL `{url}`.\"\n                f\" Received status code {code} with message {response.content}\"\n            )\n        return response.content\n\n    def _fetch_from_url(self, url_suffix: str) -&gt; bytes:\n        return self._fetch_weather_from_url(url_suffix, self._get_api_params())\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/the_weather_company/weather_forecast_api_v1/","title":"Weather Forecast API V1","text":""},{"location":"sdk/code-reference/pipelines/sources/spark/the_weather_company/weather_forecast_api_v1/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.the_weather_company.weather_forecast_api_v1.SparkWeatherCompanyForecastAPIV1Source","title":"<code>SparkWeatherCompanyForecastAPIV1Source</code>","text":"<p>             Bases: <code>SparkWeatherCompanyBaseWeatherSource</code></p> <p>The Weather Forecast API V1 Source is used to read 15 days forecast from the Weather API.</p> <p>URL:  https://api.weather.com/v1/geocode/32.3667/-95.4/forecast/hourly/360hour.json</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance</p> required <code>options</code> <code>dict</code> <p>A dictionary of ISO Source specific configurations (See Attributes table below).</p> required <p>Attributes:</p> Name Type Description <code>lat</code> <code>str</code> <p>Latitude of the Weather Station.</p> <code>lon</code> <code>str</code> <p>Longitude of the Weather Station.</p> <code>api_key</code> <code>str</code> <p>Weather API key.</p> <code>language</code> <code>str</code> <p>API response language. Defaults to <code>en-US</code>.</p> <code>units</code> <code>str</code> <p>Unit of measurements. Defaults to <code>e</code>.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/the_weather_company/weather_forecast_api_v1.py</code> <pre><code>class SparkWeatherCompanyForecastAPIV1Source(SparkWeatherCompanyBaseWeatherSource):\n\"\"\"\n    The Weather Forecast API V1 Source is used to read 15 days forecast from the Weather API.\n\n    URL: &lt;a href=\"https://api.weather.com/v1/geocode/32.3667/-95.4/forecast/hourly/360hour.json\"&gt;\n    https://api.weather.com/v1/geocode/32.3667/-95.4/forecast/hourly/360hour.json&lt;/a&gt;\n\n    Parameters:\n        spark (SparkSession): Spark Session instance\n        options (dict): A dictionary of ISO Source specific configurations (See Attributes table below).\n\n    Attributes:\n        lat (str): Latitude of the Weather Station.\n        lon (str): Longitude of the Weather Station.\n        api_key (str): Weather API key.\n        language (str): API response language. Defaults to `en-US`.\n        units (str): Unit of measurements. Defaults to `e`.\n    \"\"\"\n\n    spark: SparkSession\n    spark_schema = WEATHER_FORECAST_SCHEMA\n    options: dict\n    weather_url: str = \"https://api.weather.com/v1/geocode/\"\n    required_options = [\"lat\", \"lon\", \"api_key\"]\n\n    def __init__(self, spark: SparkSession, options: dict) -&gt; None:\n        super(SparkWeatherCompanyForecastAPIV1Source, self).__init__(spark, options)\n        self.spark = spark\n        self.options = options\n        self.lat = self.options.get(\"lat\", \"\").strip()\n        self.lon = self.options.get(\"lon\", \"\").strip()\n        self.api_key = self.options.get(\"api_key\", \"\").strip()\n        self.language = self.options.get(\"language\", \"en-US\").strip()\n        self.units = self.options.get(\"units\", \"e\").strip()\n\n    def _prepare_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n        Prepares weather data for the use.\n\n        Args:\n            df: Data received after preparation.\n\n        Returns:\n            Final data after all the transformations.\n\n        \"\"\"\n\n        rename_cols = {\n            \"latitude\": \"Latitude\",\n            \"longitude\": \"Longitude\",\n            \"class\": \"Class\",\n            \"expire_time_gmt\": \"ExpireTimeGmt\",\n            \"fcst_valid\": \"FcstValid\",\n            \"fcst_valid_local\": \"FcstValidLocal\",\n            \"num\": \"Num\",\n            \"day_ind\": \"DayInd\",\n            \"temp\": \"Temp\",\n            \"dewpt\": \"Dewpt\",\n            \"hi\": \"Hi\",\n            \"wc\": \"Wc\",\n            \"feels_like\": \"FeelsLike\",\n            \"icon_extd\": \"IconExtd\",\n            \"wxman\": \"Wxman\",\n            \"icon_code\": \"IconCode\",\n            \"dow\": \"Dow\",\n            \"phrase_12char\": \"Phrase12Char\",\n            \"phrase_22char\": \"Phrase22Char\",\n            \"phrase_32char\": \"Phrase32Char\",\n            \"subphrase_pt1\": \"SubphrasePt1\",\n            \"subphrase_pt2\": \"SubphrasePt2\",\n            \"subphrase_pt3\": \"SubphrasePt3\",\n            \"pop\": \"Pop\",\n            \"precip_type\": \"PrecipType\",\n            \"qpf\": \"Qpf\",\n            \"snow_qpf\": \"SnowQpf\",\n            \"rh\": \"Rh\",\n            \"wspd\": \"Wspd\",\n            \"wdir\": \"Wdir\",\n            \"wdir_cardinal\": \"WdirCardinal\",\n            \"gust\": \"Gust\",\n            \"clds\": \"Clds\",\n            \"vis\": \"Vis\",\n            \"mslp\": \"Mslp\",\n            \"uv_index_raw\": \"UvIndexRaw\",\n            \"uv_index\": \"UvIndex\",\n            \"uv_warning\": \"UvWarning\",\n            \"uv_desc\": \"UvDesc\",\n            \"golf_index\": \"GolfIndex\",\n            \"golf_category\": \"GolfCategory\",\n            \"severity\": \"Severity\",\n        }\n\n        df = df.rename(columns=rename_cols)\n\n        fields = self.spark_schema.fields\n\n        str_cols = list(\n            map(\n                lambda x: x.name,\n                filter(lambda x: isinstance(x.dataType, StringType), fields),\n            )\n        )\n        double_cols = list(\n            map(\n                lambda x: x.name,\n                filter(lambda x: isinstance(x.dataType, DoubleType), fields),\n            )\n        )\n        int_cols = list(\n            map(\n                lambda x: x.name,\n                filter(lambda x: isinstance(x.dataType, IntegerType), fields),\n            )\n        )\n\n        df[str_cols] = df[str_cols].astype(str)\n        df[double_cols] = df[double_cols].astype(float)\n        df[int_cols] = df[int_cols].astype(int)\n\n        df.reset_index(inplace=True, drop=True)\n\n        return df\n\n    def _get_api_params(self):\n        params = {\n            \"language\": self.language,\n            \"units\": self.units,\n            \"apiKey\": self.api_key,\n        }\n        return params\n\n    def _pull_for_weather_station(self, lat: str, lon: str) -&gt; pd.DataFrame:\n        response = json.loads(\n            self._fetch_from_url(f\"{lat}/{lon}/forecast/hourly/360hour.json\").decode(\n                \"utf-8\"\n            )\n        )\n        return pd.DataFrame(response[\"forecasts\"])\n\n    def _pull_data(self) -&gt; pd.DataFrame:\n\"\"\"\n        Pulls data from the Weather API and parses the JSON file.\n\n        Returns:\n            Raw form of data.\n        \"\"\"\n\n        df = self._pull_for_weather_station(self.lat, self.lon)\n        df[\"latitude\"] = self.lat\n        df[\"longitude\"] = self.lon\n\n        return df\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/the_weather_company/weather_forecast_api_v1_multi/","title":"Weather Forecast V1 Multi","text":""},{"location":"sdk/code-reference/pipelines/sources/spark/the_weather_company/weather_forecast_api_v1_multi/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.the_weather_company.weather_forecast_api_v1_multi.SparkWeatherCompanyForecastAPIV1MultiSource","title":"<code>SparkWeatherCompanyForecastAPIV1MultiSource</code>","text":"<p>             Bases: <code>SparkWeatherCompanyForecastAPIV1Source</code></p> <p>The Weather Forecast API V1 Multi Source is used to read 15 days forecast from the Weather API. It allows to pull weather data for multiple stations and returns all of them in a single DataFrame.</p> <p>URL for one station:  https://api.weather.com/v1/geocode/32.3667/-95.4/forecast/hourly/360hour.json</p> <p>It takes a list of Weather Stations. Each station item must contain comma separated Latitude &amp; Longitude.</p>"},{"location":"sdk/code-reference/pipelines/sources/spark/the_weather_company/weather_forecast_api_v1_multi/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.the_weather_company.weather_forecast_api_v1_multi.SparkWeatherCompanyForecastAPIV1MultiSource--examples","title":"Examples","text":"<p><code>[\"32.3667,-95.4\", \"51.52,-0.11\"]</code></p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance</p> required <code>options</code> <code>dict</code> <p>A dictionary of ISO Source specific configurations (See Attributes table below).</p> required <p>Attributes:</p> Name Type Description <code>stations</code> <code>list[str]</code> <p>List of Weather Stations.</p> <code>api_key</code> <code>str</code> <p>Weather API key.</p> <code>language</code> <code>str</code> <p>API response language. Defaults to <code>en-US</code>.</p> <code>units</code> <code>str</code> <p>Unit of measurements. Defaults to <code>e</code>.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/the_weather_company/weather_forecast_api_v1_multi.py</code> <pre><code>class SparkWeatherCompanyForecastAPIV1MultiSource(\n    SparkWeatherCompanyForecastAPIV1Source\n):\n\"\"\"\n    The Weather Forecast API V1 Multi Source is used to read 15 days forecast from the Weather API. It allows to\n    pull weather data for multiple stations and returns all of them in a single DataFrame.\n\n    URL for one station: &lt;a href=\"https://api.weather.com/v1/geocode/32.3667/-95.4/forecast/hourly/360hour.json\"&gt;\n    https://api.weather.com/v1/geocode/32.3667/-95.4/forecast/hourly/360hour.json&lt;/a&gt;\n\n    It takes a list of Weather Stations. Each station item must contain comma separated Latitude &amp; Longitude.\n\n    Examples\n    --------\n    `[\"32.3667,-95.4\", \"51.52,-0.11\"]`\n\n    Parameters:\n        spark (SparkSession): Spark Session instance\n        options (dict): A dictionary of ISO Source specific configurations (See Attributes table below).\n\n    Attributes:\n        stations (list[str]): List of Weather Stations.\n        api_key (str): Weather API key.\n        language (str): API response language. Defaults to `en-US`.\n        units (str): Unit of measurements. Defaults to `e`.\n    \"\"\"\n\n    spark: SparkSession\n    options: dict\n    spark_schema = WEATHER_FORECAST_SCHEMA\n    required_options = [\"stations\", \"api_key\"]\n\n    def __init__(self, spark: SparkSession, options: dict) -&gt; None:\n        super(SparkWeatherCompanyForecastAPIV1MultiSource, self).__init__(\n            spark, options\n        )\n        self.spark = spark\n        self.options = options\n        self.stations = self.options.get(\"stations\", [])\n        self.api_key = self.options.get(\"api_key\", \"\").strip()\n        self.language = self.options.get(\"language\", \"en-US\").strip()\n        self.units = self.options.get(\"units\", \"e\").strip()\n\n    def _pull_data(self) -&gt; pd.DataFrame:\n\"\"\"\n        Pulls data from the Weather API and parses the JSON file for multiple stations\n\n        Returns:\n            Raw form of data.\n        \"\"\"\n\n        result_df = None\n        for station in self.stations:\n            parts = station.split(\",\")\n            lat, lon = parts\n\n            df = self._pull_for_weather_station(lat, lon)\n            df[\"latitude\"] = lat\n            df[\"longitude\"] = lon\n\n            if result_df is not None:\n                result_df = pd.concat([result_df, df])\n            else:\n                result_df = df\n\n        return result_df\n\n    def _validate_options(self) -&gt; bool:\n        for station in self.stations:\n            parts = station.split(\",\")\n\n            if len(parts) != 2 or parts[0].strip() == \"\" or parts[1].strip() == \"\":\n                raise ValueError(\n                    f\"Each station item must contain comma separated Latitude &amp; Longitude. Eg: 10.23:45.2\"\n                )\n\n        return True\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/base_raw_to_mdm/","title":"Base raw to mdm","text":""},{"location":"sdk/code-reference/pipelines/transformers/spark/base_raw_to_mdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.base_raw_to_mdm.BaseRawToMDMTransformer","title":"<code>BaseRawToMDMTransformer</code>","text":"<p>             Bases: <code>TransformerInterface</code></p> <p>Base class for all the Raw to Meters Data Model Transformers.</p> Meters Data Model requires two outputs <ul> <li><code>UsageData</code> : To store measurement(value) as timeseries data.</li> <li><code>MetaData</code> : To store meters related meta information.</li> </ul> <p>It supports the generation of both the outputs as they share some common properties.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance.</p> required <code>data</code> <code>DataFrame</code> <p>Dataframe containing the raw MISO data.</p> required <code>output_type</code> <code>str</code> <p>Must be one of <code>usage</code> or <code>meta</code>.</p> required <code>name</code> <code>str</code> <p>Set this to override default <code>name</code> column.</p> <code>None</code> <code>description</code> <code>str</code> <p>Set this to override default <code>description</code> column.</p> <code>None</code> <code>value_type</code> <code>ValueType</code> <p>Set this to override default <code>value_type</code> column.</p> <code>None</code> <code>version</code> <code>str</code> <p>Set this to override default <code>version</code> column.</p> <code>None</code> <code>series_id</code> <code>str</code> <p>Set this to override default <code>series_id</code> column.</p> <code>None</code> <code>series_parent_id</code> <code>str</code> <p>Set this to override default <code>series_parent_id</code> column.</p> <code>None</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/base_raw_to_mdm.py</code> <pre><code>class BaseRawToMDMTransformer(TransformerInterface):\n\"\"\"\n    Base class for all the Raw to Meters Data Model Transformers.\n\n    Meters Data Model requires two outputs:\n        - `UsageData` : To store measurement(value) as timeseries data.\n        - `MetaData` : To store meters related meta information.\n\n    It supports the generation of both the outputs as they share some common properties.\n\n    Parameters:\n        spark (SparkSession): Spark Session instance.\n        data (DataFrame): Dataframe containing the raw MISO data.\n        output_type (str): Must be one of `usage` or `meta`.\n        name (str): Set this to override default `name` column.\n        description (str): Set this to override default `description` column.\n        value_type (ValueType): Set this to override default `value_type` column.\n        version (str): Set this to override default `version` column.\n        series_id (str): Set this to override default `series_id` column.\n        series_parent_id (str): Set this to override default `series_parent_id` column.\n    \"\"\"\n\n    spark: SparkSession\n    data: DataFrame\n    output_type: str\n    input_schema: StructType\n    target_schema: StructType\n    uid_col: str\n    series_id_col: str\n    timestamp_col: str\n    interval_timestamp_col: str\n    value_col: str\n    series_parent_id_col: str\n    name_col: str\n    uom_col: str\n    description_col: str\n    timestamp_start_col: str\n    timestamp_end_col: str\n    time_zone_col: str\n    version_col: str\n    series_type: SeriesType\n    model_type: ModelType\n    value_type: ValueType\n    properties_col: str\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        data: DataFrame,\n        output_type: str,\n        name: str = None,\n        description: str = None,\n        value_type: ValueType = None,\n        version: str = None,\n        series_id: str = None,\n        series_parent_id: str = None,\n    ):\n        self.spark = spark\n        self.data = data\n        self.output_type = output_type\n        self.name = name if name is not None else self.name_col\n        self.description = (\n            description if description is not None else self.description_col\n        )\n        self.value_type = value_type if value_type is not None else self.value_type\n        self.version = version if version is not None else self.version_col\n        self.series_id = series_id if series_id is not None else self.series_id_col\n        self.series_parent_id = (\n            series_parent_id\n            if series_parent_id is not None\n            else self.series_parent_id_col\n        )\n\n    @staticmethod\n    def system_type():\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_transform_validation(self) -&gt; bool:\n        valid_output_types = [\"usage\", \"meta\"]\n        if self.output_type not in valid_output_types:\n            raise ValueError(\n                f\"Invalid output_type `{self.output_type}` given. Must be one of {valid_output_types}\"\n            )\n\n        assert str(self.data.schema) == str(self.input_schema)\n        assert type(self.series_type).__name__ == SeriesType.__name__\n        assert type(self.model_type).__name__ == ModelType.__name__\n        assert type(self.value_type).__name__ == ValueType.__name__\n        return True\n\n    def post_transform_validation(self) -&gt; bool:\n        assert str(self.data.schema) == str(self.target_schema)\n        return True\n\n    def _get_transformed_df(self) -&gt; DataFrame:\n        if self.output_type == \"usage\":\n            self.target_schema = MDM_USAGE_SCHEMA\n            return self._get_usage_transformed_df()\n        else:\n            self.target_schema = MDM_META_SCHEMA\n            return self._get_meta_transformed_df()\n\n    def _convert_into_target_schema(self) -&gt; None:\n\"\"\"\n        Converts a Spark DataFrame structure into new structure based on the Target Schema.\n\n        Returns: Nothing.\n\n        \"\"\"\n\n        df: DataFrame = self.data\n        df = df.select(self.target_schema.names)\n\n        for field in self.target_schema.fields:\n            df = df.withColumn(field.name, col(field.name).cast(field.dataType))\n\n        self.data = self.spark.createDataFrame(df.rdd, self.target_schema)\n\n    def transform(self) -&gt; DataFrame:\n\"\"\"\n        Returns:\n            DataFrame: A dataframe with the raw data converted into MDM.\n        \"\"\"\n\n        self.pre_transform_validation()\n        self.data = self._get_transformed_df()\n        self._convert_into_target_schema()\n        self.post_transform_validation()\n\n        return self.data\n\n    def _add_uid_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Uid\", expr(self.uid_col))\n\n    def _add_series_id_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"SeriesId\", expr(self.series_id))\n\n    def _add_timestamp_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Timestamp\", expr(self.timestamp_col))\n\n    def _add_interval_timestamp_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"IntervalTimestamp\", expr(self.interval_timestamp_col))\n\n    def _add_value_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Value\", expr(self.value_col))\n\n    def _add_series_parent_id_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"SeriesParentId\", expr(self.series_parent_id))\n\n    def _add_name_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Name\", expr(self.name))\n\n    def _add_uom_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Uom\", expr(self.uom_col))\n\n    def _add_description_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Description\", expr(self.description))\n\n    def _add_timestamp_start_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"TimestampStart\", expr(self.timestamp_start_col))\n\n    def _add_timestamp_end_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"TimestampEnd\", expr(self.timestamp_end_col))\n\n    def _add_time_zone_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Timezone\", expr(self.time_zone_col))\n\n    def _add_version_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Version\", expr(self.version))\n\n    def _add_series_type_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"SeriesType\", lit(self.series_type.value))\n\n    def _add_model_type_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"ModelType\", lit(self.model_type.value))\n\n    def _add_value_type_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"ValueType\", lit(self.value_type.value))\n\n    def _add_properties_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Properties\", expr(self.properties_col))\n\n    def _pre_process(self) -&gt; DataFrame:\n        return self.data\n\n    @staticmethod\n    def _post_process(df: DataFrame) -&gt; DataFrame:\n        return df\n\n    def _get_usage_transformed_df(self) -&gt; DataFrame:\n        df = self._pre_process()\n\n        df = self._add_uid_column(df)\n        df = self._add_series_id_column(df)\n        df = self._add_timestamp_column(df)\n        df = self._add_interval_timestamp_column(df)\n        df = self._add_value_column(df)\n\n        df = self._post_process(df)\n\n        return df\n\n    def _get_meta_transformed_df(self) -&gt; DataFrame:\n        df = self._pre_process()\n\n        df = self._add_uid_column(df)\n        df = self._add_series_id_column(df)\n        df = self._add_series_parent_id_column(df)\n        df = self._add_name_column(df)\n        df = self._add_uom_column(df)\n        df = self._add_description_column(df)\n        df = self._add_timestamp_start_column(df)\n        df = self._add_timestamp_end_column(df)\n        df = self._add_time_zone_column(df)\n        df = self._add_version_column(df)\n        df = self._add_series_type_column(df)\n        df = self._add_model_type_column(df)\n        df = self._add_value_type_column(df)\n        df = self._add_properties_column(df)\n\n        df = self._post_process(df)\n\n        return df\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/base_raw_to_mdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.base_raw_to_mdm.BaseRawToMDMTransformer.transform","title":"<code>transform()</code>","text":"<p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe with the raw data converted into MDM.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/base_raw_to_mdm.py</code> <pre><code>def transform(self) -&gt; DataFrame:\n\"\"\"\n    Returns:\n        DataFrame: A dataframe with the raw data converted into MDM.\n    \"\"\"\n\n    self.pre_transform_validation()\n    self.data = self._get_transformed_df()\n    self._convert_into_target_schema()\n    self.post_transform_validation()\n\n    return self.data\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/binary_to_string/","title":"Convert Eventhub Body Column from Binary to String","text":""},{"location":"sdk/code-reference/pipelines/transformers/spark/binary_to_string/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.binary_to_string.BinaryToStringTransformer","title":"<code>BinaryToStringTransformer</code>","text":"<p>             Bases: <code>TransformerInterface</code></p> <p>Converts a dataframe body column from a binary to a string.</p>"},{"location":"sdk/code-reference/pipelines/transformers/spark/binary_to_string/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.binary_to_string.BinaryToStringTransformer--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.transformers import BinaryToStringTransformer\n\nbinary_to_string_transformer = BinaryToStringTransformer(\n    data=df,\n    souce_column_name=\"body\",\n    target_column_name=\"body\"\n)\n\nresult = binary_to_string_transformer.transform()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Dataframe to be transformed</p> required <code>source_column_name</code> <code>str</code> <p>Spark Dataframe column containing the Binary data</p> required <code>target_column_name</code> <code>str</code> <p>Spark Dataframe column name to be used for the String data</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/binary_to_string.py</code> <pre><code>class BinaryToStringTransformer(TransformerInterface):\n\"\"\"\n    Converts a dataframe body column from a binary to a string.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.transformers import BinaryToStringTransformer\n\n    binary_to_string_transformer = BinaryToStringTransformer(\n        data=df,\n        souce_column_name=\"body\",\n        target_column_name=\"body\"\n    )\n\n    result = binary_to_string_transformer.transform()\n    ```\n\n    Parameters:\n        data (DataFrame): Dataframe to be transformed\n        source_column_name (str): Spark Dataframe column containing the Binary data\n        target_column_name (str): Spark Dataframe column name to be used for the String data\n    \"\"\"\n\n    data: DataFrame\n    source_column_name: str\n    target_column_name: str\n\n    def __init__(\n        self, data: DataFrame, source_column_name: str, target_column_name: str\n    ) -&gt; None:\n        self.data = data\n        self.source_column_name = source_column_name\n        self.target_column_name = target_column_name\n\n    @staticmethod\n    def system_type():\n\"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_transform_validation(self):\n        return True\n\n    def post_transform_validation(self):\n        return True\n\n    def transform(self) -&gt; DataFrame:\n\"\"\"\n        Returns:\n            DataFrame: A dataframe with the body column converted to string.\n        \"\"\"\n        return self.data.withColumn(\n            self.target_column_name, self.data[self.source_column_name].cast(\"string\")\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/binary_to_string/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.binary_to_string.BinaryToStringTransformer.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/binary_to_string.py</code> <pre><code>@staticmethod\ndef system_type():\n\"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/binary_to_string/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.binary_to_string.BinaryToStringTransformer.transform","title":"<code>transform()</code>","text":"<p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe with the body column converted to string.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/binary_to_string.py</code> <pre><code>def transform(self) -&gt; DataFrame:\n\"\"\"\n    Returns:\n        DataFrame: A dataframe with the body column converted to string.\n    \"\"\"\n    return self.data.withColumn(\n        self.target_column_name, self.data[self.source_column_name].cast(\"string\")\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/edgex_opcua_json_to_pcdm/","title":"Convert Edge Xpert Json to Process Control Data Model","text":""},{"location":"sdk/code-reference/pipelines/transformers/spark/edgex_opcua_json_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.edgex_opcua_json_to_pcdm.EdgeXOPCUAJsonToPCDMTransformer","title":"<code>EdgeXOPCUAJsonToPCDMTransformer</code>","text":"<p>             Bases: <code>TransformerInterface</code></p> <p>Converts a Spark Dataframe column containing a json string created by EdgeX to the Process Control Data Model.</p>"},{"location":"sdk/code-reference/pipelines/transformers/spark/edgex_opcua_json_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.edgex_opcua_json_to_pcdm.EdgeXOPCUAJsonToPCDMTransformer--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.transformers import EdgeXOPCUAJsonToPCDMTransformer\n\nedge_opcua_json_to_pcdm_transformer = EdgeXOPCUAJsonToPCDMTransformer(\n    data=df,\n    souce_column_name=\"body\",\n    status_null_value=\"Good\",\n    change_type_value=\"insert\"\n)\n\nresult = edge_opcua_json_to_pcdm_transformer.transform()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Dataframe containing the column with EdgeX data</p> required <code>source_column_name</code> <code>str</code> <p>Spark Dataframe column containing the OPC Publisher Json OPC UA data</p> required <code>status_null_value</code> <code>optional str</code> <p>If populated, will replace 'Good' in the Status column with the specified value.</p> <code>'Good'</code> <code>change_type_value</code> <code>optional str</code> <p>If populated, will replace 'insert' in the ChangeType column with the specified value.</p> <code>'insert'</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/edgex_opcua_json_to_pcdm.py</code> <pre><code>class EdgeXOPCUAJsonToPCDMTransformer(TransformerInterface):\n\"\"\"\n    Converts a Spark Dataframe column containing a json string created by EdgeX to the Process Control Data Model.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.transformers import EdgeXOPCUAJsonToPCDMTransformer\n\n    edge_opcua_json_to_pcdm_transformer = EdgeXOPCUAJsonToPCDMTransformer(\n        data=df,\n        souce_column_name=\"body\",\n        status_null_value=\"Good\",\n        change_type_value=\"insert\"\n    )\n\n    result = edge_opcua_json_to_pcdm_transformer.transform()\n    ```\n\n    Parameters:\n        data (DataFrame): Dataframe containing the column with EdgeX data\n        source_column_name (str): Spark Dataframe column containing the OPC Publisher Json OPC UA data\n        status_null_value (optional str): If populated, will replace 'Good' in the Status column with the specified value.\n        change_type_value (optional str): If populated, will replace 'insert' in the ChangeType column with the specified value.\n    \"\"\"\n\n    data: DataFrame\n    source_column_name: str\n    status_null_value: str\n    change_type_value: str\n    tagname_field: str\n\n    def __init__(\n        self,\n        data: DataFrame,\n        source_column_name: str,\n        status_null_value: str = \"Good\",\n        change_type_value: str = \"insert\",\n        tagname_field=\"resourceName\",\n    ) -&gt; None:\n        self.data = data\n        self.source_column_name = source_column_name\n        self.status_null_value = status_null_value\n        self.change_type_value = change_type_value\n        self.tagname_field = tagname_field\n\n    @staticmethod\n    def system_type():\n\"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_transform_validation(self):\n        return True\n\n    def post_transform_validation(self):\n        return True\n\n    def transform(self) -&gt; DataFrame:\n\"\"\"\n        Returns:\n            DataFrame: A dataframe with the specified column converted to PCDM\n        \"\"\"\n        df = (\n            self.data.withColumn(\n                self.source_column_name,\n                from_json(self.source_column_name, EDGEX_SCHEMA),\n            )\n            .select(\"*\", explode(\"{}.readings\".format(self.source_column_name)))\n            .selectExpr(\n                \"explode({}.readings.{}) as TagName\".format(\n                    self.source_column_name, self.tagname_field\n                ),\n                \"to_utc_timestamp(to_timestamp((col.origin / 1000000000)), current_timezone()) as EventTime\",\n                \"col.value as Value\",\n                \"col.valueType as ValueType\",\n            )\n            .withColumn(\"Status\", lit(self.status_null_value))\n            .withColumn(\"ChangeType\", lit(self.change_type_value))\n            .withColumn(\n                \"ValueType\",\n                (\n                    when(col(\"ValueType\") == \"Int8\", \"integer\")\n                    .when(col(\"ValueType\") == \"Int16\", \"integer\")\n                    .when(col(\"ValueType\") == \"Int32\", \"integer\")\n                    .when(col(\"ValueType\") == \"Int64\", \"integer\")\n                    .when(col(\"ValueType\") == \"Uint8\", \"integer\")\n                    .when(col(\"ValueType\") == \"Uint16\", \"integer\")\n                    .when(col(\"ValueType\") == \"Uint32\", \"integer\")\n                    .when(col(\"ValueType\") == \"Uint64\", \"integer\")\n                    .when(col(\"ValueType\") == \"Float32\", \"float\")\n                    .when(col(\"ValueType\") == \"Float64\", \"float\")\n                    .when(col(\"ValueType\") == \"Bool\", \"bool\")\n                    .otherwise(\"string\")\n                ),\n            )\n        )\n\n        return df.select(\n            \"TagName\", \"EventTime\", \"Status\", \"Value\", \"ValueType\", \"ChangeType\"\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/edgex_opcua_json_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.edgex_opcua_json_to_pcdm.EdgeXOPCUAJsonToPCDMTransformer.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/edgex_opcua_json_to_pcdm.py</code> <pre><code>@staticmethod\ndef system_type():\n\"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/edgex_opcua_json_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.edgex_opcua_json_to_pcdm.EdgeXOPCUAJsonToPCDMTransformer.transform","title":"<code>transform()</code>","text":"<p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe with the specified column converted to PCDM</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/edgex_opcua_json_to_pcdm.py</code> <pre><code>def transform(self) -&gt; DataFrame:\n\"\"\"\n    Returns:\n        DataFrame: A dataframe with the specified column converted to PCDM\n    \"\"\"\n    df = (\n        self.data.withColumn(\n            self.source_column_name,\n            from_json(self.source_column_name, EDGEX_SCHEMA),\n        )\n        .select(\"*\", explode(\"{}.readings\".format(self.source_column_name)))\n        .selectExpr(\n            \"explode({}.readings.{}) as TagName\".format(\n                self.source_column_name, self.tagname_field\n            ),\n            \"to_utc_timestamp(to_timestamp((col.origin / 1000000000)), current_timezone()) as EventTime\",\n            \"col.value as Value\",\n            \"col.valueType as ValueType\",\n        )\n        .withColumn(\"Status\", lit(self.status_null_value))\n        .withColumn(\"ChangeType\", lit(self.change_type_value))\n        .withColumn(\n            \"ValueType\",\n            (\n                when(col(\"ValueType\") == \"Int8\", \"integer\")\n                .when(col(\"ValueType\") == \"Int16\", \"integer\")\n                .when(col(\"ValueType\") == \"Int32\", \"integer\")\n                .when(col(\"ValueType\") == \"Int64\", \"integer\")\n                .when(col(\"ValueType\") == \"Uint8\", \"integer\")\n                .when(col(\"ValueType\") == \"Uint16\", \"integer\")\n                .when(col(\"ValueType\") == \"Uint32\", \"integer\")\n                .when(col(\"ValueType\") == \"Uint64\", \"integer\")\n                .when(col(\"ValueType\") == \"Float32\", \"float\")\n                .when(col(\"ValueType\") == \"Float64\", \"float\")\n                .when(col(\"ValueType\") == \"Bool\", \"bool\")\n                .otherwise(\"string\")\n            ),\n        )\n    )\n\n    return df.select(\n        \"TagName\", \"EventTime\", \"Status\", \"Value\", \"ValueType\", \"ChangeType\"\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/fledge_opcua_json_to_pcdm/","title":"Convert Fledge Json to Process Control Data Model","text":""},{"location":"sdk/code-reference/pipelines/transformers/spark/fledge_opcua_json_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.fledge_opcua_json_to_pcdm.FledgeOPCUAJsonToPCDMTransformer","title":"<code>FledgeOPCUAJsonToPCDMTransformer</code>","text":"<p>             Bases: <code>TransformerInterface</code></p> <p>Converts a Spark Dataframe column containing a json string created by Fledge to the Process Control Data Model.</p>"},{"location":"sdk/code-reference/pipelines/transformers/spark/fledge_opcua_json_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.fledge_opcua_json_to_pcdm.FledgeOPCUAJsonToPCDMTransformer--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.transformers import FledgeOPCUAJsonToPCDMTransformer\n\nfledge_opcua_json_to_pcdm_transfromer = FledgeOPCUAJsonToPCDMTransformer(\n    data=df,\n    souce_column_name=\"body\",\n    status_null_value=\"Good\",\n    change_type_value=\"insert\",\n    timestamp_formats=[\n        \"yyyy-MM-dd'T'HH:mm:ss.SSSX\",\n        \"yyyy-MM-dd'T'HH:mm:ssX\",\n    ]\n)\n\nresult = fledge_opcua_json_to_pcdm_transfromer.transform()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Dataframe containing the column with Json Fledge data</p> required <code>source_column_name</code> <code>str</code> <p>Spark Dataframe column containing the OPC Publisher Json OPC UA data</p> required <code>status_null_value</code> <code>str</code> <p>If populated, will replace 'Good' in the Status column with the specified value.</p> <code>'Good'</code> <code>change_type_value</code> <code>optional str</code> <p>If populated, will replace 'insert' in the ChangeType column with the specified value.</p> <code>'insert'</code> <code>timestamp_formats</code> <code>list[str]</code> <p>Specifies the timestamp formats to be used for converting the timestamp string to a Timestamp Type. For more information on formats, refer to this documentation.</p> <code>[\"yyyy-MM-dd'T'HH:mm:ss.SSSX\", \"yyyy-MM-dd'T'HH:mm:ssX\"]</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/fledge_opcua_json_to_pcdm.py</code> <pre><code>class FledgeOPCUAJsonToPCDMTransformer(TransformerInterface):\n\"\"\"\n    Converts a Spark Dataframe column containing a json string created by Fledge to the Process Control Data Model.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.transformers import FledgeOPCUAJsonToPCDMTransformer\n\n    fledge_opcua_json_to_pcdm_transfromer = FledgeOPCUAJsonToPCDMTransformer(\n        data=df,\n        souce_column_name=\"body\",\n        status_null_value=\"Good\",\n        change_type_value=\"insert\",\n        timestamp_formats=[\n            \"yyyy-MM-dd'T'HH:mm:ss.SSSX\",\n            \"yyyy-MM-dd'T'HH:mm:ssX\",\n        ]\n    )\n\n    result = fledge_opcua_json_to_pcdm_transfromer.transform()\n    ```\n\n    Parameters:\n        data (DataFrame): Dataframe containing the column with Json Fledge data\n        source_column_name (str): Spark Dataframe column containing the OPC Publisher Json OPC UA data\n        status_null_value (str): If populated, will replace 'Good' in the Status column with the specified value.\n        change_type_value (optional str): If populated, will replace 'insert' in the ChangeType column with the specified value.\n        timestamp_formats (list[str]): Specifies the timestamp formats to be used for converting the timestamp string to a Timestamp Type. For more information on formats, refer to this [documentation.](https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html)\n    \"\"\"\n\n    data: DataFrame\n    source_column_name: str\n    status_null_value: str\n    change_type_value: str\n    timestamp_formats: list\n\n    def __init__(\n        self,\n        data: DataFrame,\n        source_column_name: str,\n        status_null_value: str = \"Good\",\n        change_type_value: str = \"insert\",\n        timestamp_formats: list = [\n            \"yyyy-MM-dd'T'HH:mm:ss.SSSX\",\n            \"yyyy-MM-dd'T'HH:mm:ssX\",\n        ],\n    ) -&gt; None:  # NOSONAR\n        self.data = data\n        self.source_column_name = source_column_name\n        self.status_null_value = status_null_value\n        self.change_type_value = change_type_value\n        self.timestamp_formats = timestamp_formats\n\n    @staticmethod\n    def system_type():\n\"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_transform_validation(self):\n        return True\n\n    def post_transform_validation(self):\n        return True\n\n    def transform(self) -&gt; DataFrame:\n\"\"\"\n        Returns:\n            DataFrame: A dataframe with the specified column converted to PCDM\n        \"\"\"\n        df = (\n            self.data.withColumn(\n                self.source_column_name,\n                from_json(self.source_column_name, FLEDGE_SCHEMA),\n            )\n            .selectExpr(\"inline({})\".format(self.source_column_name))\n            .select(explode(\"readings\"), \"timestamp\")\n            .withColumn(\n                \"EventTime\",\n                coalesce(\n                    *[to_timestamp(col(\"timestamp\"), f) for f in self.timestamp_formats]\n                ),\n            )\n            .withColumnRenamed(\"key\", \"TagName\")\n            .withColumnRenamed(\"value\", \"Value\")\n            .withColumn(\"Status\", lit(self.status_null_value))\n            .withColumn(\n                \"ValueType\",\n                when(col(\"value\").cast(\"float\").isNotNull(), \"float\").when(\n                    col(\"value\").cast(\"float\").isNull(), \"string\"\n                ),\n            )\n            .withColumn(\"ChangeType\", lit(self.change_type_value))\n        )\n\n        return df.select(\n            \"TagName\", \"EventTime\", \"Status\", \"Value\", \"ValueType\", \"ChangeType\"\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/fledge_opcua_json_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.fledge_opcua_json_to_pcdm.FledgeOPCUAJsonToPCDMTransformer.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/fledge_opcua_json_to_pcdm.py</code> <pre><code>@staticmethod\ndef system_type():\n\"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/fledge_opcua_json_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.fledge_opcua_json_to_pcdm.FledgeOPCUAJsonToPCDMTransformer.transform","title":"<code>transform()</code>","text":"<p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe with the specified column converted to PCDM</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/fledge_opcua_json_to_pcdm.py</code> <pre><code>def transform(self) -&gt; DataFrame:\n\"\"\"\n    Returns:\n        DataFrame: A dataframe with the specified column converted to PCDM\n    \"\"\"\n    df = (\n        self.data.withColumn(\n            self.source_column_name,\n            from_json(self.source_column_name, FLEDGE_SCHEMA),\n        )\n        .selectExpr(\"inline({})\".format(self.source_column_name))\n        .select(explode(\"readings\"), \"timestamp\")\n        .withColumn(\n            \"EventTime\",\n            coalesce(\n                *[to_timestamp(col(\"timestamp\"), f) for f in self.timestamp_formats]\n            ),\n        )\n        .withColumnRenamed(\"key\", \"TagName\")\n        .withColumnRenamed(\"value\", \"Value\")\n        .withColumn(\"Status\", lit(self.status_null_value))\n        .withColumn(\n            \"ValueType\",\n            when(col(\"value\").cast(\"float\").isNotNull(), \"float\").when(\n                col(\"value\").cast(\"float\").isNull(), \"string\"\n            ),\n        )\n        .withColumn(\"ChangeType\", lit(self.change_type_value))\n    )\n\n    return df.select(\n        \"TagName\", \"EventTime\", \"Status\", \"Value\", \"ValueType\", \"ChangeType\"\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/honeywell_apm_to_pcdm/","title":"Convert Honeywell APM Json to Process Control Data Model","text":""},{"location":"sdk/code-reference/pipelines/transformers/spark/honeywell_apm_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.honeywell_apm_to_pcdm.HoneywellAPMJsonToPCDMTransformer","title":"<code>HoneywellAPMJsonToPCDMTransformer</code>","text":"<p>             Bases: <code>TransformerInterface</code></p> <p>Converts a Spark Dataframe column containing a json string created by Honeywell APM to the Process Control Data Model.</p>"},{"location":"sdk/code-reference/pipelines/transformers/spark/honeywell_apm_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.honeywell_apm_to_pcdm.HoneywellAPMJsonToPCDMTransformer--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.transformers import HoneywellAPMJsonToPCDMTransformer\n\nhoneywell_apm_json_to_pcdm_transformer = HoneywellAPMJsonToPCDMTransformer(\n    data=df,\n    souce_column_name=\"body\",\n    status_null_value=\"Good\",\n    change_type_value=\"insert\"\n)\n\nresult = honeywell_apm_json_to_pcdm_transformer.transform()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Dataframe containing the column with EdgeX data</p> required <code>source_column_name</code> <code>str</code> <p>Spark Dataframe column containing the OPC Publisher Json OPC UA data</p> required <code>status_null_value</code> <code>optional str</code> <p>If populated, will replace 'Good' in the Status column with the specified value.</p> <code>'Good'</code> <code>change_type_value</code> <code>optional str</code> <p>If populated, will replace 'insert' in the ChangeType column with the specified value.</p> <code>'insert'</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/honeywell_apm_to_pcdm.py</code> <pre><code>class HoneywellAPMJsonToPCDMTransformer(TransformerInterface):\n\"\"\"\n    Converts a Spark Dataframe column containing a json string created by Honeywell APM to the Process Control Data Model.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.transformers import HoneywellAPMJsonToPCDMTransformer\n\n    honeywell_apm_json_to_pcdm_transformer = HoneywellAPMJsonToPCDMTransformer(\n        data=df,\n        souce_column_name=\"body\",\n        status_null_value=\"Good\",\n        change_type_value=\"insert\"\n    )\n\n    result = honeywell_apm_json_to_pcdm_transformer.transform()\n    ```\n\n    Parameters:\n        data (DataFrame): Dataframe containing the column with EdgeX data\n        source_column_name (str): Spark Dataframe column containing the OPC Publisher Json OPC UA data\n        status_null_value (optional str): If populated, will replace 'Good' in the Status column with the specified value.\n        change_type_value (optional str): If populated, will replace 'insert' in the ChangeType column with the specified value.\n    \"\"\"\n\n    data: DataFrame\n    source_column_name: str\n    status_null_value: str\n    change_type_value: str\n\n    def __init__(\n        self,\n        data: DataFrame,\n        source_column_name: str,\n        status_null_value: str = \"Good\",\n        change_type_value: str = \"insert\",\n    ) -&gt; None:\n        self.data = data\n        self.source_column_name = source_column_name\n        self.status_null_value = status_null_value\n        self.change_type_value = change_type_value\n\n    @staticmethod\n    def system_type():\n\"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_transform_validation(self):\n        return True\n\n    def post_transform_validation(self):\n        return True\n\n    def transform(self) -&gt; DataFrame:\n\"\"\"\n        Returns:\n            DataFrame: A dataframe with the specified column converted to PCDM\n        \"\"\"\n        df = (\n            self.data.withColumn(\"body\", from_json(self.source_column_name, APM_SCHEMA))\n            .select(explode(\"body.SystemTimeSeries.Samples\"))\n            .selectExpr(\"*\", \"to_timestamp(col.Time) as EventTime\")\n            .withColumn(\"TagName\", col(\"col.Itemname\"))\n            .withColumn(\"Status\", lit(self.status_null_value))\n            .withColumn(\"Value\", col(\"col.Value\"))\n            .withColumn(\n                \"ValueType\",\n                when(col(\"value\").cast(\"float\").isNotNull(), \"float\").when(\n                    col(\"value\").cast(\"float\").isNull(), \"string\"\n                ),\n            )\n            .withColumn(\"ChangeType\", lit(self.change_type_value))\n        )\n\n        return df.select(\n            \"TagName\", \"EventTime\", \"Status\", \"Value\", \"ValueType\", \"ChangeType\"\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/honeywell_apm_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.honeywell_apm_to_pcdm.HoneywellAPMJsonToPCDMTransformer.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/honeywell_apm_to_pcdm.py</code> <pre><code>@staticmethod\ndef system_type():\n\"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/honeywell_apm_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.honeywell_apm_to_pcdm.HoneywellAPMJsonToPCDMTransformer.transform","title":"<code>transform()</code>","text":"<p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe with the specified column converted to PCDM</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/honeywell_apm_to_pcdm.py</code> <pre><code>def transform(self) -&gt; DataFrame:\n\"\"\"\n    Returns:\n        DataFrame: A dataframe with the specified column converted to PCDM\n    \"\"\"\n    df = (\n        self.data.withColumn(\"body\", from_json(self.source_column_name, APM_SCHEMA))\n        .select(explode(\"body.SystemTimeSeries.Samples\"))\n        .selectExpr(\"*\", \"to_timestamp(col.Time) as EventTime\")\n        .withColumn(\"TagName\", col(\"col.Itemname\"))\n        .withColumn(\"Status\", lit(self.status_null_value))\n        .withColumn(\"Value\", col(\"col.Value\"))\n        .withColumn(\n            \"ValueType\",\n            when(col(\"value\").cast(\"float\").isNotNull(), \"float\").when(\n                col(\"value\").cast(\"float\").isNull(), \"string\"\n            ),\n        )\n        .withColumn(\"ChangeType\", lit(self.change_type_value))\n    )\n\n    return df.select(\n        \"TagName\", \"EventTime\", \"Status\", \"Value\", \"ValueType\", \"ChangeType\"\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/mirico_json_to_pcdm/","title":"Convert Mirico Json to Process Control Data Model","text":""},{"location":"sdk/code-reference/pipelines/transformers/spark/mirico_json_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.mirico_json_to_pcdm.MiricoJsonToPCDMTransformer","title":"<code>MiricoJsonToPCDMTransformer</code>","text":"<p>             Bases: <code>TransformerInterface</code></p> <p>Converts a Spark Dataframe column containing a json string created from Mirico to the Process Control Data Model.</p>"},{"location":"sdk/code-reference/pipelines/transformers/spark/mirico_json_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.mirico_json_to_pcdm.MiricoJsonToPCDMTransformer--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.transformers import MiricoJsonToPCDMTransformer\n\nmirico_json_to_pcdm_transformer = MiricoJsonToPCDMTransformer(\n    data=df\n    source_column_name=\"body\",\n    status_null_value=\"Good\",\n    change_type_value=\"insert\"\n)\n\nresult = mirico_json_to_pcdm_transformer.transform()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Dataframe containing the column with SEM data</p> required <code>source_column_name</code> <code>str</code> <p>Spark Dataframe column containing the OPC Publisher Json OPC UA data</p> required <code>status_null_value</code> <code>optional str</code> <p>If populated, will replace 'Good' in the Status column with the specified value.</p> <code>'Good'</code> <code>change_type_value</code> <code>optional str</code> <p>If populated, will replace 'insert' in the ChangeType column with the specified value.</p> <code>'insert'</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/mirico_json_to_pcdm.py</code> <pre><code>class MiricoJsonToPCDMTransformer(TransformerInterface):\n\"\"\"\n    Converts a Spark Dataframe column containing a json string created from Mirico to the Process Control Data Model.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.transformers import MiricoJsonToPCDMTransformer\n\n    mirico_json_to_pcdm_transformer = MiricoJsonToPCDMTransformer(\n        data=df\n        source_column_name=\"body\",\n        status_null_value=\"Good\",\n        change_type_value=\"insert\"\n    )\n\n    result = mirico_json_to_pcdm_transformer.transform()\n    ```\n\n    Parameters:\n        data (DataFrame): Dataframe containing the column with SEM data\n        source_column_name (str): Spark Dataframe column containing the OPC Publisher Json OPC UA data\n        status_null_value (optional str): If populated, will replace 'Good' in the Status column with the specified value.\n        change_type_value (optional str): If populated, will replace 'insert' in the ChangeType column with the specified value.\n    \"\"\"\n\n    data: DataFrame\n    source_column_name: str\n    status_null_value: str\n    change_type_value: str\n\n    def __init__(\n        self,\n        data: DataFrame,\n        source_column_name: str,\n        status_null_value: str = \"Good\",\n        change_type_value: str = \"insert\",\n    ) -&gt; None:\n        _package_version_meets_minimum(\"pyspark\", \"3.4.0\")\n        self.data = data\n        self.source_column_name = source_column_name\n        self.status_null_value = status_null_value\n        self.change_type_value = change_type_value\n\n    @staticmethod\n    def system_type():\n\"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_transform_validation(self):\n        return True\n\n    def post_transform_validation(self):\n        return True\n\n    def transform(self) -&gt; DataFrame:\n\"\"\"\n        Returns:\n            DataFrame: A dataframe with the specified column converted to PCDM\n        \"\"\"\n\n        mapping = mirico_field_mappings.MIRICO_FIELD_MAPPINGS\n        df = (\n            self.data.withColumn(\n                self.source_column_name,\n                from_json(self.source_column_name, \"map&lt;string,string&gt;\"),\n            )\n            .withColumn(\"TagName\", map_keys(\"body\"))\n            .withColumn(\"Value\", map_values(\"body\"))\n            .select(\n                map_from_arrays(\"TagName\", \"Value\").alias(\"x\"),\n                col(\"x.timeStamp\").alias(\"EventTime\"),\n                col(\"x.siteName\").alias(\"SiteName\"),\n            )\n            .select(\"EventTime\", \"SiteName\", posexplode(\"x\"))\n            .withColumn(\n                \"ValueType\", udf(lambda row: mapping[row][\"ValueType\"])(col(\"pos\"))\n            )\n            .withColumn(\"Status\", lit(\"Good\"))\n            .withColumn(\"ChangeType\", lit(\"insert\"))\n            .withColumn(\"TagName\", concat_ws(\":\", *[col(\"SiteName\"), col(\"key\")]))\n        )\n        return df.select(\n            \"EventTime\", \"TagName\", \"Status\", \"Value\", \"ValueType\", \"ChangeType\"\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/mirico_json_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.mirico_json_to_pcdm.MiricoJsonToPCDMTransformer.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/mirico_json_to_pcdm.py</code> <pre><code>@staticmethod\ndef system_type():\n\"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/mirico_json_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.mirico_json_to_pcdm.MiricoJsonToPCDMTransformer.transform","title":"<code>transform()</code>","text":"<p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe with the specified column converted to PCDM</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/mirico_json_to_pcdm.py</code> <pre><code>def transform(self) -&gt; DataFrame:\n\"\"\"\n    Returns:\n        DataFrame: A dataframe with the specified column converted to PCDM\n    \"\"\"\n\n    mapping = mirico_field_mappings.MIRICO_FIELD_MAPPINGS\n    df = (\n        self.data.withColumn(\n            self.source_column_name,\n            from_json(self.source_column_name, \"map&lt;string,string&gt;\"),\n        )\n        .withColumn(\"TagName\", map_keys(\"body\"))\n        .withColumn(\"Value\", map_values(\"body\"))\n        .select(\n            map_from_arrays(\"TagName\", \"Value\").alias(\"x\"),\n            col(\"x.timeStamp\").alias(\"EventTime\"),\n            col(\"x.siteName\").alias(\"SiteName\"),\n        )\n        .select(\"EventTime\", \"SiteName\", posexplode(\"x\"))\n        .withColumn(\n            \"ValueType\", udf(lambda row: mapping[row][\"ValueType\"])(col(\"pos\"))\n        )\n        .withColumn(\"Status\", lit(\"Good\"))\n        .withColumn(\"ChangeType\", lit(\"insert\"))\n        .withColumn(\"TagName\", concat_ws(\":\", *[col(\"SiteName\"), col(\"key\")]))\n    )\n    return df.select(\n        \"EventTime\", \"TagName\", \"Status\", \"Value\", \"ValueType\", \"ChangeType\"\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/opc_publisher_opcae_json_to_pcdm/","title":"Convert OPC Publisher Json for A&amp;E(Alarm &amp; Events) Data to Process Control Data Model","text":""},{"location":"sdk/code-reference/pipelines/transformers/spark/opc_publisher_opcae_json_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.opc_publisher_opcae_json_to_pcdm.OPCPublisherOPCAEJsonToPCDMTransformer","title":"<code>OPCPublisherOPCAEJsonToPCDMTransformer</code>","text":"<p>             Bases: <code>TransformerInterface</code></p> <p>Converts a Spark Dataframe column containing a json string created by OPC Publisher for A&amp;E(Alarm &amp;Events) data to the Process Control Data Model.</p>"},{"location":"sdk/code-reference/pipelines/transformers/spark/opc_publisher_opcae_json_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.opc_publisher_opcae_json_to_pcdm.OPCPublisherOPCAEJsonToPCDMTransformer--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.transformers import OPCPublisherOPCAEJsonToPCDMTransformer\n\nopc_publisher_opcae_json_to_pcdm_transformer = OPCPublisherOPCAEJsonToPCDMTransformer(\n    data=df,\n    souce_column_name=\"body\",\n    timestamp_formats=[\n        \"yyyy-MM-dd'T'HH:mm:ss.SSSX\",\n        \"yyyy-MM-dd'T'HH:mm:ssX\"\n    ],\n    filter=None\n)\n\nresult = opc_publisher_opcae_json_to_pcdm_transformer.transform()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Dataframe containing the column with Json OPC AE data</p> required <code>source_column_name</code> <code>str</code> <p>Spark Dataframe column containing the OPC Publisher Json OPC AE data</p> required <code>timestamp_formats</code> <code>optional list[str]</code> <p>Specifies the timestamp formats to be used for converting the timestamp string to a Timestamp Type. For more information on formats, refer to this documentation.</p> <code>None</code> <code>filter</code> <code>optional str</code> <p>Enables providing a filter to the data which can be required in certain scenarios. For example, it would be possible to filter on IoT Hub Device Id and Module by providing a filter in SQL format such as <code>systemProperties.iothub-connection-device-id = \"&lt;Device Id&gt;\" AND systemProperties.iothub-connection-module-id = \"&lt;Module&gt;\"</code></p> <code>None</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/opc_publisher_opcae_json_to_pcdm.py</code> <pre><code>class OPCPublisherOPCAEJsonToPCDMTransformer(TransformerInterface):\n\"\"\"\n    Converts a Spark Dataframe column containing a json string created by OPC Publisher for A&amp;E(Alarm &amp;Events) data to the Process Control Data Model.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.transformers import OPCPublisherOPCAEJsonToPCDMTransformer\n\n    opc_publisher_opcae_json_to_pcdm_transformer = OPCPublisherOPCAEJsonToPCDMTransformer(\n        data=df,\n        souce_column_name=\"body\",\n        timestamp_formats=[\n            \"yyyy-MM-dd'T'HH:mm:ss.SSSX\",\n            \"yyyy-MM-dd'T'HH:mm:ssX\"\n        ],\n        filter=None\n    )\n\n    result = opc_publisher_opcae_json_to_pcdm_transformer.transform()\n    ```\n\n    Parameters:\n        data (DataFrame): Dataframe containing the column with Json OPC AE data\n        source_column_name (str): Spark Dataframe column containing the OPC Publisher Json OPC AE data\n        timestamp_formats (optional list[str]): Specifies the timestamp formats to be used for converting the timestamp string to a Timestamp Type. For more information on formats, refer to this [documentation.](https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html)\n        filter (optional str): Enables providing a filter to the data which can be required in certain scenarios. For example, it would be possible to filter on IoT Hub Device Id and Module by providing a filter in SQL format such as `systemProperties.iothub-connection-device-id = \"&lt;Device Id&gt;\" AND systemProperties.iothub-connection-module-id = \"&lt;Module&gt;\"`\n    \"\"\"\n\n    data: DataFrame\n    source_column_name: str\n    timestamp_formats: list\n    filter: str\n\n    def __init__(\n        self,\n        data: DataFrame,\n        source_column_name: str,\n        timestamp_formats=None,\n        filter: str = None,\n    ) -&gt; None:  # NOSONAR\n        self.data = data\n        self.source_column_name = source_column_name\n        self.timestamp_formats = timestamp_formats or [\n            \"yyyy-MM-dd'T'HH:mm:ss.SSSX\",\n            \"yyyy-MM-dd'T'HH:mm:ssX\",\n        ]\n        self.filter = filter\n\n    @staticmethod\n    def system_type():\n\"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_transform_validation(self):\n        return True\n\n    def post_transform_validation(self):\n        return True\n\n    def transform(self) -&gt; DataFrame:\n\"\"\"\n        Returns:\n            DataFrame: A dataframe with the OPC Publisher A&amp;E data converted to the Process Control Data Model\n        \"\"\"\n\n        df = self.data.withColumn(\n            self.source_column_name,\n            from_json(col(self.source_column_name), ArrayType(StringType())),\n        ).withColumn(self.source_column_name, explode(self.source_column_name))\n\n        if self.filter != None:\n            df = df.where(self.filter)\n\n        df = df.withColumn(\n            \"OPCAE\", from_json(col(self.source_column_name), OPC_PUBLISHER_AE_SCHEMA)\n        )\n\n        df = df.select(\n            col(\"OPCAE.NodeId\"),\n            col(\"OPCAE.DisplayName\"),\n            col(\"OPCAE.Value.ConditionId.Value\").alias(\"ConditionId\"),\n            col(\"OPCAE.Value.AckedState.Value\").alias(\"AckedState\"),\n            col(\"OPCAE.Value.AckedState/FalseState.Value\").alias(\n                \"AckedState/FalseState\"\n            ),\n            col(\"OPCAE.Value.AckedState/Id.Value\").alias(\"AckedState/Id\"),\n            col(\"OPCAE.Value.AckedState/TrueState.Value\").alias(\"AckedState/TrueState\"),\n            col(\"OPCAE.Value.ActiveState.Value\").alias(\"ActiveState\"),\n            col(\"OPCAE.Value.ActiveState/FalseState.Value\").alias(\n                \"ActiveState/FalseState\"\n            ),\n            col(\"OPCAE.Value.ActiveState/Id.Value\").alias(\"ActiveState/Id\"),\n            col(\"OPCAE.Value.ActiveState/TrueState.Value\").alias(\n                \"ActiveState/TrueState\"\n            ),\n            col(\"OPCAE.Value.EnabledState.Value\").alias(\"EnabledState\"),\n            col(\"OPCAE.Value.EnabledState/FalseState.Value\").alias(\n                \"EnabledState/FalseState\"\n            ),\n            col(\"OPCAE.Value.EnabledState/Id.Value\").alias(\"EnabledState/Id\"),\n            col(\"OPCAE.Value.EnabledState/TrueState.Value\").alias(\n                \"EnabledState/TrueState\"\n            ),\n            col(\"OPCAE.Value.EventId.Value\").alias(\"EventId\"),\n            col(\"OPCAE.Value.EventType.Value\").alias(\"EventType\"),\n            col(\"OPCAE.Value.HighHighLimit.Value\").alias(\"HighHighLimit\"),\n            col(\"OPCAE.Value.HighLimit.Value\").alias(\"HighLimit\"),\n            col(\"OPCAE.Value.InputNode.Value\").alias(\"InputNode\"),\n            col(\"OPCAE.Value.LowLimit.Value\").alias(\"LowLimit\"),\n            col(\"OPCAE.Value.LowLowLimit.Value\").alias(\"LowLowLimit\"),\n            col(\"OPCAE.Value.Message.Value\").alias(\"Message\"),\n            col(\"OPCAE.Value.Quality.Value\").alias(\"Quality\"),\n            col(\"OPCAE.Value.ReceiveTime.Value\").alias(\"ReceiveTime\"),\n            col(\"OPCAE.Value.Retain.Value\").alias(\"Retain\"),\n            col(\"OPCAE.Value.Severity.Value\").alias(\"Severity\"),\n            col(\"OPCAE.Value.SourceName.Value\").alias(\"SourceName\"),\n            col(\"OPCAE.Value.SourceNode.Value\").alias(\"SourceNode\"),\n            col(\"OPCAE.Value.Time.Value\").alias(\"EventTime\"),\n        )\n\n        df = df.withColumn(\n            \"EventTime\",\n            coalesce(\n                *[to_timestamp(col(\"EventTime\"), f) for f in self.timestamp_formats]\n            ),\n        )\n\n        return df\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/opc_publisher_opcae_json_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.opc_publisher_opcae_json_to_pcdm.OPCPublisherOPCAEJsonToPCDMTransformer.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/opc_publisher_opcae_json_to_pcdm.py</code> <pre><code>@staticmethod\ndef system_type():\n\"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/opc_publisher_opcae_json_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.opc_publisher_opcae_json_to_pcdm.OPCPublisherOPCAEJsonToPCDMTransformer.transform","title":"<code>transform()</code>","text":"<p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe with the OPC Publisher A&amp;E data converted to the Process Control Data Model</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/opc_publisher_opcae_json_to_pcdm.py</code> <pre><code>def transform(self) -&gt; DataFrame:\n\"\"\"\n    Returns:\n        DataFrame: A dataframe with the OPC Publisher A&amp;E data converted to the Process Control Data Model\n    \"\"\"\n\n    df = self.data.withColumn(\n        self.source_column_name,\n        from_json(col(self.source_column_name), ArrayType(StringType())),\n    ).withColumn(self.source_column_name, explode(self.source_column_name))\n\n    if self.filter != None:\n        df = df.where(self.filter)\n\n    df = df.withColumn(\n        \"OPCAE\", from_json(col(self.source_column_name), OPC_PUBLISHER_AE_SCHEMA)\n    )\n\n    df = df.select(\n        col(\"OPCAE.NodeId\"),\n        col(\"OPCAE.DisplayName\"),\n        col(\"OPCAE.Value.ConditionId.Value\").alias(\"ConditionId\"),\n        col(\"OPCAE.Value.AckedState.Value\").alias(\"AckedState\"),\n        col(\"OPCAE.Value.AckedState/FalseState.Value\").alias(\n            \"AckedState/FalseState\"\n        ),\n        col(\"OPCAE.Value.AckedState/Id.Value\").alias(\"AckedState/Id\"),\n        col(\"OPCAE.Value.AckedState/TrueState.Value\").alias(\"AckedState/TrueState\"),\n        col(\"OPCAE.Value.ActiveState.Value\").alias(\"ActiveState\"),\n        col(\"OPCAE.Value.ActiveState/FalseState.Value\").alias(\n            \"ActiveState/FalseState\"\n        ),\n        col(\"OPCAE.Value.ActiveState/Id.Value\").alias(\"ActiveState/Id\"),\n        col(\"OPCAE.Value.ActiveState/TrueState.Value\").alias(\n            \"ActiveState/TrueState\"\n        ),\n        col(\"OPCAE.Value.EnabledState.Value\").alias(\"EnabledState\"),\n        col(\"OPCAE.Value.EnabledState/FalseState.Value\").alias(\n            \"EnabledState/FalseState\"\n        ),\n        col(\"OPCAE.Value.EnabledState/Id.Value\").alias(\"EnabledState/Id\"),\n        col(\"OPCAE.Value.EnabledState/TrueState.Value\").alias(\n            \"EnabledState/TrueState\"\n        ),\n        col(\"OPCAE.Value.EventId.Value\").alias(\"EventId\"),\n        col(\"OPCAE.Value.EventType.Value\").alias(\"EventType\"),\n        col(\"OPCAE.Value.HighHighLimit.Value\").alias(\"HighHighLimit\"),\n        col(\"OPCAE.Value.HighLimit.Value\").alias(\"HighLimit\"),\n        col(\"OPCAE.Value.InputNode.Value\").alias(\"InputNode\"),\n        col(\"OPCAE.Value.LowLimit.Value\").alias(\"LowLimit\"),\n        col(\"OPCAE.Value.LowLowLimit.Value\").alias(\"LowLowLimit\"),\n        col(\"OPCAE.Value.Message.Value\").alias(\"Message\"),\n        col(\"OPCAE.Value.Quality.Value\").alias(\"Quality\"),\n        col(\"OPCAE.Value.ReceiveTime.Value\").alias(\"ReceiveTime\"),\n        col(\"OPCAE.Value.Retain.Value\").alias(\"Retain\"),\n        col(\"OPCAE.Value.Severity.Value\").alias(\"Severity\"),\n        col(\"OPCAE.Value.SourceName.Value\").alias(\"SourceName\"),\n        col(\"OPCAE.Value.SourceNode.Value\").alias(\"SourceNode\"),\n        col(\"OPCAE.Value.Time.Value\").alias(\"EventTime\"),\n    )\n\n    df = df.withColumn(\n        \"EventTime\",\n        coalesce(\n            *[to_timestamp(col(\"EventTime\"), f) for f in self.timestamp_formats]\n        ),\n    )\n\n    return df\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/opc_publisher_opcua_json_to_pcdm/","title":"Convert OPC Publisher Json to Process Control Data Model","text":""},{"location":"sdk/code-reference/pipelines/transformers/spark/opc_publisher_opcua_json_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.opc_publisher_opcua_json_to_pcdm.OPCPublisherOPCUAJsonToPCDMTransformer","title":"<code>OPCPublisherOPCUAJsonToPCDMTransformer</code>","text":"<p>             Bases: <code>TransformerInterface</code></p> <p>Converts a Spark Dataframe column containing a json string created by OPC Publisher to the Process Control Data Model.</p>"},{"location":"sdk/code-reference/pipelines/transformers/spark/opc_publisher_opcua_json_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.opc_publisher_opcua_json_to_pcdm.OPCPublisherOPCUAJsonToPCDMTransformer--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.transformers import OPCPublisherOPCUAJsonToPCDMTransformer\n\nopc_publisher_opcua_json_to_pcdm_transformer = OPCPublisherOPCUAJsonToPCDMTransformer(\n    data=df,\n    souce_column_name=\"body\",\n    multiple_rows_per_message=True,\n    status_null_value=\"Good\",\n    change_type_value=\"insert\",\n    timestamp_formats=[\n        \"yyyy-MM-dd'T'HH:mm:ss.SSSX\",\n        \"yyyy-MM-dd'T'HH:mm:ssX\"\n    ],\n    filter=None\n)\n\nresult = opc_publisher_opcua_json_to_pcdm_transformer.transform()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Dataframe containing the column with Json OPC UA data</p> required <code>source_column_name</code> <code>str</code> <p>Spark Dataframe column containing the OPC Publisher Json OPC UA data</p> required <code>multiple_rows_per_message</code> <code>optional bool</code> <p>Each Dataframe Row contains an array of/multiple OPC UA messages. The list of Json will be exploded into rows in the Dataframe.</p> <code>True</code> <code>status_null_value</code> <code>optional str</code> <p>If populated, will replace null values in the Status column with the specified value.</p> <code>None</code> <code>change_type_value</code> <code>optional str</code> <p>If populated, will replace 'insert' in the ChangeType column with the specified value.</p> <code>'insert'</code> <code>timestamp_formats</code> <code>optional list[str]</code> <p>Specifies the timestamp formats to be used for converting the timestamp string to a Timestamp Type. For more information on formats, refer to this documentation.</p> <code>[\"yyyy-MM-dd'T'HH:mm:ss.SSSX\", \"yyyy-MM-dd'T'HH:mm:ssX\"]</code> <code>filter</code> <code>optional str</code> <p>Enables providing a filter to the data which can be required in certain scenarios. For example, it would be possible to filter on IoT Hub Device Id and Module by providing a filter in SQL format such as <code>systemProperties.iothub-connection-device-id = \"&lt;Device Id&gt;\" AND systemProperties.iothub-connection-module-id = \"&lt;Module&gt;\"</code></p> <code>None</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/opc_publisher_opcua_json_to_pcdm.py</code> <pre><code>class OPCPublisherOPCUAJsonToPCDMTransformer(TransformerInterface):\n\"\"\"\n    Converts a Spark Dataframe column containing a json string created by OPC Publisher to the Process Control Data Model.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.transformers import OPCPublisherOPCUAJsonToPCDMTransformer\n\n    opc_publisher_opcua_json_to_pcdm_transformer = OPCPublisherOPCUAJsonToPCDMTransformer(\n        data=df,\n        souce_column_name=\"body\",\n        multiple_rows_per_message=True,\n        status_null_value=\"Good\",\n        change_type_value=\"insert\",\n        timestamp_formats=[\n            \"yyyy-MM-dd'T'HH:mm:ss.SSSX\",\n            \"yyyy-MM-dd'T'HH:mm:ssX\"\n        ],\n        filter=None\n    )\n\n    result = opc_publisher_opcua_json_to_pcdm_transformer.transform()\n    ```\n\n    Parameters:\n        data (DataFrame): Dataframe containing the column with Json OPC UA data\n        source_column_name (str): Spark Dataframe column containing the OPC Publisher Json OPC UA data\n        multiple_rows_per_message (optional bool): Each Dataframe Row contains an array of/multiple OPC UA messages. The list of Json will be exploded into rows in the Dataframe.\n        status_null_value (optional str): If populated, will replace null values in the Status column with the specified value.\n        change_type_value (optional str): If populated, will replace 'insert' in the ChangeType column with the specified value.\n        timestamp_formats (optional list[str]): Specifies the timestamp formats to be used for converting the timestamp string to a Timestamp Type. For more information on formats, refer to this [documentation.](https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html)\n        filter (optional str): Enables providing a filter to the data which can be required in certain scenarios. For example, it would be possible to filter on IoT Hub Device Id and Module by providing a filter in SQL format such as `systemProperties.iothub-connection-device-id = \"&lt;Device Id&gt;\" AND systemProperties.iothub-connection-module-id = \"&lt;Module&gt;\"`\n    \"\"\"\n\n    data: DataFrame\n    source_column_name: str\n    multiple_rows_per_message: bool\n    tagname_field: str\n    status_null_value: str\n    change_type_value: str\n    timestamp_formats: list\n    filter: str\n\n    def __init__(\n        self,\n        data: DataFrame,\n        source_column_name: str,\n        multiple_rows_per_message: bool = True,\n        tagname_field: str = \"DisplayName\",\n        status_null_value: str = None,\n        change_type_value: str = \"insert\",\n        timestamp_formats: list = [\n            \"yyyy-MM-dd'T'HH:mm:ss.SSSX\",\n            \"yyyy-MM-dd'T'HH:mm:ssX\",\n        ],\n        filter: str = None,\n    ) -&gt; None:  # NOSONAR\n        self.data = data\n        self.source_column_name = source_column_name\n        self.multiple_rows_per_message = multiple_rows_per_message\n        self.tagname_field = tagname_field\n        self.status_null_value = status_null_value\n        self.change_type_value = change_type_value\n        self.timestamp_formats = timestamp_formats\n        self.filter = filter\n\n    @staticmethod\n    def system_type():\n\"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_transform_validation(self):\n        return True\n\n    def post_transform_validation(self):\n        return True\n\n    def transform(self) -&gt; DataFrame:\n\"\"\"\n        Returns:\n            DataFrame: A dataframe with the specified column converted to PCDM\n        \"\"\"\n        if self.multiple_rows_per_message:\n            df = self.data.withColumn(\n                self.source_column_name,\n                from_json(col(self.source_column_name), ArrayType(StringType())),\n            ).withColumn(self.source_column_name, explode(self.source_column_name))\n        else:\n            df = self.data.withColumn(\n                self.source_column_name,\n                from_json(col(self.source_column_name), StringType()),\n            )\n\n        if self.filter != None:\n            df = df.where(self.filter)\n\n        df = (\n            df.withColumn(\n                \"OPCUA\", from_json(col(self.source_column_name), OPC_PUBLISHER_SCHEMA)\n            )\n            .withColumn(\"TagName\", (col(\"OPCUA.{}\".format(self.tagname_field))))\n            .withColumn(\n                \"EventTime\",\n                coalesce(\n                    *[\n                        to_timestamp(col(\"OPCUA.Value.SourceTimestamp\"), f)\n                        for f in self.timestamp_formats\n                    ]\n                ),\n            )\n            .withColumn(\"Value\", col(\"OPCUA.Value.Value\"))\n            .withColumn(\n                \"ValueType\",\n                when(col(\"Value\").cast(\"float\").isNotNull(), \"float\")\n                .when(col(\"Value\").cast(\"float\").isNull(), \"string\")\n                .otherwise(\"unknown\"),\n            )\n            .withColumn(\"ChangeType\", lit(self.change_type_value))\n        )\n\n        status_col_name = \"OPCUA.Value.StatusCode.Symbol\"\n        if self.status_null_value != None:\n            df = df.withColumn(\n                \"Status\",\n                when(col(status_col_name).isNotNull(), col(status_col_name)).otherwise(\n                    lit(self.status_null_value)\n                ),\n            )\n        else:\n            df = df.withColumn(\"Status\", col(status_col_name))\n\n        return df.select(\n            \"TagName\", \"EventTime\", \"Status\", \"Value\", \"ValueType\", \"ChangeType\"\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/opc_publisher_opcua_json_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.opc_publisher_opcua_json_to_pcdm.OPCPublisherOPCUAJsonToPCDMTransformer.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/opc_publisher_opcua_json_to_pcdm.py</code> <pre><code>@staticmethod\ndef system_type():\n\"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/opc_publisher_opcua_json_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.opc_publisher_opcua_json_to_pcdm.OPCPublisherOPCUAJsonToPCDMTransformer.transform","title":"<code>transform()</code>","text":"<p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe with the specified column converted to PCDM</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/opc_publisher_opcua_json_to_pcdm.py</code> <pre><code>def transform(self) -&gt; DataFrame:\n\"\"\"\n    Returns:\n        DataFrame: A dataframe with the specified column converted to PCDM\n    \"\"\"\n    if self.multiple_rows_per_message:\n        df = self.data.withColumn(\n            self.source_column_name,\n            from_json(col(self.source_column_name), ArrayType(StringType())),\n        ).withColumn(self.source_column_name, explode(self.source_column_name))\n    else:\n        df = self.data.withColumn(\n            self.source_column_name,\n            from_json(col(self.source_column_name), StringType()),\n        )\n\n    if self.filter != None:\n        df = df.where(self.filter)\n\n    df = (\n        df.withColumn(\n            \"OPCUA\", from_json(col(self.source_column_name), OPC_PUBLISHER_SCHEMA)\n        )\n        .withColumn(\"TagName\", (col(\"OPCUA.{}\".format(self.tagname_field))))\n        .withColumn(\n            \"EventTime\",\n            coalesce(\n                *[\n                    to_timestamp(col(\"OPCUA.Value.SourceTimestamp\"), f)\n                    for f in self.timestamp_formats\n                ]\n            ),\n        )\n        .withColumn(\"Value\", col(\"OPCUA.Value.Value\"))\n        .withColumn(\n            \"ValueType\",\n            when(col(\"Value\").cast(\"float\").isNotNull(), \"float\")\n            .when(col(\"Value\").cast(\"float\").isNull(), \"string\")\n            .otherwise(\"unknown\"),\n        )\n        .withColumn(\"ChangeType\", lit(self.change_type_value))\n    )\n\n    status_col_name = \"OPCUA.Value.StatusCode.Symbol\"\n    if self.status_null_value != None:\n        df = df.withColumn(\n            \"Status\",\n            when(col(status_col_name).isNotNull(), col(status_col_name)).otherwise(\n                lit(self.status_null_value)\n            ),\n        )\n    else:\n        df = df.withColumn(\"Status\", col(status_col_name))\n\n    return df.select(\n        \"TagName\", \"EventTime\", \"Status\", \"Value\", \"ValueType\", \"ChangeType\"\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/pandas_to_pyspark/","title":"Pandas to PySpark DataFrame Conversion","text":""},{"location":"sdk/code-reference/pipelines/transformers/spark/pandas_to_pyspark/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.pandas_to_pyspark.PandasToPySparkTransformer","title":"<code>PandasToPySparkTransformer</code>","text":"<p>             Bases: <code>TransformerInterface</code></p> <p>Converts a Pandas DataFrame to a PySpark DataFrame.</p>"},{"location":"sdk/code-reference/pipelines/transformers/spark/pandas_to_pyspark/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.pandas_to_pyspark.PandasToPySparkTransformer--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.transformers import PandasToPySparkTransformer\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\npandas_to_pyspark = PandasToPySparkTransformer(\n    spark=spark,\n    df=df,\n)\n\nresult = pandas_to_pyspark.transform()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session required to convert DataFrame</p> required <code>df</code> <code>DataFrame</code> <p>Pandas DataFrame to be converted</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/pandas_to_pyspark.py</code> <pre><code>class PandasToPySparkTransformer(TransformerInterface):\n\"\"\"\n    Converts a Pandas DataFrame to a PySpark DataFrame.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.transformers import PandasToPySparkTransformer\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    pandas_to_pyspark = PandasToPySparkTransformer(\n        spark=spark,\n        df=df,\n    )\n\n    result = pandas_to_pyspark.transform()\n    ```\n\n    Parameters:\n        spark (SparkSession): Spark Session required to convert DataFrame\n        df (DataFrame): Pandas DataFrame to be converted\n    \"\"\"\n\n    spark: SparkSession\n    df: PandasDataFrame\n\n    def __init__(self, spark: SparkSession, df: PandasDataFrame) -&gt; None:\n        self.spark = spark\n        self.df = df\n\n    @staticmethod\n    def system_type():\n\"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_transform_validation(self):\n        return True\n\n    def post_transform_validation(self):\n        return True\n\n    def transform(self) -&gt; PySparkDataFrame:\n\"\"\"\n        Returns:\n            DataFrame: A PySpark dataframe converted from a Pandas DataFrame.\n        \"\"\"\n\n        self.df = _prepare_pandas_to_convert_to_spark(self.df)\n        df = self.spark.createDataFrame(self.df)\n\n        return df\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/pandas_to_pyspark/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.pandas_to_pyspark.PandasToPySparkTransformer.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/pandas_to_pyspark.py</code> <pre><code>@staticmethod\ndef system_type():\n\"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/pandas_to_pyspark/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.pandas_to_pyspark.PandasToPySparkTransformer.transform","title":"<code>transform()</code>","text":"<p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A PySpark dataframe converted from a Pandas DataFrame.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/pandas_to_pyspark.py</code> <pre><code>def transform(self) -&gt; PySparkDataFrame:\n\"\"\"\n    Returns:\n        DataFrame: A PySpark dataframe converted from a Pandas DataFrame.\n    \"\"\"\n\n    self.df = _prepare_pandas_to_convert_to_spark(self.df)\n    df = self.spark.createDataFrame(self.df)\n\n    return df\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/pcdm_to_honeywell_apm/","title":"Convert Process Control Data Model to Honeywell APM Json","text":""},{"location":"sdk/code-reference/pipelines/transformers/spark/pcdm_to_honeywell_apm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.pcdm_to_honeywell_apm.PCDMToHoneywellAPMTransformer","title":"<code>PCDMToHoneywellAPMTransformer</code>","text":"<p>             Bases: <code>TransformerInterface</code></p> <p>Converts a Spark Dataframe in PCDM format to Honeywell APM format.</p>"},{"location":"sdk/code-reference/pipelines/transformers/spark/pcdm_to_honeywell_apm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.pcdm_to_honeywell_apm.PCDMToHoneywellAPMTransformer--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.transformers import PCDMToHoneywellAPMTransformer\n\npcdm_to_honeywell_apm_transformer = PCDMToHoneywellAPMTransformer(\n    data=df,\n    quality=\"Good\",\n    history_samples_per_message=1,\n    compress_payload=True\n)\n\nresult = pcdm_to_honeywell_apm_transformer.transform()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dataframe</code> <p>Spark Dataframe in PCDM format</p> required <code>quality</code> <code>str</code> <p>Value for quality inside HistorySamples</p> <code>'Good'</code> <code>history_samples_per_message</code> <code>int</code> <p>The number of HistorySamples for each row in the DataFrame (Batch Only)</p> <code>1</code> <code>compress_payload</code> <code>bool</code> <p>If True compresses CloudPlatformEvent with gzip compression</p> <code>True</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/pcdm_to_honeywell_apm.py</code> <pre><code>class PCDMToHoneywellAPMTransformer(TransformerInterface):\n\"\"\"\n    Converts a Spark Dataframe in PCDM format to Honeywell APM format.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.transformers import PCDMToHoneywellAPMTransformer\n\n    pcdm_to_honeywell_apm_transformer = PCDMToHoneywellAPMTransformer(\n        data=df,\n        quality=\"Good\",\n        history_samples_per_message=1,\n        compress_payload=True\n    )\n\n    result = pcdm_to_honeywell_apm_transformer.transform()\n    ```\n\n    Parameters:\n        data (Dataframe): Spark Dataframe in PCDM format\n        quality (str): Value for quality inside HistorySamples\n        history_samples_per_message (int): The number of HistorySamples for each row in the DataFrame (Batch Only)\n        compress_payload (bool): If True compresses CloudPlatformEvent with gzip compression\n    \"\"\"\n\n    data: DataFrame\n    quality: str\n    history_samples_per_message: int\n    compress_payload: bool\n\n    def __init__(\n        self,\n        data: DataFrame,\n        quality: str = \"Good\",\n        history_samples_per_message: int = 1,\n        compress_payload: bool = True,\n    ) -&gt; None:\n        self.data = data\n        self.quality = quality\n        self.history_samples_per_message = history_samples_per_message\n        self.compress_payload = compress_payload\n\n    @staticmethod\n    def system_type():\n\"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_transform_validation(self):\n        return True\n\n    def post_transform_validation(self):\n        return True\n\n    def transform(self) -&gt; DataFrame:\n\"\"\"\n        Returns:\n            DataFrame: A dataframe with with rows in Honeywell APM format\n        \"\"\"\n\n        @udf(\"string\")\n        def _compress_payload(data):\n            compressed_data = gzip.compress(data.encode(\"utf-8\"))\n            encoded_data = base64.b64encode(compressed_data).decode(\"utf-8\")\n            return encoded_data\n\n        if self.data.isStreaming == False and self.history_samples_per_message &gt; 1:\n            w = Window.partitionBy(\"TagName\").orderBy(\"TagName\")\n            cleaned_pcdm_df = (\n                self.data.withColumn(\n                    \"index\",\n                    floor(\n                        (row_number().over(w) - 0.01) / self.history_samples_per_message\n                    ),\n                )\n                .withColumn(\n                    \"HistorySamples\",\n                    struct(\n                        col(\"TagName\").alias(\"ItemName\"),\n                        lit(self.quality).alias(\"Quality\"),\n                        col(\"EventTime\").alias(\"Time\"),\n                        col(\"Value\").alias(\"Value\"),\n                    ).alias(\"HistorySamples\"),\n                )\n                .groupBy(\"TagName\", \"index\")\n                .agg(collect_list(\"HistorySamples\").alias(\"HistorySamples\"))\n                .withColumn(\"guid\", sha2(col(\"TagName\"), 256).cast(\"string\"))\n                .withColumn(\n                    \"value\",\n                    struct(\n                        col(\"guid\").alias(\"SystemGuid\"), col(\"HistorySamples\")\n                    ).alias(\"value\"),\n                )\n            )\n        else:\n            cleaned_pcdm_df = self.data.withColumn(\n                \"guid\", sha2(col(\"TagName\"), 256).cast(\"string\")\n            ).withColumn(\n                \"value\",\n                struct(\n                    col(\"guid\").alias(\"SystemGuid\"),\n                    array(\n                        struct(\n                            col(\"TagName\").alias(\"ItemName\"),\n                            lit(self.quality).alias(\"Quality\"),\n                            col(\"EventTime\").alias(\"Time\"),\n                            col(\"Value\").alias(\"Value\"),\n                        ),\n                    ).alias(\"HistorySamples\"),\n                ),\n            )\n\n        df = (\n            cleaned_pcdm_df.withColumn(\n                \"CloudPlatformEvent\",\n                struct(\n                    lit(datetime.now(tz=pytz.UTC)).alias(\"CreatedTime\"),\n                    lit(expr(\"uuid()\")).alias(\"Id\"),\n                    col(\"guid\").alias(\"CreatorId\"),\n                    lit(\"CloudPlatformSystem\").alias(\"CreatorType\"),\n                    lit(None).alias(\"GeneratorId\"),\n                    lit(\"CloudPlatformTenant\").alias(\"GeneratorType\"),\n                    col(\"guid\").alias(\"TargetId\"),\n                    lit(\"CloudPlatformTenant\").alias(\"TargetType\"),\n                    lit(None).alias(\"TargetContext\"),\n                    struct(\n                        lit(\"TextualBody\").alias(\"type\"),\n                        to_json(col(\"value\")).alias(\"value\"),\n                        lit(\"application/json\").alias(\"format\"),\n                    ).alias(\"Body\"),\n                    array(\n                        struct(\n                            lit(\"SystemType\").alias(\"Key\"),\n                            lit(\"apm-system\").alias(\"Value\"),\n                        ),\n                        struct(\n                            lit(\"SystemGuid\").alias(\"Key\"), col(\"guid\").alias(\"Value\")\n                        ),\n                    ).alias(\"BodyProperties\"),\n                    lit(\"DataChange.Update\").alias(\"EventType\"),\n                ),\n            )\n            .withColumn(\"AnnotationStreamIds\", lit(\",\"))\n            .withColumn(\"partitionKey\", col(\"guid\"))\n        )\n        if self.compress_payload:\n            return df.select(\n                _compress_payload(to_json(\"CloudPlatformEvent\")).alias(\n                    \"CloudPlatformEvent\"\n                ),\n                \"AnnotationStreamIds\",\n                \"partitionKey\",\n            )\n        else:\n            return df.select(\n                \"CloudPlatformEvent\", \"AnnotationStreamIds\", \"partitionKey\"\n            )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/pcdm_to_honeywell_apm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.pcdm_to_honeywell_apm.PCDMToHoneywellAPMTransformer.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/pcdm_to_honeywell_apm.py</code> <pre><code>@staticmethod\ndef system_type():\n\"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/pcdm_to_honeywell_apm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.pcdm_to_honeywell_apm.PCDMToHoneywellAPMTransformer.transform","title":"<code>transform()</code>","text":"<p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe with with rows in Honeywell APM format</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/pcdm_to_honeywell_apm.py</code> <pre><code>def transform(self) -&gt; DataFrame:\n\"\"\"\n    Returns:\n        DataFrame: A dataframe with with rows in Honeywell APM format\n    \"\"\"\n\n    @udf(\"string\")\n    def _compress_payload(data):\n        compressed_data = gzip.compress(data.encode(\"utf-8\"))\n        encoded_data = base64.b64encode(compressed_data).decode(\"utf-8\")\n        return encoded_data\n\n    if self.data.isStreaming == False and self.history_samples_per_message &gt; 1:\n        w = Window.partitionBy(\"TagName\").orderBy(\"TagName\")\n        cleaned_pcdm_df = (\n            self.data.withColumn(\n                \"index\",\n                floor(\n                    (row_number().over(w) - 0.01) / self.history_samples_per_message\n                ),\n            )\n            .withColumn(\n                \"HistorySamples\",\n                struct(\n                    col(\"TagName\").alias(\"ItemName\"),\n                    lit(self.quality).alias(\"Quality\"),\n                    col(\"EventTime\").alias(\"Time\"),\n                    col(\"Value\").alias(\"Value\"),\n                ).alias(\"HistorySamples\"),\n            )\n            .groupBy(\"TagName\", \"index\")\n            .agg(collect_list(\"HistorySamples\").alias(\"HistorySamples\"))\n            .withColumn(\"guid\", sha2(col(\"TagName\"), 256).cast(\"string\"))\n            .withColumn(\n                \"value\",\n                struct(\n                    col(\"guid\").alias(\"SystemGuid\"), col(\"HistorySamples\")\n                ).alias(\"value\"),\n            )\n        )\n    else:\n        cleaned_pcdm_df = self.data.withColumn(\n            \"guid\", sha2(col(\"TagName\"), 256).cast(\"string\")\n        ).withColumn(\n            \"value\",\n            struct(\n                col(\"guid\").alias(\"SystemGuid\"),\n                array(\n                    struct(\n                        col(\"TagName\").alias(\"ItemName\"),\n                        lit(self.quality).alias(\"Quality\"),\n                        col(\"EventTime\").alias(\"Time\"),\n                        col(\"Value\").alias(\"Value\"),\n                    ),\n                ).alias(\"HistorySamples\"),\n            ),\n        )\n\n    df = (\n        cleaned_pcdm_df.withColumn(\n            \"CloudPlatformEvent\",\n            struct(\n                lit(datetime.now(tz=pytz.UTC)).alias(\"CreatedTime\"),\n                lit(expr(\"uuid()\")).alias(\"Id\"),\n                col(\"guid\").alias(\"CreatorId\"),\n                lit(\"CloudPlatformSystem\").alias(\"CreatorType\"),\n                lit(None).alias(\"GeneratorId\"),\n                lit(\"CloudPlatformTenant\").alias(\"GeneratorType\"),\n                col(\"guid\").alias(\"TargetId\"),\n                lit(\"CloudPlatformTenant\").alias(\"TargetType\"),\n                lit(None).alias(\"TargetContext\"),\n                struct(\n                    lit(\"TextualBody\").alias(\"type\"),\n                    to_json(col(\"value\")).alias(\"value\"),\n                    lit(\"application/json\").alias(\"format\"),\n                ).alias(\"Body\"),\n                array(\n                    struct(\n                        lit(\"SystemType\").alias(\"Key\"),\n                        lit(\"apm-system\").alias(\"Value\"),\n                    ),\n                    struct(\n                        lit(\"SystemGuid\").alias(\"Key\"), col(\"guid\").alias(\"Value\")\n                    ),\n                ).alias(\"BodyProperties\"),\n                lit(\"DataChange.Update\").alias(\"EventType\"),\n            ),\n        )\n        .withColumn(\"AnnotationStreamIds\", lit(\",\"))\n        .withColumn(\"partitionKey\", col(\"guid\"))\n    )\n    if self.compress_payload:\n        return df.select(\n            _compress_payload(to_json(\"CloudPlatformEvent\")).alias(\n                \"CloudPlatformEvent\"\n            ),\n            \"AnnotationStreamIds\",\n            \"partitionKey\",\n        )\n    else:\n        return df.select(\n            \"CloudPlatformEvent\", \"AnnotationStreamIds\", \"partitionKey\"\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/pyspark_to_pandas/","title":"PySpark to Pandas DataFrame Conversion","text":""},{"location":"sdk/code-reference/pipelines/transformers/spark/pyspark_to_pandas/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.pyspark_to_pandas.PySparkToPandasTransformer","title":"<code>PySparkToPandasTransformer</code>","text":"<p>             Bases: <code>TransformerInterface</code></p> <p>Converts a PySpark DataFrame to a Pandas DataFrame.</p>"},{"location":"sdk/code-reference/pipelines/transformers/spark/pyspark_to_pandas/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.pyspark_to_pandas.PySparkToPandasTransformer--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.transformers import PySparkToPandasTransformer\n\npyspark_to_pandas = PySparkToPandasTransformer(\n    df=df\n)\n\nresult = pyspark_to_pandas.transform()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>PySpark DataFrame to be converted</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/pyspark_to_pandas.py</code> <pre><code>class PySparkToPandasTransformer(TransformerInterface):\n\"\"\"\n    Converts a PySpark DataFrame to a Pandas DataFrame.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.transformers import PySparkToPandasTransformer\n\n    pyspark_to_pandas = PySparkToPandasTransformer(\n        df=df\n    )\n\n    result = pyspark_to_pandas.transform()\n    ```\n\n    Parameters:\n        df (DataFrame): PySpark DataFrame to be converted\n    \"\"\"\n\n    df: PySparkDataFrame\n\n    def __init__(self, df: PySparkDataFrame) -&gt; None:\n        self.df = df\n\n    @staticmethod\n    def system_type():\n\"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_transform_validation(self):\n        return True\n\n    def post_transform_validation(self):\n        return True\n\n    def transform(self) -&gt; PandasDataFrame:\n\"\"\"\n        Returns:\n            DataFrame: A Pandas dataframe converted from a PySpark DataFrame.\n        \"\"\"\n        df = self.df.toPandas()\n        return df\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/pyspark_to_pandas/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.pyspark_to_pandas.PySparkToPandasTransformer.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/pyspark_to_pandas.py</code> <pre><code>@staticmethod\ndef system_type():\n\"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/pyspark_to_pandas/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.pyspark_to_pandas.PySparkToPandasTransformer.transform","title":"<code>transform()</code>","text":"<p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A Pandas dataframe converted from a PySpark DataFrame.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/pyspark_to_pandas.py</code> <pre><code>def transform(self) -&gt; PandasDataFrame:\n\"\"\"\n    Returns:\n        DataFrame: A Pandas dataframe converted from a PySpark DataFrame.\n    \"\"\"\n    df = self.df.toPandas()\n    return df\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/sem_json_to_pcdm/","title":"Convert SEM Json to Process Control Data Model","text":""},{"location":"sdk/code-reference/pipelines/transformers/spark/sem_json_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.sem_json_to_pcdm.SEMJsonToPCDMTransformer","title":"<code>SEMJsonToPCDMTransformer</code>","text":"<p>             Bases: <code>TransformerInterface</code></p> <p>Converts a Spark Dataframe column containing a json string created by SEM to the Process Control Data Model.</p>"},{"location":"sdk/code-reference/pipelines/transformers/spark/sem_json_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.sem_json_to_pcdm.SEMJsonToPCDMTransformer--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.transformers import SEMJsonToPCDMTransformer\n\nsem_json_to_pcdm_transformer = SEMJsonToPCDMTransformer(\n    data=df\n    source_column_name=\"body\",\n    version=10,\n    status_null_value=\"Good\",\n    change_type_value=\"insert\"\n)\n\nresult = sem_json_to_pcdm_transformer.transform()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Dataframe containing the column with SEM data</p> required <code>source_column_name</code> <code>str</code> <p>Spark Dataframe column containing the OPC Publisher Json OPC UA data</p> required <code>version</code> <code>int</code> <p>The version for the OBC field mappings. The latest version is 10.</p> required <code>status_null_value</code> <code>optional str</code> <p>If populated, will replace 'Good' in the Status column with the specified value.</p> <code>'Good'</code> <code>change_type_value</code> <code>optional str</code> <p>If populated, will replace 'insert' in the ChangeType column with the specified value.</p> <code>'insert'</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/sem_json_to_pcdm.py</code> <pre><code>class SEMJsonToPCDMTransformer(TransformerInterface):\n\"\"\"\n    Converts a Spark Dataframe column containing a json string created by SEM to the Process Control Data Model.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.transformers import SEMJsonToPCDMTransformer\n\n    sem_json_to_pcdm_transformer = SEMJsonToPCDMTransformer(\n        data=df\n        source_column_name=\"body\",\n        version=10,\n        status_null_value=\"Good\",\n        change_type_value=\"insert\"\n    )\n\n    result = sem_json_to_pcdm_transformer.transform()\n    ```\n\n    Parameters:\n        data (DataFrame): Dataframe containing the column with SEM data\n        source_column_name (str): Spark Dataframe column containing the OPC Publisher Json OPC UA data\n        version (int): The version for the OBC field mappings. The latest version is 10.\n        status_null_value (optional str): If populated, will replace 'Good' in the Status column with the specified value.\n        change_type_value (optional str): If populated, will replace 'insert' in the ChangeType column with the specified value.\n    \"\"\"\n\n    data: DataFrame\n    source_column_name: str\n    version: int\n    status_null_value: str\n    change_type_value: str\n\n    def __init__(\n        self,\n        data: DataFrame,\n        source_column_name: str,\n        version: int,\n        status_null_value: str = \"Good\",\n        change_type_value: str = \"insert\",\n    ) -&gt; None:\n        _package_version_meets_minimum(\"pyspark\", \"3.4.0\")\n        self.data = data\n        self.source_column_name = source_column_name\n        self.version = version\n        self.status_null_value = status_null_value\n        self.change_type_value = change_type_value\n\n    @staticmethod\n    def system_type():\n\"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_transform_validation(self):\n        return True\n\n    def post_transform_validation(self):\n        return True\n\n    def transform(self) -&gt; DataFrame:\n\"\"\"\n        Returns:\n            DataFrame: A dataframe with the specified column converted to PCDM\n        \"\"\"\n        if self.version == 10:\n            mapping = obc_field_mappings.OBC_FIELD_MAPPINGS_V10\n            df = (\n                self.data.withColumn(\n                    self.source_column_name,\n                    from_json(self.source_column_name, SEM_SCHEMA),\n                )\n                .select(self.source_column_name + \".readings\")\n                .melt(\n                    ids=[\"readings.resourceName\"],\n                    values=[\"readings.value\"],\n                    variableColumnName=\"var\",\n                    valueColumnName=\"value\",\n                )\n                .drop(\"var\")\n                .select(map_from_arrays(\"resourceName\", \"value\").alias(\"resourceName\"))\n                .select(\"resourceName.dID\", \"resourceName.d\", \"resourceName.t\")\n                .select(\n                    regexp_replace(col(\"t\").cast(\"string\"), \"(\\d{10})(\\d+)\", \"$1.$2\")\n                    .cast(\"double\")\n                    .alias(\"timestamp\"),\n                    \"dID\",\n                    posexplode(split(expr(\"substring(d, 2, length(d)-2)\"), \",\")),\n                )\n                .select(\n                    to_timestamp(\"timestamp\").alias(\"EventTime\"),\n                    col(\"dID\"),\n                    col(\"pos\").cast(\"string\"),\n                    col(\"col\").alias(\"Value\"),\n                )\n                .withColumn(\n                    \"TagName\",\n                    concat(\n                        col(\"dID\"),\n                        lit(\":\"),\n                        udf(lambda row: mapping[row][\"TagName\"])(col(\"pos\")),\n                    ),\n                )\n                .withColumn(\n                    \"ValueType\", udf(lambda row: mapping[row][\"ValueType\"])(col(\"pos\"))\n                )\n                .withColumn(\"Status\", lit(self.status_null_value))\n                .withColumn(\"ChangeType\", lit(self.change_type_value))\n            )\n            return df.select(\n                \"EventTime\", \"TagName\", \"Status\", \"Value\", \"ValueType\", \"ChangeType\"\n            )\n        else:\n            return logging.exception(\n                \"The wrong version was specified. Please use the latest version\"\n            )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/sem_json_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.sem_json_to_pcdm.SEMJsonToPCDMTransformer.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/sem_json_to_pcdm.py</code> <pre><code>@staticmethod\ndef system_type():\n\"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/sem_json_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.sem_json_to_pcdm.SEMJsonToPCDMTransformer.transform","title":"<code>transform()</code>","text":"<p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe with the specified column converted to PCDM</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/sem_json_to_pcdm.py</code> <pre><code>def transform(self) -&gt; DataFrame:\n\"\"\"\n    Returns:\n        DataFrame: A dataframe with the specified column converted to PCDM\n    \"\"\"\n    if self.version == 10:\n        mapping = obc_field_mappings.OBC_FIELD_MAPPINGS_V10\n        df = (\n            self.data.withColumn(\n                self.source_column_name,\n                from_json(self.source_column_name, SEM_SCHEMA),\n            )\n            .select(self.source_column_name + \".readings\")\n            .melt(\n                ids=[\"readings.resourceName\"],\n                values=[\"readings.value\"],\n                variableColumnName=\"var\",\n                valueColumnName=\"value\",\n            )\n            .drop(\"var\")\n            .select(map_from_arrays(\"resourceName\", \"value\").alias(\"resourceName\"))\n            .select(\"resourceName.dID\", \"resourceName.d\", \"resourceName.t\")\n            .select(\n                regexp_replace(col(\"t\").cast(\"string\"), \"(\\d{10})(\\d+)\", \"$1.$2\")\n                .cast(\"double\")\n                .alias(\"timestamp\"),\n                \"dID\",\n                posexplode(split(expr(\"substring(d, 2, length(d)-2)\"), \",\")),\n            )\n            .select(\n                to_timestamp(\"timestamp\").alias(\"EventTime\"),\n                col(\"dID\"),\n                col(\"pos\").cast(\"string\"),\n                col(\"col\").alias(\"Value\"),\n            )\n            .withColumn(\n                \"TagName\",\n                concat(\n                    col(\"dID\"),\n                    lit(\":\"),\n                    udf(lambda row: mapping[row][\"TagName\"])(col(\"pos\")),\n                ),\n            )\n            .withColumn(\n                \"ValueType\", udf(lambda row: mapping[row][\"ValueType\"])(col(\"pos\"))\n            )\n            .withColumn(\"Status\", lit(self.status_null_value))\n            .withColumn(\"ChangeType\", lit(self.change_type_value))\n        )\n        return df.select(\n            \"EventTime\", \"TagName\", \"Status\", \"Value\", \"ValueType\", \"ChangeType\"\n        )\n    else:\n        return logging.exception(\n            \"The wrong version was specified. Please use the latest version\"\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/ssip_pi_binary_file_to_pcdm/","title":"Convert SSIP PI Binary File data to the Process Control Data Model","text":""},{"location":"sdk/code-reference/pipelines/transformers/spark/ssip_pi_binary_file_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.ssip_pi_binary_file_to_pcdm.SSIPPIBinaryFileToPCDMTransformer","title":"<code>SSIPPIBinaryFileToPCDMTransformer</code>","text":"<p>             Bases: <code>TransformerInterface</code></p> <p>Converts a Spark DataFrame column containing binaryFile parquet data to the Process Control Data Model.</p> <p>This DataFrame should contain a path and the binary data. Typically this can be done using the Autoloader source component and specify \"binaryFile\" as the format.</p> <p>For more information about the SSIP PI Batch Connector, please see here.</p>"},{"location":"sdk/code-reference/pipelines/transformers/spark/ssip_pi_binary_file_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.ssip_pi_binary_file_to_pcdm.SSIPPIBinaryFileToPCDMTransformer--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.transformers import SSIPPIBinaryFileToPCDMTransformer\n\nssip_pi_binary_file_to_pcdm_transformer = SSIPPIBinaryFileToPCDMTransformer(\n    data=df\n)\n\nresult = ssip_pi_binary_file_to_pcdm_transformer.transform()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>DataFrame containing the path and binaryFile data</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/ssip_pi_binary_file_to_pcdm.py</code> <pre><code>class SSIPPIBinaryFileToPCDMTransformer(TransformerInterface):\n\"\"\"\n    Converts a Spark DataFrame column containing binaryFile parquet data to the Process Control Data Model.\n\n    This DataFrame should contain a path and the binary data. Typically this can be done using the Autoloader source component and specify \"binaryFile\" as the format.\n\n    For more information about the SSIP PI Batch Connector, please see [here.](https://bakerhughesc3.ai/oai-solution/shell-sensor-intelligence-platform/)\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.transformers import SSIPPIBinaryFileToPCDMTransformer\n\n    ssip_pi_binary_file_to_pcdm_transformer = SSIPPIBinaryFileToPCDMTransformer(\n        data=df\n    )\n\n    result = ssip_pi_binary_file_to_pcdm_transformer.transform()\n    ```\n\n    Parameters:\n        data (DataFrame): DataFrame containing the path and binaryFile data\n    \"\"\"\n\n    data: DataFrame\n\n    def __init__(self, data: DataFrame) -&gt; None:\n        self.data = data\n\n    @staticmethod\n    def system_type():\n\"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        libraries.add_pypi_library(get_default_package(\"pyarrow\"))\n        libraries.add_pypi_library(get_default_package(\"pandas\"))\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_transform_validation(self):\n        return True\n\n    def post_transform_validation(self):\n        return True\n\n    @staticmethod\n    def _convert_binary_to_pandas(pdf):\n        try:\n            binary_list = pdf.values.tolist()\n            binary_data = binary_list[0][3]\n            buf = pa.py_buffer(binary_data)\n            table = pq.read_table(buf)\n        except Exception:\n            return pd.DataFrame(\n                {\n                    \"EventDate\": pd.Series([], dtype=\"datetime64[ns]\"),\n                    \"TagName\": pd.Series([], dtype=\"str\"),\n                    \"EventTime\": pd.Series([], dtype=\"datetime64[ns]\"),\n                    \"Status\": pd.Series([], dtype=\"str\"),\n                    \"Value\": pd.Series([], dtype=\"str\"),\n                    \"ValueType\": pd.Series([], dtype=\"str\"),\n                    \"ChangeType\": pd.Series([], dtype=\"str\"),\n                }\n            )\n\n        value_type = str(table.schema.field(\"Value\").type)\n        if value_type == \"int16\" or value_type == \"int32\":\n            value_type = \"integer\"\n\n        output_pdf = table.to_pandas()\n\n        output_pdf[\"EventDate\"] = output_pdf[\"EventTime\"].dt.date\n        output_pdf[\"Value\"] = output_pdf[\"Value\"].astype(str)\n        output_pdf[\"ChangeType\"] = \"insert\"\n        output_pdf[\"ValueType\"] = value_type\n        output_pdf = output_pdf[\n            [\n                \"EventDate\",\n                \"TagName\",\n                \"EventTime\",\n                \"Status\",\n                \"Value\",\n                \"ValueType\",\n                \"ChangeType\",\n            ]\n        ]\n        return output_pdf\n\n    def transform(self) -&gt; DataFrame:\n\"\"\"\n        Returns:\n            DataFrame: A dataframe with the provided Binary data convert to PCDM\n        \"\"\"\n        return self.data.groupBy(\"path\").applyInPandas(\n            SSIPPIBinaryFileToPCDMTransformer._convert_binary_to_pandas,\n            schema=\"EventDate DATE, TagName STRING, EventTime TIMESTAMP, Status STRING, Value STRING, ValueType STRING, ChangeType STRING\",\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/ssip_pi_binary_file_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.ssip_pi_binary_file_to_pcdm.SSIPPIBinaryFileToPCDMTransformer.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/ssip_pi_binary_file_to_pcdm.py</code> <pre><code>@staticmethod\ndef system_type():\n\"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/ssip_pi_binary_file_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.ssip_pi_binary_file_to_pcdm.SSIPPIBinaryFileToPCDMTransformer.transform","title":"<code>transform()</code>","text":"<p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe with the provided Binary data convert to PCDM</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/ssip_pi_binary_file_to_pcdm.py</code> <pre><code>def transform(self) -&gt; DataFrame:\n\"\"\"\n    Returns:\n        DataFrame: A dataframe with the provided Binary data convert to PCDM\n    \"\"\"\n    return self.data.groupBy(\"path\").applyInPandas(\n        SSIPPIBinaryFileToPCDMTransformer._convert_binary_to_pandas,\n        schema=\"EventDate DATE, TagName STRING, EventTime TIMESTAMP, Status STRING, Value STRING, ValueType STRING, ChangeType STRING\",\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/ssip_pi_binary_json_to_pcdm/","title":"Convert SSIP PI Binary JSON data to the Process Control Data Model","text":""},{"location":"sdk/code-reference/pipelines/transformers/spark/ssip_pi_binary_json_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.ssip_pi_binary_json_to_pcdm.SSIPPIJsonStreamToPCDMTransformer","title":"<code>SSIPPIJsonStreamToPCDMTransformer</code>","text":"<p>             Bases: <code>TransformerInterface</code></p> <p>Converts a Spark DataFrame containing Binary JSON data and related Properties to the Process Control Data Model</p> <p>For more information about the SSIP PI Streaming Connector, please see here.</p>"},{"location":"sdk/code-reference/pipelines/transformers/spark/ssip_pi_binary_json_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.ssip_pi_binary_json_to_pcdm.SSIPPIJsonStreamToPCDMTransformer--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.transformers import SSIPPIJsonStreamToPCDMTransformer\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\nssip_pi_json_stream_to_pcdm_transformer = SSIPPIJsonStreamToPCDMTransformer(\n    spark=spark,\n    data=df,\n    source_column_name=\"body\",\n    properties_column_name=\"\",\n    metadata_delta_table=None\n)\n\nresult = ssip_pi_json_stream_to_pcdm_transformer.transform()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session</p> required <code>data</code> <code>DataFrame</code> <p>DataFrame containing the path and binaryFile data</p> required <code>source_column_name</code> <code>str</code> <p>Spark Dataframe column containing the Binary json data</p> required <code>properties_column_name</code> <code>str</code> <p>Spark Dataframe struct typed column containing an element with the PointType</p> required <code>metadata_delta_table</code> <code>(optional, str)</code> <p>Name of a metadata table that can be used for PointType mappings</p> <code>None</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/ssip_pi_binary_json_to_pcdm.py</code> <pre><code>class SSIPPIJsonStreamToPCDMTransformer(TransformerInterface):\n\"\"\"\n    Converts a Spark DataFrame containing Binary JSON data and related Properties to the Process Control Data Model\n\n    For more information about the SSIP PI Streaming Connector, please see [here.](https://bakerhughesc3.ai/oai-solution/shell-sensor-intelligence-platform/)\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.transformers import SSIPPIJsonStreamToPCDMTransformer\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    ssip_pi_json_stream_to_pcdm_transformer = SSIPPIJsonStreamToPCDMTransformer(\n        spark=spark,\n        data=df,\n        source_column_name=\"body\",\n        properties_column_name=\"\",\n        metadata_delta_table=None\n    )\n\n    result = ssip_pi_json_stream_to_pcdm_transformer.transform()\n    ```\n\n    Parameters:\n        spark (SparkSession): Spark Session\n        data (DataFrame): DataFrame containing the path and binaryFile data\n        source_column_name (str): Spark Dataframe column containing the Binary json data\n        properties_column_name (str): Spark Dataframe struct typed column containing an element with the PointType\n        metadata_delta_table (optional, str): Name of a metadata table that can be used for PointType mappings\n    \"\"\"\n\n    spark: SparkSession\n    data: DataFrame\n    source_column_name: str\n    properties_column_name: str\n    metadata_delta_table: str\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        data: DataFrame,\n        source_column_name: str,\n        properties_column_name: str,\n        metadata_delta_table: str = None,\n    ) -&gt; None:\n        self.spark = spark\n        self.data = data\n        self.source_column_name = source_column_name\n        self.properties_column_name = properties_column_name\n        self.metadata_delta_table = metadata_delta_table\n\n    @staticmethod\n    def system_type():\n\"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_transform_validation(self):\n        return True\n\n    def post_transform_validation(self):\n        return True\n\n    def transform(self) -&gt; DataFrame:\n\"\"\"\n        Returns:\n            DataFrame: A dataframe with the provided Binary data converted to PCDM\n        \"\"\"\n        df = (\n            self.data.withColumn(\n                self.source_column_name, col(self.source_column_name).cast(\"string\")\n            )\n            .withColumn(\n                \"EventDate\",\n                get_json_object(col(self.source_column_name), \"$.EventTime\").cast(\n                    \"date\"\n                ),\n            )\n            .withColumn(\n                \"TagName\",\n                get_json_object(col(self.source_column_name), \"$.TagName\").cast(\n                    \"string\"\n                ),\n            )\n            .withColumn(\n                \"EventTime\",\n                get_json_object(col(self.source_column_name), \"$.EventTime\").cast(\n                    \"timestamp\"\n                ),\n            )\n            .withColumn(\n                \"Status\",\n                get_json_object(col(self.source_column_name), \"$.Quality\").cast(\n                    \"string\"\n                ),\n            )\n            .withColumn(\n                \"Value\",\n                get_json_object(col(self.source_column_name), \"$.Value\").cast(\"string\"),\n            )\n            .withColumn(\n                \"PointType\", element_at(col(self.properties_column_name), \"PointType\")\n            )\n            .withColumn(\n                \"Action\",\n                element_at(col(self.properties_column_name), \"Action\").cast(\"string\"),\n            )\n        )\n\n        if self.metadata_delta_table != None:\n            metadata_df = SparkDeltaSource(\n                self.spark, {}, self.metadata_delta_table\n            ).read_batch()\n            metadata_df = metadata_df.select(\n                \"TagName\", col(\"PointType\").alias(\"MetadataPointType\")\n            )\n            df = df.join(metadata_df, (df.TagName == metadata_df.TagName), \"left\")\n            df = df.withColumn(\n                \"PointType\",\n                (when(col(\"PointType\").isNull(), col(\"MetadataPointType\"))).otherwise(\n                    col(\"PointType\")\n                ),\n            )\n\n        return (\n            df.withColumn(\n                \"ValueType\",\n                (\n                    when(col(\"PointType\") == \"Digital\", \"string\")\n                    .when(col(\"PointType\") == \"String\", \"string\")\n                    .when(col(\"PointType\") == \"Float16\", \"float\")\n                    .when(col(\"PointType\") == \"Float32\", \"float\")\n                    .when(col(\"PointType\") == \"Float64\", \"float\")\n                    .when(col(\"PointType\") == \"Int16\", \"integer\")\n                    .when(col(\"PointType\") == \"Int32\", \"integer\")\n                    .otherwise(\"string\")\n                ),\n            )\n            .selectExpr(\n                \"*\",\n                \"CASE WHEN ValueType = 'integer' THEN try_cast(Value as integer) END as Value_Integer\",\n                \"CASE WHEN ValueType = 'float' THEN try_cast(Value as float) END as Value_Float\",\n            )\n            .withColumn(\n                \"ValueType\",\n                when(\n                    (col(\"Value_Integer\").isNull()) &amp; (col(\"ValueType\") == \"integer\"),\n                    \"string\",\n                )\n                .when(\n                    (col(\"Value_Float\").isNull()) &amp; (col(\"ValueType\") == \"float\"),\n                    \"string\",\n                )\n                .otherwise(col(\"ValueType\")),\n            )\n            .withColumn(\n                \"ChangeType\",\n                (\n                    when(col(\"Action\") == \"Insert\", \"insert\")\n                    .when(col(\"Action\") == \"Add\", \"insert\")\n                    .when(col(\"Action\") == \"Delete\", \"delete\")\n                    .when(col(\"Action\") == \"Update\", \"update\")\n                    .when(col(\"Action\") == \"Refresh\", \"update\")\n                ),\n            )\n            .select(\n                col(\"EventDate\"),\n                col(\"TagName\"),\n                col(\"EventTime\"),\n                col(\"Status\"),\n                col(\"Value\"),\n                col(\"ValueType\"),\n                col(\"ChangeType\"),\n            )\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/ssip_pi_binary_json_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.ssip_pi_binary_json_to_pcdm.SSIPPIJsonStreamToPCDMTransformer.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/ssip_pi_binary_json_to_pcdm.py</code> <pre><code>@staticmethod\ndef system_type():\n\"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/ssip_pi_binary_json_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.ssip_pi_binary_json_to_pcdm.SSIPPIJsonStreamToPCDMTransformer.transform","title":"<code>transform()</code>","text":"<p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe with the provided Binary data converted to PCDM</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/ssip_pi_binary_json_to_pcdm.py</code> <pre><code>def transform(self) -&gt; DataFrame:\n\"\"\"\n    Returns:\n        DataFrame: A dataframe with the provided Binary data converted to PCDM\n    \"\"\"\n    df = (\n        self.data.withColumn(\n            self.source_column_name, col(self.source_column_name).cast(\"string\")\n        )\n        .withColumn(\n            \"EventDate\",\n            get_json_object(col(self.source_column_name), \"$.EventTime\").cast(\n                \"date\"\n            ),\n        )\n        .withColumn(\n            \"TagName\",\n            get_json_object(col(self.source_column_name), \"$.TagName\").cast(\n                \"string\"\n            ),\n        )\n        .withColumn(\n            \"EventTime\",\n            get_json_object(col(self.source_column_name), \"$.EventTime\").cast(\n                \"timestamp\"\n            ),\n        )\n        .withColumn(\n            \"Status\",\n            get_json_object(col(self.source_column_name), \"$.Quality\").cast(\n                \"string\"\n            ),\n        )\n        .withColumn(\n            \"Value\",\n            get_json_object(col(self.source_column_name), \"$.Value\").cast(\"string\"),\n        )\n        .withColumn(\n            \"PointType\", element_at(col(self.properties_column_name), \"PointType\")\n        )\n        .withColumn(\n            \"Action\",\n            element_at(col(self.properties_column_name), \"Action\").cast(\"string\"),\n        )\n    )\n\n    if self.metadata_delta_table != None:\n        metadata_df = SparkDeltaSource(\n            self.spark, {}, self.metadata_delta_table\n        ).read_batch()\n        metadata_df = metadata_df.select(\n            \"TagName\", col(\"PointType\").alias(\"MetadataPointType\")\n        )\n        df = df.join(metadata_df, (df.TagName == metadata_df.TagName), \"left\")\n        df = df.withColumn(\n            \"PointType\",\n            (when(col(\"PointType\").isNull(), col(\"MetadataPointType\"))).otherwise(\n                col(\"PointType\")\n            ),\n        )\n\n    return (\n        df.withColumn(\n            \"ValueType\",\n            (\n                when(col(\"PointType\") == \"Digital\", \"string\")\n                .when(col(\"PointType\") == \"String\", \"string\")\n                .when(col(\"PointType\") == \"Float16\", \"float\")\n                .when(col(\"PointType\") == \"Float32\", \"float\")\n                .when(col(\"PointType\") == \"Float64\", \"float\")\n                .when(col(\"PointType\") == \"Int16\", \"integer\")\n                .when(col(\"PointType\") == \"Int32\", \"integer\")\n                .otherwise(\"string\")\n            ),\n        )\n        .selectExpr(\n            \"*\",\n            \"CASE WHEN ValueType = 'integer' THEN try_cast(Value as integer) END as Value_Integer\",\n            \"CASE WHEN ValueType = 'float' THEN try_cast(Value as float) END as Value_Float\",\n        )\n        .withColumn(\n            \"ValueType\",\n            when(\n                (col(\"Value_Integer\").isNull()) &amp; (col(\"ValueType\") == \"integer\"),\n                \"string\",\n            )\n            .when(\n                (col(\"Value_Float\").isNull()) &amp; (col(\"ValueType\") == \"float\"),\n                \"string\",\n            )\n            .otherwise(col(\"ValueType\")),\n        )\n        .withColumn(\n            \"ChangeType\",\n            (\n                when(col(\"Action\") == \"Insert\", \"insert\")\n                .when(col(\"Action\") == \"Add\", \"insert\")\n                .when(col(\"Action\") == \"Delete\", \"delete\")\n                .when(col(\"Action\") == \"Update\", \"update\")\n                .when(col(\"Action\") == \"Refresh\", \"update\")\n            ),\n        )\n        .select(\n            col(\"EventDate\"),\n            col(\"TagName\"),\n            col(\"EventTime\"),\n            col(\"Status\"),\n            col(\"Value\"),\n            col(\"ValueType\"),\n            col(\"ChangeType\"),\n        )\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/ecmwf/nc_extractbase_to_weather_data_model/","title":"NC Forecast Extract Base To Weather Data Model","text":""},{"location":"sdk/code-reference/pipelines/transformers/spark/ecmwf/nc_extractbase_to_weather_data_model/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.ecmwf.nc_extractbase_to_weather_data_model.ECMWFExtractBaseToWeatherDataModel","title":"<code>ECMWFExtractBaseToWeatherDataModel</code>","text":"<p>             Bases: <code>TransformerInterface</code></p> <p>Base class for extracting forecast data downloaded in .nc format from ECMWF MARS Server.</p> <p>Parameters:</p> Name Type Description Default <code>load_path</code> <code>str</code> <p>Path to local directory where the nc files will be stored, in format \"yyyy-mm-dd_HH.nc\"</p> required <code>date_start</code> <code>str</code> <p>Start date of extraction in \"YYYY-MM-DD HH:MM:SS\" format</p> required <code>date_end</code> <code>str</code> <p>End date of extraction in \"YYYY-MM-DD HH:MM:SS\" format</p> required <code>run_frequency</code> <code>str</code> <p>Frequency format of runs to download, e.g. \"H\"</p> required <code>run_interval</code> <code>str</code> <p>Interval of runs, e.g. a run_frequency of \"H\" and run_interval of \"12\" will extract the data of the 00 and 12 run for each day.</p> required <code>lat</code> <code>DataArray</code> <p>Latitude values to extract from nc files</p> required <code>lon</code> <code>DataArray</code> <p>Longitude values to extract from nc files</p> required <code>utc</code> <code>bool = True</code> <p>Whether to convert the time to UTC or not</p> <code>True</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/ecmwf/nc_extractbase_to_weather_data_model.py</code> <pre><code>class ECMWFExtractBaseToWeatherDataModel(TransformerInterface):\n\"\"\"\n    Base class for extracting forecast data downloaded in .nc format from ECMWF MARS Server.\n\n    Args:\n        load_path (str): Path to local directory where the nc files will be stored, in format \"yyyy-mm-dd_HH.nc\"\n        date_start (str): Start date of extraction in \"YYYY-MM-DD HH:MM:SS\" format\n        date_end (str): End date of extraction in \"YYYY-MM-DD HH:MM:SS\" format\n        run_frequency (str):Frequency format of runs to download, e.g. \"H\"\n        run_interval (str): Interval of runs, e.g. a run_frequency of \"H\" and run_interval of \"12\" will extract the data of the 00 and 12 run for each day.\n        lat (DataArray): Latitude values to extract from nc files\n        lon (DataArray): Longitude values to extract from nc files\n        utc (bool = True): Whether to convert the time to UTC or not\n    \"\"\"\n\n    def __init__(\n        self,\n        load_path: str,\n        date_start: str,\n        date_end: str,\n        run_interval: str,\n        run_frequency: str,\n        lat: xr.DataArray,\n        lon: xr.DataArray,\n        utc: bool = True,\n    ):\n        self.load_path = load_path\n        self.lat = lat\n        self.lon = lon\n        self.date_start = date_start\n        self.date_end = date_end\n        self.run_frequency = run_frequency\n        self.run_interval = run_interval\n        self.utc = utc\n        self.dates = pd.date_range(\n            start=self.date_start,\n            end=self.date_end,\n            freq=self.run_interval + self.run_frequency,\n        )\n\n    @staticmethod\n    def system_type():\n\"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_transform_validation(self):\n        return True\n\n    def post_transform_validation(self):\n        return True\n\n    @staticmethod\n    def _convert_ws_tag_names(x: list):\n\"\"\"\n        Converts the tag names of wind speed from the format used in the nc files to the format used in the weather data model.\n\n        Args:\n            x (list): List of variable names of raw tags to be extracted from the nc files\n\n        Returns:\n            new_tags(list): List of variable names of raw tags to be extracted from the nc files, converted to the format used in the weather data model.\n        \"\"\"\n        convert_dict = {\n            \"10u\": \"u10\",\n            \"100u\": \"u100\",\n            \"200u\": \"u200\",\n            \"10v\": \"v10\",\n            \"100v\": \"v100\",\n            \"200v\": \"v200\",\n        }\n        new_tags = [convert_dict[i] if i in convert_dict.keys() else i for i in x]\n        return new_tags\n\n    def transform(\n        self, tag_prefix: str, variables: list, method: str = \"nearest\"\n    ) -&gt; pd.DataFrame:\n\"\"\"Extract raw data from stored nc filed downloaded via ECMWF MARS.\n\n        Args:\n            tag_prefix (str): Prefix of the tag names of raw tags to be added to the dataframe\n            variables (list): List of variable names of raw tags to be extracted from the nc files\n            method (str, optional): The method used to match latitude/longitude in xarray using .sel(), by default \"nearest\"\n\n        Returns:\n            df (pd.DataFrame): Raw data extracted with lat, lon, run_time, target_time as a pd.multiindex and variables as columns.\n        \"\"\"\n        df = []\n        # e.g. 10u variable is saved as u10 in the file...\n        vars_processed = self._convert_ws_tag_names(variables)\n\n        for i in self.dates:\n            filename = f\"{str(i.date())}_{i.hour:02}.nc\"\n            fullpath = os.path.join(self.load_path, filename)\n            ds = xr.open_dataset(fullpath)\n            tmp = (\n                ds[vars_processed]\n                .sel(latitude=self.lat, longitude=self.lon, method=method)\n                .to_dataframe()\n            )\n            tmp[\"run_time\"] = i\n            df.append(tmp)\n            ds.close()\n\n        df = pd.concat(df, axis=0)\n\n        df = df.rename_axis(\n            index={\n                \"time\": \"target_time\",\n                \"latitude\": \"lat\",\n                \"longitude\": \"lon\",\n            }\n        )\n\n        df = df.reset_index([\"lat\", \"lon\"])\n        df[[\"lat\", \"lon\"]] = df[[\"lat\", \"lon\"]].apply(\n            lambda x: np.round(x.astype(float), 5)\n        )\n\n        if \"level\" in df.index.names:\n            index_names = [\"lat\", \"lon\", \"level\", \"run_time\", \"target_time\"]\n        else:\n            index_names = [\"lat\", \"lon\", \"run_time\", \"target_time\"]\n        df = df.reset_index().set_index(index_names)\n\n        if self.utc:\n            df = df.tz_localize(\"UTC\", level=\"target_time\")\n            df = df.tz_localize(\"UTC\", level=\"run_time\")\n\n        df = df[~(df.index.duplicated(keep=\"first\"))]\n        df = df.sort_index(axis=0)\n        df = df.sort_index(axis=1)\n\n        df_new = df.reset_index()\n\n        df_new = df_new.rename(\n            columns={\n                \"lat\": \"Latitude\",\n                \"lon\": \"Longitude\",\n                \"run_time\": \"EnqueuedTime\",\n                \"target_time\": \"EventTime\",\n            }\n        )\n\n        df_new = (\n            df_new.set_index([\"Latitude\", \"Longitude\", \"EnqueuedTime\", \"EventTime\"])[\n                vars_processed\n            ]\n            .rename_axis(\"Measure\", axis=1)\n            .stack()\n            .reset_index(name=\"Value\")\n        )\n\n        df_new[\"Source\"] = \"ECMWF_MARS\"\n        df_new[\"Status\"] = \"Good\"\n        df_new[\"Latest\"] = True\n        df_new[\"EventDate\"] = pd.to_datetime(df_new[\"EventTime\"]).dt.date\n        df_new[\"TagName\"] = (\n            tag_prefix\n            + df_new[\"Latitude\"].astype(str)\n            + \"_\"\n            + df_new[\"Longitude\"].astype(str)\n            + \"_\"\n            + df_new[\"Source\"]\n            + \"_\"\n            + df_new[\"Measure\"]\n        )\n        df_final = df_new.drop(\"Measure\", axis=1)\n\n        return df_final\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/ecmwf/nc_extractbase_to_weather_data_model/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.ecmwf.nc_extractbase_to_weather_data_model.ECMWFExtractBaseToWeatherDataModel.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/ecmwf/nc_extractbase_to_weather_data_model.py</code> <pre><code>@staticmethod\ndef system_type():\n\"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/ecmwf/nc_extractbase_to_weather_data_model/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.ecmwf.nc_extractbase_to_weather_data_model.ECMWFExtractBaseToWeatherDataModel.transform","title":"<code>transform(tag_prefix, variables, method='nearest')</code>","text":"<p>Extract raw data from stored nc filed downloaded via ECMWF MARS.</p> <p>Parameters:</p> Name Type Description Default <code>tag_prefix</code> <code>str</code> <p>Prefix of the tag names of raw tags to be added to the dataframe</p> required <code>variables</code> <code>list</code> <p>List of variable names of raw tags to be extracted from the nc files</p> required <code>method</code> <code>str</code> <p>The method used to match latitude/longitude in xarray using .sel(), by default \"nearest\"</p> <code>'nearest'</code> <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>Raw data extracted with lat, lon, run_time, target_time as a pd.multiindex and variables as columns.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/ecmwf/nc_extractbase_to_weather_data_model.py</code> <pre><code>def transform(\n    self, tag_prefix: str, variables: list, method: str = \"nearest\"\n) -&gt; pd.DataFrame:\n\"\"\"Extract raw data from stored nc filed downloaded via ECMWF MARS.\n\n    Args:\n        tag_prefix (str): Prefix of the tag names of raw tags to be added to the dataframe\n        variables (list): List of variable names of raw tags to be extracted from the nc files\n        method (str, optional): The method used to match latitude/longitude in xarray using .sel(), by default \"nearest\"\n\n    Returns:\n        df (pd.DataFrame): Raw data extracted with lat, lon, run_time, target_time as a pd.multiindex and variables as columns.\n    \"\"\"\n    df = []\n    # e.g. 10u variable is saved as u10 in the file...\n    vars_processed = self._convert_ws_tag_names(variables)\n\n    for i in self.dates:\n        filename = f\"{str(i.date())}_{i.hour:02}.nc\"\n        fullpath = os.path.join(self.load_path, filename)\n        ds = xr.open_dataset(fullpath)\n        tmp = (\n            ds[vars_processed]\n            .sel(latitude=self.lat, longitude=self.lon, method=method)\n            .to_dataframe()\n        )\n        tmp[\"run_time\"] = i\n        df.append(tmp)\n        ds.close()\n\n    df = pd.concat(df, axis=0)\n\n    df = df.rename_axis(\n        index={\n            \"time\": \"target_time\",\n            \"latitude\": \"lat\",\n            \"longitude\": \"lon\",\n        }\n    )\n\n    df = df.reset_index([\"lat\", \"lon\"])\n    df[[\"lat\", \"lon\"]] = df[[\"lat\", \"lon\"]].apply(\n        lambda x: np.round(x.astype(float), 5)\n    )\n\n    if \"level\" in df.index.names:\n        index_names = [\"lat\", \"lon\", \"level\", \"run_time\", \"target_time\"]\n    else:\n        index_names = [\"lat\", \"lon\", \"run_time\", \"target_time\"]\n    df = df.reset_index().set_index(index_names)\n\n    if self.utc:\n        df = df.tz_localize(\"UTC\", level=\"target_time\")\n        df = df.tz_localize(\"UTC\", level=\"run_time\")\n\n    df = df[~(df.index.duplicated(keep=\"first\"))]\n    df = df.sort_index(axis=0)\n    df = df.sort_index(axis=1)\n\n    df_new = df.reset_index()\n\n    df_new = df_new.rename(\n        columns={\n            \"lat\": \"Latitude\",\n            \"lon\": \"Longitude\",\n            \"run_time\": \"EnqueuedTime\",\n            \"target_time\": \"EventTime\",\n        }\n    )\n\n    df_new = (\n        df_new.set_index([\"Latitude\", \"Longitude\", \"EnqueuedTime\", \"EventTime\"])[\n            vars_processed\n        ]\n        .rename_axis(\"Measure\", axis=1)\n        .stack()\n        .reset_index(name=\"Value\")\n    )\n\n    df_new[\"Source\"] = \"ECMWF_MARS\"\n    df_new[\"Status\"] = \"Good\"\n    df_new[\"Latest\"] = True\n    df_new[\"EventDate\"] = pd.to_datetime(df_new[\"EventTime\"]).dt.date\n    df_new[\"TagName\"] = (\n        tag_prefix\n        + df_new[\"Latitude\"].astype(str)\n        + \"_\"\n        + df_new[\"Longitude\"].astype(str)\n        + \"_\"\n        + df_new[\"Source\"]\n        + \"_\"\n        + df_new[\"Measure\"]\n    )\n    df_final = df_new.drop(\"Measure\", axis=1)\n\n    return df_final\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/ecmwf/nc_extractgrid_to_weather_data_model/","title":"NC Forecast Extract Grid To Weather Data Model","text":""},{"location":"sdk/code-reference/pipelines/transformers/spark/ecmwf/nc_extractgrid_to_weather_data_model/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.ecmwf.nc_extractgrid_to_weather_data_model.ECMWFExtractGridToWeatherDataModel","title":"<code>ECMWFExtractGridToWeatherDataModel</code>","text":"<p>             Bases: <code>ECMWFExtractBaseToWeatherDataModel</code></p> <p>Extract a grid from a local .nc file downloaded from ECMWF via MARS</p> <p>Parameters:</p> Name Type Description Default <code>lat_min</code> <code>float</code> <p>Minimum latitude of grid to extract</p> required <code>lat_max</code> <code>float</code> <p>Maximum latitude of grid to extract</p> required <code>lon_min</code> <code>float</code> <p>Minimum longitude of grid to extract</p> required <code>lon_max</code> <code>float</code> <p>Maximum longitude of grid to extract</p> required <code>grid_step</code> <code>float</code> <p>The grid length to use to define the grid, e.g. 0.1.</p> required <code>load_path</code> <code>str</code> <p>Path to local directory with nc files downloaded in format \"yyyy-mm-dd_HH.nc\"</p> required <code>date_start</code> <code>str</code> <p>Start date of extraction in \"YYYY-MM-DD HH:MM:SS\" format</p> required <code>date_end</code> <code>str</code> <p>End date of extraction in \"YYYY-MM-DD HH:MM:SS\" format</p> required <code>run_frequency</code> <code>str</code> <p>Frequency format of runs to download, e.g. \"H\"</p> required <code>run_interval</code> <code>str</code> <p>Interval of runs, e.g. a run_frequency of \"H\" and run_interval of \"12\" will extract the data of the 00 and 12 run for each day.</p> required <code>utc</code> <code>bool</code> <p>Add utc to the datetime indexes? Defaults to True.</p> <code>True</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/ecmwf/nc_extractgrid_to_weather_data_model.py</code> <pre><code>class ECMWFExtractGridToWeatherDataModel(ECMWFExtractBaseToWeatherDataModel):\n\"\"\"Extract a grid from a local .nc file downloaded from ECMWF via MARS\n\n    Args:\n        lat_min (float): Minimum latitude of grid to extract\n        lat_max (float): Maximum latitude of grid to extract\n        lon_min (float): Minimum longitude of grid to extract\n        lon_max (float): Maximum longitude of grid to extract\n        grid_step (float): The grid length to use to define the grid, e.g. 0.1.\n        load_path (str): Path to local directory with nc files downloaded in format \"yyyy-mm-dd_HH.nc\"\n        date_start (str): Start date of extraction in \"YYYY-MM-DD HH:MM:SS\" format\n        date_end (str): End date of extraction in \"YYYY-MM-DD HH:MM:SS\" format\n        run_frequency (str): Frequency format of runs to download, e.g. \"H\"\n        run_interval (str): Interval of runs, e.g. a run_frequency of \"H\" and run_interval of \"12\" will extract the data of the 00 and 12 run for each day.\n        utc (bool, optional): Add utc to the datetime indexes? Defaults to True.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        lat_min: float,\n        lat_max: float,\n        lon_min: float,\n        lon_max: float,\n        grid_step: float,\n        load_path: str,\n        date_start: str,\n        date_end: str,\n        run_interval: str,\n        run_frequency: str,\n        utc: bool = True,\n    ):\n        # hmm careful with floating points, this seems to work ok...\n        lat_xr = xr.DataArray(\n            np.linspace(\n                lat_min, lat_max, int(np.round((lat_max - lat_min) / grid_step)) + 1\n            ),\n            dims=[\"latitude\"],\n        )\n        lon_xr = xr.DataArray(\n            np.linspace(\n                lon_min, lon_max, int(np.round((lon_max - lon_min) / grid_step)) + 1\n            ),\n            dims=[\"longitude\"],\n        )\n\n        self.load_path = load_path\n        self.lat_min = lat_min\n        self.lat_max = lat_max\n        self.lon_min = lon_min\n        self.lon_max = lon_max\n        self.grid_step = grid_step\n        self.lat = lat_xr\n        self.lon = lon_xr\n        self.date_start = date_start\n        self.date_end = date_end\n        self.run_frequency = run_frequency\n        self.run_interval = run_interval\n        self.utc = utc\n\n        super(ECMWFExtractGridToWeatherDataModel, self).__init__(\n            lat=lat_xr,\n            lon=lon_xr,\n            load_path=load_path,\n            date_start=date_start,\n            date_end=date_end,\n            run_interval=run_interval,\n            run_frequency=run_frequency,\n            utc=utc,\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/ecmwf/nc_extractpoint_to_weather_data_model/","title":"NC Forecast Extract Point To Weather Data Model","text":""},{"location":"sdk/code-reference/pipelines/transformers/spark/ecmwf/nc_extractpoint_to_weather_data_model/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.ecmwf.nc_extractpoint_to_weather_data_model.ECMWFExtractPointToWeatherDataModel","title":"<code>ECMWFExtractPointToWeatherDataModel</code>","text":"<p>             Bases: <code>ECMWFExtractBaseToWeatherDataModel</code></p> <p>Extract a single point from a local .nc file downloaded from ECMWF via MARS</p> <p>Parameters:</p> Name Type Description Default <code>lat</code> <code>float</code> <p>Latitude of point to extract</p> required <code>lon</code> <code>float</code> <p>Longitude of point to extract</p> required <code>load_path</code> <code>str</code> <p>Path to local directory with nc files downloaded in format \"yyyy-mm-dd_HH.nc\"</p> required <code>date_start</code> <code>str</code> <p>Start date of extraction in \"YYYY-MM-DD HH:MM:SS\" format</p> required <code>date_end</code> <code>str</code> <p>End date of extraction in \"YYYY-MM-DD HH:MM:SS\" format</p> required <code>run_frequency</code> <code>str</code> <p>Frequency format of runs to download, e.g. \"H\"</p> required <code>run_interval</code> <code>str</code> <p>Interval of runs, e.g. a run_frequency of \"H\" and run_interval of \"12\" will extract the data of the 00 and 12 run for each day.</p> required <code>utc</code> <code>bool</code> <p>Add utc to the datetime indexes? Defaults to True.</p> <code>True</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/ecmwf/nc_extractpoint_to_weather_data_model.py</code> <pre><code>class ECMWFExtractPointToWeatherDataModel(ECMWFExtractBaseToWeatherDataModel):\n\"\"\"\n    Extract a single point from a local .nc file downloaded from ECMWF via MARS\n\n    Args:\n        lat (float): Latitude of point to extract\n        lon (float): Longitude of point to extract\n        load_path (str): Path to local directory with nc files downloaded in format \"yyyy-mm-dd_HH.nc\"\n        date_start (str): Start date of extraction in \"YYYY-MM-DD HH:MM:SS\" format\n        date_end (str): End date of extraction in \"YYYY-MM-DD HH:MM:SS\" format\n        run_frequency (str): Frequency format of runs to download, e.g. \"H\"\n        run_interval (str): Interval of runs, e.g. a run_frequency of \"H\" and run_interval of \"12\" will extract the data of the 00 and 12 run for each day.\n        utc (bool, optional): Add utc to the datetime indexes? Defaults to True.\n    \"\"\"\n\n    def __init__(\n        self,\n        lat: float,\n        lon: float,\n        load_path: str,\n        date_start: str,\n        date_end: str,\n        run_interval: str,\n        run_frequency: str,\n        utc: bool = True,\n    ):\n        lat_xr = xr.DataArray([lat], dims=[\"latitude\"])\n        lon_xr = xr.DataArray([lon], dims=[\"longitude\"])\n\n        self.lat = lat_xr\n        self.lon = lon_xr\n        self.load_path = load_path\n        self.date_start = date_start\n        self.date_end = date_end\n        self.run_frequency = run_frequency\n        self.run_interval = run_interval\n        self.utc = utc\n\n        super(ECMWFExtractPointToWeatherDataModel, self).__init__(\n            lat=lat_xr,\n            lon=lon_xr,\n            load_path=load_path,\n            date_start=date_start,\n            date_end=date_end,\n            run_interval=run_interval,\n            run_frequency=run_frequency,\n            utc=utc,\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/iso/ercot_to_mdm/","title":"ERCOT To Meters Data Model","text":""},{"location":"sdk/code-reference/pipelines/transformers/spark/iso/ercot_to_mdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.iso.ercot_to_mdm.ERCOTToMDMTransformer","title":"<code>ERCOTToMDMTransformer</code>","text":"<p>             Bases: <code>BaseRawToMDMTransformer</code></p> <p>Converts ERCOT Raw data into Meters Data Model.</p> <p>Please check the BaseRawToMDMTransformer for the required arguments and methods.</p>"},{"location":"sdk/code-reference/pipelines/transformers/spark/iso/ercot_to_mdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.iso.ercot_to_mdm.ERCOTToMDMTransformer--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.transformers import ERCOTToMDMTransformer\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\nercot_to_mdm_transformer = ERCOTToMDMTransformer(\n    spark=spark,\n    data=df,\n    output_type=\"usage\",\n    name=None,\n    description=None,\n    value_type=None,\n    version=None,\n    series_id=None,\n    series_parent_id=None\n)\n\nresult = ercot_to_mdm_transformer.transform()\n</code></pre> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/iso/ercot_to_mdm.py</code> <pre><code>class ERCOTToMDMTransformer(BaseRawToMDMTransformer):\n\"\"\"\n    Converts ERCOT Raw data into Meters Data Model.\n\n    Please check the BaseRawToMDMTransformer for the required arguments and methods.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.transformers import ERCOTToMDMTransformer\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    ercot_to_mdm_transformer = ERCOTToMDMTransformer(\n        spark=spark,\n        data=df,\n        output_type=\"usage\",\n        name=None,\n        description=None,\n        value_type=None,\n        version=None,\n        series_id=None,\n        series_parent_id=None\n    )\n\n    result = ercot_to_mdm_transformer.transform()\n    ```\n    \"\"\"\n\n    spark: SparkSession\n    data: DataFrame\n    input_schema = ERCOT_SCHEMA\n    uid_col = \"variable\"\n    series_id_col = \"'series_std_001'\"\n    timestamp_col = \"to_utc_timestamp(StartTime, 'America/Chicago')\"\n    interval_timestamp_col = \"Timestamp + INTERVAL 1 HOURS\"\n    value_col = \"value\"\n    series_parent_id_col = \"'series_parent_std_001'\"\n    name_col = \"'ERCOT API'\"\n    uom_col = \"'mwh'\"\n    description_col = \"'ERCOT data pulled from ERCOT ISO API'\"\n    timestamp_start_col = \"StartTime\"\n    timestamp_end_col = \"StartTime + INTERVAL 1 HOURS\"\n    time_zone_col = \"'America/Chicago'\"\n    version_col = \"'1'\"\n    series_type = SeriesType.Hour\n    model_type = ModelType.Default\n    value_type = ValueType.Usage\n    properties_col = \"null\"\n\n    def _pre_process(self) -&gt; DataFrame:\n        df: DataFrame = super(ERCOTToMDMTransformer, self)._pre_process()\n        df = melt(\n            df,\n            id_vars=[\"Date\", \"HourEnding\", \"DstFlag\"],\n            value_vars=[\n                \"Coast\",\n                \"East\",\n                \"FarWest\",\n                \"North\",\n                \"NorthCentral\",\n                \"SouthCentral\",\n                \"Southern\",\n                \"West\",\n                \"SystemTotal\",\n            ],\n        )\n        df = df.withColumn(\n            \"StartTime\",\n            F.expr(\n                \"Date + MAKE_INTERVAL(0,0,0,0,cast(split(HourEnding,':')[0] as integer),0,0)\"\n            ),\n        )\n        return df\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/iso/miso_to_mdm/","title":"MISO To Meters Data Model","text":""},{"location":"sdk/code-reference/pipelines/transformers/spark/iso/miso_to_mdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.iso.miso_to_mdm.MISOToMDMTransformer","title":"<code>MISOToMDMTransformer</code>","text":"<p>             Bases: <code>BaseRawToMDMTransformer</code></p> <p>Converts MISO Raw data into Meters Data Model.</p> <p>Please check the BaseRawToMDMTransformer for the required arguments and methods.</p>"},{"location":"sdk/code-reference/pipelines/transformers/spark/iso/miso_to_mdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.iso.miso_to_mdm.MISOToMDMTransformer--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.transformers import MISOToMDMTransformer\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\nmiso_to_mdm_transformer = MISOToMDMTransformer(\n    spark=spark,\n    data=df,\n    output_type=\"usage\",\n    name=None,\n    description=None,\n    value_type=None,\n    version=None,\n    series_id=None,\n    series_parent_id=None\n)\n\nresult = miso_to_mdm_transformer.transform()\n</code></pre> BaseRawToMDMTransformer Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/iso/miso_to_mdm.py</code> <pre><code>class MISOToMDMTransformer(BaseRawToMDMTransformer):\n\"\"\"\n    Converts MISO Raw data into Meters Data Model.\n\n    Please check the BaseRawToMDMTransformer for the required arguments and methods.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.transformers import MISOToMDMTransformer\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    miso_to_mdm_transformer = MISOToMDMTransformer(\n        spark=spark,\n        data=df,\n        output_type=\"usage\",\n        name=None,\n        description=None,\n        value_type=None,\n        version=None,\n        series_id=None,\n        series_parent_id=None\n    )\n\n    result = miso_to_mdm_transformer.transform()\n    ```\n\n    BaseRawToMDMTransformer:\n        ::: src.sdk.python.rtdip_sdk.pipelines.transformers.spark.base_raw_to_mdm\n    \"\"\"\n\n    spark: SparkSession\n    data: DataFrame\n    input_schema = MISO_SCHEMA\n    uid_col = \"variable\"\n    series_id_col = \"'series_std_001'\"\n    timestamp_col = \"to_utc_timestamp(Datetime, 'US/Central')\"\n    interval_timestamp_col = \"Timestamp + INTERVAL 1 HOURS\"\n    value_col = \"bround(value, 2)\"\n    series_parent_id_col = \"'series_parent_std_001'\"\n    name_col = \"'Miso API'\"\n    uom_col = \"'mwh'\"\n    description_col = \"'Miso data pulled from Miso ISO API'\"\n    timestamp_start_col = \"Datetime\"\n    timestamp_end_col = \"Datetime + INTERVAL 1 HOURS\"\n    time_zone_col = \"'US/Central'\"\n    version_col = \"'1'\"\n    series_type = SeriesType.Hour\n    model_type = ModelType.Default\n    value_type = ValueType.Usage\n    properties_col = \"null\"\n\n    def _pre_process(self) -&gt; DataFrame:\n        df: DataFrame = super(MISOToMDMTransformer, self)._pre_process()\n        df = melt(\n            df,\n            id_vars=[\"Datetime\"],\n            value_vars=[\n                \"Lrz1\",\n                \"Lrz2_7\",\n                \"Lrz3_5\",\n                \"Lrz4\",\n                \"Lrz6\",\n                \"Lrz8_9_10\",\n                \"Miso\",\n            ],\n        )\n        return df\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/iso/miso_to_mdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.base_raw_to_mdm.BaseRawToMDMTransformer","title":"<code>BaseRawToMDMTransformer</code>","text":"<p>             Bases: <code>TransformerInterface</code></p> <p>Base class for all the Raw to Meters Data Model Transformers.</p> Meters Data Model requires two outputs <ul> <li><code>UsageData</code> : To store measurement(value) as timeseries data.</li> <li><code>MetaData</code> : To store meters related meta information.</li> </ul> <p>It supports the generation of both the outputs as they share some common properties.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance.</p> required <code>data</code> <code>DataFrame</code> <p>Dataframe containing the raw MISO data.</p> required <code>output_type</code> <code>str</code> <p>Must be one of <code>usage</code> or <code>meta</code>.</p> required <code>name</code> <code>str</code> <p>Set this to override default <code>name</code> column.</p> <code>None</code> <code>description</code> <code>str</code> <p>Set this to override default <code>description</code> column.</p> <code>None</code> <code>value_type</code> <code>ValueType</code> <p>Set this to override default <code>value_type</code> column.</p> <code>None</code> <code>version</code> <code>str</code> <p>Set this to override default <code>version</code> column.</p> <code>None</code> <code>series_id</code> <code>str</code> <p>Set this to override default <code>series_id</code> column.</p> <code>None</code> <code>series_parent_id</code> <code>str</code> <p>Set this to override default <code>series_parent_id</code> column.</p> <code>None</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/base_raw_to_mdm.py</code> <pre><code>class BaseRawToMDMTransformer(TransformerInterface):\n\"\"\"\n    Base class for all the Raw to Meters Data Model Transformers.\n\n    Meters Data Model requires two outputs:\n        - `UsageData` : To store measurement(value) as timeseries data.\n        - `MetaData` : To store meters related meta information.\n\n    It supports the generation of both the outputs as they share some common properties.\n\n    Parameters:\n        spark (SparkSession): Spark Session instance.\n        data (DataFrame): Dataframe containing the raw MISO data.\n        output_type (str): Must be one of `usage` or `meta`.\n        name (str): Set this to override default `name` column.\n        description (str): Set this to override default `description` column.\n        value_type (ValueType): Set this to override default `value_type` column.\n        version (str): Set this to override default `version` column.\n        series_id (str): Set this to override default `series_id` column.\n        series_parent_id (str): Set this to override default `series_parent_id` column.\n    \"\"\"\n\n    spark: SparkSession\n    data: DataFrame\n    output_type: str\n    input_schema: StructType\n    target_schema: StructType\n    uid_col: str\n    series_id_col: str\n    timestamp_col: str\n    interval_timestamp_col: str\n    value_col: str\n    series_parent_id_col: str\n    name_col: str\n    uom_col: str\n    description_col: str\n    timestamp_start_col: str\n    timestamp_end_col: str\n    time_zone_col: str\n    version_col: str\n    series_type: SeriesType\n    model_type: ModelType\n    value_type: ValueType\n    properties_col: str\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        data: DataFrame,\n        output_type: str,\n        name: str = None,\n        description: str = None,\n        value_type: ValueType = None,\n        version: str = None,\n        series_id: str = None,\n        series_parent_id: str = None,\n    ):\n        self.spark = spark\n        self.data = data\n        self.output_type = output_type\n        self.name = name if name is not None else self.name_col\n        self.description = (\n            description if description is not None else self.description_col\n        )\n        self.value_type = value_type if value_type is not None else self.value_type\n        self.version = version if version is not None else self.version_col\n        self.series_id = series_id if series_id is not None else self.series_id_col\n        self.series_parent_id = (\n            series_parent_id\n            if series_parent_id is not None\n            else self.series_parent_id_col\n        )\n\n    @staticmethod\n    def system_type():\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_transform_validation(self) -&gt; bool:\n        valid_output_types = [\"usage\", \"meta\"]\n        if self.output_type not in valid_output_types:\n            raise ValueError(\n                f\"Invalid output_type `{self.output_type}` given. Must be one of {valid_output_types}\"\n            )\n\n        assert str(self.data.schema) == str(self.input_schema)\n        assert type(self.series_type).__name__ == SeriesType.__name__\n        assert type(self.model_type).__name__ == ModelType.__name__\n        assert type(self.value_type).__name__ == ValueType.__name__\n        return True\n\n    def post_transform_validation(self) -&gt; bool:\n        assert str(self.data.schema) == str(self.target_schema)\n        return True\n\n    def _get_transformed_df(self) -&gt; DataFrame:\n        if self.output_type == \"usage\":\n            self.target_schema = MDM_USAGE_SCHEMA\n            return self._get_usage_transformed_df()\n        else:\n            self.target_schema = MDM_META_SCHEMA\n            return self._get_meta_transformed_df()\n\n    def _convert_into_target_schema(self) -&gt; None:\n\"\"\"\n        Converts a Spark DataFrame structure into new structure based on the Target Schema.\n\n        Returns: Nothing.\n\n        \"\"\"\n\n        df: DataFrame = self.data\n        df = df.select(self.target_schema.names)\n\n        for field in self.target_schema.fields:\n            df = df.withColumn(field.name, col(field.name).cast(field.dataType))\n\n        self.data = self.spark.createDataFrame(df.rdd, self.target_schema)\n\n    def transform(self) -&gt; DataFrame:\n\"\"\"\n        Returns:\n            DataFrame: A dataframe with the raw data converted into MDM.\n        \"\"\"\n\n        self.pre_transform_validation()\n        self.data = self._get_transformed_df()\n        self._convert_into_target_schema()\n        self.post_transform_validation()\n\n        return self.data\n\n    def _add_uid_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Uid\", expr(self.uid_col))\n\n    def _add_series_id_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"SeriesId\", expr(self.series_id))\n\n    def _add_timestamp_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Timestamp\", expr(self.timestamp_col))\n\n    def _add_interval_timestamp_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"IntervalTimestamp\", expr(self.interval_timestamp_col))\n\n    def _add_value_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Value\", expr(self.value_col))\n\n    def _add_series_parent_id_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"SeriesParentId\", expr(self.series_parent_id))\n\n    def _add_name_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Name\", expr(self.name))\n\n    def _add_uom_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Uom\", expr(self.uom_col))\n\n    def _add_description_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Description\", expr(self.description))\n\n    def _add_timestamp_start_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"TimestampStart\", expr(self.timestamp_start_col))\n\n    def _add_timestamp_end_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"TimestampEnd\", expr(self.timestamp_end_col))\n\n    def _add_time_zone_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Timezone\", expr(self.time_zone_col))\n\n    def _add_version_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Version\", expr(self.version))\n\n    def _add_series_type_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"SeriesType\", lit(self.series_type.value))\n\n    def _add_model_type_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"ModelType\", lit(self.model_type.value))\n\n    def _add_value_type_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"ValueType\", lit(self.value_type.value))\n\n    def _add_properties_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Properties\", expr(self.properties_col))\n\n    def _pre_process(self) -&gt; DataFrame:\n        return self.data\n\n    @staticmethod\n    def _post_process(df: DataFrame) -&gt; DataFrame:\n        return df\n\n    def _get_usage_transformed_df(self) -&gt; DataFrame:\n        df = self._pre_process()\n\n        df = self._add_uid_column(df)\n        df = self._add_series_id_column(df)\n        df = self._add_timestamp_column(df)\n        df = self._add_interval_timestamp_column(df)\n        df = self._add_value_column(df)\n\n        df = self._post_process(df)\n\n        return df\n\n    def _get_meta_transformed_df(self) -&gt; DataFrame:\n        df = self._pre_process()\n\n        df = self._add_uid_column(df)\n        df = self._add_series_id_column(df)\n        df = self._add_series_parent_id_column(df)\n        df = self._add_name_column(df)\n        df = self._add_uom_column(df)\n        df = self._add_description_column(df)\n        df = self._add_timestamp_start_column(df)\n        df = self._add_timestamp_end_column(df)\n        df = self._add_time_zone_column(df)\n        df = self._add_version_column(df)\n        df = self._add_series_type_column(df)\n        df = self._add_model_type_column(df)\n        df = self._add_value_type_column(df)\n        df = self._add_properties_column(df)\n\n        df = self._post_process(df)\n\n        return df\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/iso/miso_to_mdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.base_raw_to_mdm.BaseRawToMDMTransformer.transform","title":"<code>transform()</code>","text":"<p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe with the raw data converted into MDM.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/base_raw_to_mdm.py</code> <pre><code>def transform(self) -&gt; DataFrame:\n\"\"\"\n    Returns:\n        DataFrame: A dataframe with the raw data converted into MDM.\n    \"\"\"\n\n    self.pre_transform_validation()\n    self.data = self._get_transformed_df()\n    self._convert_into_target_schema()\n    self.post_transform_validation()\n\n    return self.data\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/iso/pjm_to_mdm/","title":"PJM To Meters Data Model","text":""},{"location":"sdk/code-reference/pipelines/transformers/spark/iso/pjm_to_mdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.iso.pjm_to_mdm.PJMToMDMTransformer","title":"<code>PJMToMDMTransformer</code>","text":"<p>             Bases: <code>BaseRawToMDMTransformer</code></p> <p>Converts PJM Raw data into Meters Data Model.</p> <p>Please check the BaseRawToMDMTransformer for the required arguments and methods.</p>"},{"location":"sdk/code-reference/pipelines/transformers/spark/iso/pjm_to_mdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.iso.pjm_to_mdm.PJMToMDMTransformer--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.transformers import PJMToMDMTransformer\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\npjm_to_mdm_transformer = PJMToMDMTransformer(\n    spark=spark,\n    data=df,\n    output_type=\"usage\",\n    name=None,\n    description=None,\n    value_type=None,\n    version=None,\n    series_id=None,\n    series_parent_id=None\n)\n\nresult = pjm_to_mdm_transformer.transform()\n</code></pre> BaseRawToMDMTransformer Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/iso/pjm_to_mdm.py</code> <pre><code>class PJMToMDMTransformer(BaseRawToMDMTransformer):\n\"\"\"\n    Converts PJM Raw data into Meters Data Model.\n\n    Please check the BaseRawToMDMTransformer for the required arguments and methods.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.transformers import PJMToMDMTransformer\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    pjm_to_mdm_transformer = PJMToMDMTransformer(\n        spark=spark,\n        data=df,\n        output_type=\"usage\",\n        name=None,\n        description=None,\n        value_type=None,\n        version=None,\n        series_id=None,\n        series_parent_id=None\n    )\n\n    result = pjm_to_mdm_transformer.transform()\n    ```\n\n    BaseRawToMDMTransformer:\n        ::: src.sdk.python.rtdip_sdk.pipelines.transformers.spark.base_raw_to_mdm\n    \"\"\"\n\n    spark: SparkSession\n    data: DataFrame\n    input_schema = PJM_SCHEMA\n    uid_col = \"Zone\"\n    series_id_col = \"'series_std_001'\"\n    timestamp_col = \"to_utc_timestamp(StartTime, 'America/New_York')\"\n    interval_timestamp_col = \"Timestamp + INTERVAL 1 HOURS\"\n    value_col = \"bround(Load, 2)\"\n    series_parent_id_col = \"'series_parent_std_001'\"\n    name_col = \"'PJM API'\"\n    uom_col = \"'mwh'\"\n    description_col = \"'PJM data pulled from PJM ISO API'\"\n    timestamp_start_col = \"StartTime\"\n    timestamp_end_col = \"StartTime + INTERVAL 1 HOURS\"\n    time_zone_col = \"'America/New_York'\"\n    version_col = \"'1'\"\n    series_type = SeriesType.Hour\n    model_type = ModelType.Default\n    value_type = ValueType.Usage\n    properties_col = \"null\"\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/iso/pjm_to_mdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.base_raw_to_mdm.BaseRawToMDMTransformer","title":"<code>BaseRawToMDMTransformer</code>","text":"<p>             Bases: <code>TransformerInterface</code></p> <p>Base class for all the Raw to Meters Data Model Transformers.</p> Meters Data Model requires two outputs <ul> <li><code>UsageData</code> : To store measurement(value) as timeseries data.</li> <li><code>MetaData</code> : To store meters related meta information.</li> </ul> <p>It supports the generation of both the outputs as they share some common properties.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance.</p> required <code>data</code> <code>DataFrame</code> <p>Dataframe containing the raw MISO data.</p> required <code>output_type</code> <code>str</code> <p>Must be one of <code>usage</code> or <code>meta</code>.</p> required <code>name</code> <code>str</code> <p>Set this to override default <code>name</code> column.</p> <code>None</code> <code>description</code> <code>str</code> <p>Set this to override default <code>description</code> column.</p> <code>None</code> <code>value_type</code> <code>ValueType</code> <p>Set this to override default <code>value_type</code> column.</p> <code>None</code> <code>version</code> <code>str</code> <p>Set this to override default <code>version</code> column.</p> <code>None</code> <code>series_id</code> <code>str</code> <p>Set this to override default <code>series_id</code> column.</p> <code>None</code> <code>series_parent_id</code> <code>str</code> <p>Set this to override default <code>series_parent_id</code> column.</p> <code>None</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/base_raw_to_mdm.py</code> <pre><code>class BaseRawToMDMTransformer(TransformerInterface):\n\"\"\"\n    Base class for all the Raw to Meters Data Model Transformers.\n\n    Meters Data Model requires two outputs:\n        - `UsageData` : To store measurement(value) as timeseries data.\n        - `MetaData` : To store meters related meta information.\n\n    It supports the generation of both the outputs as they share some common properties.\n\n    Parameters:\n        spark (SparkSession): Spark Session instance.\n        data (DataFrame): Dataframe containing the raw MISO data.\n        output_type (str): Must be one of `usage` or `meta`.\n        name (str): Set this to override default `name` column.\n        description (str): Set this to override default `description` column.\n        value_type (ValueType): Set this to override default `value_type` column.\n        version (str): Set this to override default `version` column.\n        series_id (str): Set this to override default `series_id` column.\n        series_parent_id (str): Set this to override default `series_parent_id` column.\n    \"\"\"\n\n    spark: SparkSession\n    data: DataFrame\n    output_type: str\n    input_schema: StructType\n    target_schema: StructType\n    uid_col: str\n    series_id_col: str\n    timestamp_col: str\n    interval_timestamp_col: str\n    value_col: str\n    series_parent_id_col: str\n    name_col: str\n    uom_col: str\n    description_col: str\n    timestamp_start_col: str\n    timestamp_end_col: str\n    time_zone_col: str\n    version_col: str\n    series_type: SeriesType\n    model_type: ModelType\n    value_type: ValueType\n    properties_col: str\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        data: DataFrame,\n        output_type: str,\n        name: str = None,\n        description: str = None,\n        value_type: ValueType = None,\n        version: str = None,\n        series_id: str = None,\n        series_parent_id: str = None,\n    ):\n        self.spark = spark\n        self.data = data\n        self.output_type = output_type\n        self.name = name if name is not None else self.name_col\n        self.description = (\n            description if description is not None else self.description_col\n        )\n        self.value_type = value_type if value_type is not None else self.value_type\n        self.version = version if version is not None else self.version_col\n        self.series_id = series_id if series_id is not None else self.series_id_col\n        self.series_parent_id = (\n            series_parent_id\n            if series_parent_id is not None\n            else self.series_parent_id_col\n        )\n\n    @staticmethod\n    def system_type():\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_transform_validation(self) -&gt; bool:\n        valid_output_types = [\"usage\", \"meta\"]\n        if self.output_type not in valid_output_types:\n            raise ValueError(\n                f\"Invalid output_type `{self.output_type}` given. Must be one of {valid_output_types}\"\n            )\n\n        assert str(self.data.schema) == str(self.input_schema)\n        assert type(self.series_type).__name__ == SeriesType.__name__\n        assert type(self.model_type).__name__ == ModelType.__name__\n        assert type(self.value_type).__name__ == ValueType.__name__\n        return True\n\n    def post_transform_validation(self) -&gt; bool:\n        assert str(self.data.schema) == str(self.target_schema)\n        return True\n\n    def _get_transformed_df(self) -&gt; DataFrame:\n        if self.output_type == \"usage\":\n            self.target_schema = MDM_USAGE_SCHEMA\n            return self._get_usage_transformed_df()\n        else:\n            self.target_schema = MDM_META_SCHEMA\n            return self._get_meta_transformed_df()\n\n    def _convert_into_target_schema(self) -&gt; None:\n\"\"\"\n        Converts a Spark DataFrame structure into new structure based on the Target Schema.\n\n        Returns: Nothing.\n\n        \"\"\"\n\n        df: DataFrame = self.data\n        df = df.select(self.target_schema.names)\n\n        for field in self.target_schema.fields:\n            df = df.withColumn(field.name, col(field.name).cast(field.dataType))\n\n        self.data = self.spark.createDataFrame(df.rdd, self.target_schema)\n\n    def transform(self) -&gt; DataFrame:\n\"\"\"\n        Returns:\n            DataFrame: A dataframe with the raw data converted into MDM.\n        \"\"\"\n\n        self.pre_transform_validation()\n        self.data = self._get_transformed_df()\n        self._convert_into_target_schema()\n        self.post_transform_validation()\n\n        return self.data\n\n    def _add_uid_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Uid\", expr(self.uid_col))\n\n    def _add_series_id_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"SeriesId\", expr(self.series_id))\n\n    def _add_timestamp_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Timestamp\", expr(self.timestamp_col))\n\n    def _add_interval_timestamp_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"IntervalTimestamp\", expr(self.interval_timestamp_col))\n\n    def _add_value_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Value\", expr(self.value_col))\n\n    def _add_series_parent_id_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"SeriesParentId\", expr(self.series_parent_id))\n\n    def _add_name_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Name\", expr(self.name))\n\n    def _add_uom_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Uom\", expr(self.uom_col))\n\n    def _add_description_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Description\", expr(self.description))\n\n    def _add_timestamp_start_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"TimestampStart\", expr(self.timestamp_start_col))\n\n    def _add_timestamp_end_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"TimestampEnd\", expr(self.timestamp_end_col))\n\n    def _add_time_zone_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Timezone\", expr(self.time_zone_col))\n\n    def _add_version_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Version\", expr(self.version))\n\n    def _add_series_type_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"SeriesType\", lit(self.series_type.value))\n\n    def _add_model_type_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"ModelType\", lit(self.model_type.value))\n\n    def _add_value_type_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"ValueType\", lit(self.value_type.value))\n\n    def _add_properties_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Properties\", expr(self.properties_col))\n\n    def _pre_process(self) -&gt; DataFrame:\n        return self.data\n\n    @staticmethod\n    def _post_process(df: DataFrame) -&gt; DataFrame:\n        return df\n\n    def _get_usage_transformed_df(self) -&gt; DataFrame:\n        df = self._pre_process()\n\n        df = self._add_uid_column(df)\n        df = self._add_series_id_column(df)\n        df = self._add_timestamp_column(df)\n        df = self._add_interval_timestamp_column(df)\n        df = self._add_value_column(df)\n\n        df = self._post_process(df)\n\n        return df\n\n    def _get_meta_transformed_df(self) -&gt; DataFrame:\n        df = self._pre_process()\n\n        df = self._add_uid_column(df)\n        df = self._add_series_id_column(df)\n        df = self._add_series_parent_id_column(df)\n        df = self._add_name_column(df)\n        df = self._add_uom_column(df)\n        df = self._add_description_column(df)\n        df = self._add_timestamp_start_column(df)\n        df = self._add_timestamp_end_column(df)\n        df = self._add_time_zone_column(df)\n        df = self._add_version_column(df)\n        df = self._add_series_type_column(df)\n        df = self._add_model_type_column(df)\n        df = self._add_value_type_column(df)\n        df = self._add_properties_column(df)\n\n        df = self._post_process(df)\n\n        return df\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/iso/pjm_to_mdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.base_raw_to_mdm.BaseRawToMDMTransformer.transform","title":"<code>transform()</code>","text":"<p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe with the raw data converted into MDM.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/base_raw_to_mdm.py</code> <pre><code>def transform(self) -&gt; DataFrame:\n\"\"\"\n    Returns:\n        DataFrame: A dataframe with the raw data converted into MDM.\n    \"\"\"\n\n    self.pre_transform_validation()\n    self.data = self._get_transformed_df()\n    self._convert_into_target_schema()\n    self.post_transform_validation()\n\n    return self.data\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/the_weather_company/raw_forecast_to_weather_data_model/","title":"Convert Forecast Raw JSON data to the Weather Data Model","text":""},{"location":"sdk/code-reference/pipelines/transformers/spark/the_weather_company/raw_forecast_to_weather_data_model/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.the_weather_company.raw_forecast_to_weather_data_model.RawForecastToWeatherDataModel","title":"<code>RawForecastToWeatherDataModel</code>","text":"<p>             Bases: <code>TransformerInterface</code></p> <p>Converts a raw forecast into weather data model.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance.</p> required <code>data</code> <code>DataFrame</code> <p>Dataframe to be transformed</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/the_weather_company/raw_forecast_to_weather_data_model.py</code> <pre><code>class RawForecastToWeatherDataModel(TransformerInterface):\n\"\"\"\n    Converts a raw forecast into weather data model.\n\n    Parameters:\n        spark (SparkSession): Spark Session instance.\n        data (DataFrame): Dataframe to be transformed\n    \"\"\"\n\n    spark: SparkSession\n    data: DataFrame\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        data: DataFrame,\n    ) -&gt; None:\n        self.spark = spark\n        self.data = data\n        self.target_schema = WEATHER_DATA_MODEL\n\n    @staticmethod\n    def system_type():\n\"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_transform_validation(self):\n        return True\n\n    def post_transform_validation(self) -&gt; bool:\n        assert str(self.data.schema) == str(self.target_schema)\n        return True\n\n    def _convert_into_target_schema(self) -&gt; None:\n\"\"\"\n        Converts a Spark DataFrame structure into new structure based on the Target Schema.\n\n        Returns: Nothing.\n\n        \"\"\"\n\n        df: DataFrame = self.data\n        df = df.select(self.target_schema.names)\n\n        for field in self.target_schema.fields:\n            df = df.withColumn(field.name, col(field.name).cast(field.dataType))\n\n        self.data = self.spark.createDataFrame(df.rdd, self.target_schema)\n\n    def transform(self) -&gt; DataFrame:\n\"\"\"\n        Returns:\n            DataFrame: A Forecast dataframe converted into Weather Data Model\n        \"\"\"\n\n        self.pre_transform_validation()\n\n        processed_date = datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\")\n\n        df = (\n            self.data.withColumn(\"WeatherDay\", substring(\"FcstValidLocal\", 0, 10))\n            .withColumn(\n                \"WeatherHour\",\n                (substring(\"FcstValidLocal\", 12, 2).cast(IntegerType()) + 1),\n            )\n            .withColumn(\"WeatherTimezoneOffset\", substring(\"FcstValidLocal\", 20, 5))\n            .withColumn(\"WeatherType\", lit(\"F\"))\n            .withColumn(\"ProcessedDate\", lit(processed_date))\n            .withColumnRenamed(\"Temp\", \"Temperature\")\n            .withColumnRenamed(\"Dewpt\", \"DewPoint\")\n            .withColumnRenamed(\"Rh\", \"Humidity\")\n            .withColumnRenamed(\"Hi\", \"HeatIndex\")\n            .withColumnRenamed(\"Wc\", \"WindChill\")\n            .withColumnRenamed(\"Wdir\", \"WindDirection\")\n            .withColumnRenamed(\"Wspd\", \"WindSpeed\")\n            .withColumnRenamed(\"Clds\", \"CloudCover\")\n            .withColumn(\"WetBulbTemp\", lit(\"\"))\n            .withColumn(\"SolarIrradiance\", lit(\"\"))\n            .withColumnRenamed(\"Qpf\", \"Precipitation\")\n            .withColumnRenamed(\"DayInd\", \"DayOrNight\")\n            .withColumnRenamed(\"Dow\", \"DayOfWeek\")\n            .withColumnRenamed(\"Gust\", \"WindGust\")\n            .withColumnRenamed(\"Mslp\", \"MslPressure\")\n            .withColumnRenamed(\"Num\", \"ForecastDayNum\")\n            .withColumnRenamed(\"Pop\", \"PropOfPrecip\")\n            .withColumnRenamed(\"PrecipType\", \"PrecipType\")\n            .withColumnRenamed(\"SnowQpf\", \"SnowAccumulation\")\n            .withColumnRenamed(\"UvIndex\", \"UvIndex\")\n            .withColumnRenamed(\"Vis\", \"Visibility\")\n        )\n\n        columns = df.columns\n        for column in columns:\n            df = df.withColumn(\n                column, when(col(column) == \"\", lit(None)).otherwise(col(column))\n            )\n\n        self.data = df\n        self._convert_into_target_schema()\n        self.post_transform_validation()\n\n        return self.data\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/the_weather_company/raw_forecast_to_weather_data_model/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.the_weather_company.raw_forecast_to_weather_data_model.RawForecastToWeatherDataModel.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/the_weather_company/raw_forecast_to_weather_data_model.py</code> <pre><code>@staticmethod\ndef system_type():\n\"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/the_weather_company/raw_forecast_to_weather_data_model/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.the_weather_company.raw_forecast_to_weather_data_model.RawForecastToWeatherDataModel.transform","title":"<code>transform()</code>","text":"<p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A Forecast dataframe converted into Weather Data Model</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/the_weather_company/raw_forecast_to_weather_data_model.py</code> <pre><code>def transform(self) -&gt; DataFrame:\n\"\"\"\n    Returns:\n        DataFrame: A Forecast dataframe converted into Weather Data Model\n    \"\"\"\n\n    self.pre_transform_validation()\n\n    processed_date = datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\")\n\n    df = (\n        self.data.withColumn(\"WeatherDay\", substring(\"FcstValidLocal\", 0, 10))\n        .withColumn(\n            \"WeatherHour\",\n            (substring(\"FcstValidLocal\", 12, 2).cast(IntegerType()) + 1),\n        )\n        .withColumn(\"WeatherTimezoneOffset\", substring(\"FcstValidLocal\", 20, 5))\n        .withColumn(\"WeatherType\", lit(\"F\"))\n        .withColumn(\"ProcessedDate\", lit(processed_date))\n        .withColumnRenamed(\"Temp\", \"Temperature\")\n        .withColumnRenamed(\"Dewpt\", \"DewPoint\")\n        .withColumnRenamed(\"Rh\", \"Humidity\")\n        .withColumnRenamed(\"Hi\", \"HeatIndex\")\n        .withColumnRenamed(\"Wc\", \"WindChill\")\n        .withColumnRenamed(\"Wdir\", \"WindDirection\")\n        .withColumnRenamed(\"Wspd\", \"WindSpeed\")\n        .withColumnRenamed(\"Clds\", \"CloudCover\")\n        .withColumn(\"WetBulbTemp\", lit(\"\"))\n        .withColumn(\"SolarIrradiance\", lit(\"\"))\n        .withColumnRenamed(\"Qpf\", \"Precipitation\")\n        .withColumnRenamed(\"DayInd\", \"DayOrNight\")\n        .withColumnRenamed(\"Dow\", \"DayOfWeek\")\n        .withColumnRenamed(\"Gust\", \"WindGust\")\n        .withColumnRenamed(\"Mslp\", \"MslPressure\")\n        .withColumnRenamed(\"Num\", \"ForecastDayNum\")\n        .withColumnRenamed(\"Pop\", \"PropOfPrecip\")\n        .withColumnRenamed(\"PrecipType\", \"PrecipType\")\n        .withColumnRenamed(\"SnowQpf\", \"SnowAccumulation\")\n        .withColumnRenamed(\"UvIndex\", \"UvIndex\")\n        .withColumnRenamed(\"Vis\", \"Visibility\")\n    )\n\n    columns = df.columns\n    for column in columns:\n        df = df.withColumn(\n            column, when(col(column) == \"\", lit(None)).otherwise(col(column))\n        )\n\n    self.data = df\n    self._convert_into_target_schema()\n    self.post_transform_validation()\n\n    return self.data\n</code></pre>"},{"location":"sdk/code-reference/pipelines/utilities/aws/s3_bucket_policy/","title":"S3 Bucket Policy","text":""},{"location":"sdk/code-reference/pipelines/utilities/aws/s3_bucket_policy/#src.sdk.python.rtdip_sdk.pipelines.utilities.aws.s3_bucket_policy.S3BucketPolicyUtility","title":"<code>S3BucketPolicyUtility</code>","text":"<p>             Bases: <code>UtilitiesInterface</code></p> <p>Assigns an IAM Bucket Policy to an S3 Bucket.</p>"},{"location":"sdk/code-reference/pipelines/utilities/aws/s3_bucket_policy/#src.sdk.python.rtdip_sdk.pipelines.utilities.aws.s3_bucket_policy.S3BucketPolicyUtility--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.utilities import S3BucketPolicyUtility\n\ns3_bucket_policy_utility = S3BucketPolicyUtility(\n    bucket_name=\"YOUR-BUCKET-NAME\",\n    aws_access_key_id=\"YOUR-AWS-ACCESS-KEY\",\n    aws_secret_access_key=\"YOUR-AWS-SECRET-ACCESS-KEY\",\n    aws_session_token=\"YOUR-AWS-SESSION-TOKEN\",\n    sid=\"YOUD-SID\",\n    effect=\"EFFECT\",\n    principal=\"PRINCIPAL\",\n    action=[\"ACTIONS\"],\n    resource=[\"RESOURCES\"]\n)\n\nresult = s3_bucket_policy_utility.execute()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>bucket_name</code> <code>str</code> <p>S3 Bucket Name</p> required <code>aws_access_key_id</code> <code>str</code> <p>AWS Access Key</p> required <code>aws_secret_access_key</code> <code>str</code> <p>AWS Secret Key</p> required <code>aws_session_token</code> <code>str</code> <p>AWS Session Token</p> required <code>sid</code> <code>str</code> <p>S3 Bucket Policy Sid to be updated</p> required <code>effect</code> <code>str</code> <p>Effect to be applied to the policy</p> required <code>principal</code> <code>str</code> <p>Principal to be applied to Policy</p> required <code>action</code> <code>list[str]</code> <p>List of actions to be applied to the policy</p> required <code>resource</code> <code>list[str]</code> <p>List of resources to be applied to the policy</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/utilities/aws/s3_bucket_policy.py</code> <pre><code>class S3BucketPolicyUtility(UtilitiesInterface):\n\"\"\"\n    Assigns an IAM Bucket Policy to an S3 Bucket.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.utilities import S3BucketPolicyUtility\n\n    s3_bucket_policy_utility = S3BucketPolicyUtility(\n        bucket_name=\"YOUR-BUCKET-NAME\",\n        aws_access_key_id=\"YOUR-AWS-ACCESS-KEY\",\n        aws_secret_access_key=\"YOUR-AWS-SECRET-ACCESS-KEY\",\n        aws_session_token=\"YOUR-AWS-SESSION-TOKEN\",\n        sid=\"YOUD-SID\",\n        effect=\"EFFECT\",\n        principal=\"PRINCIPAL\",\n        action=[\"ACTIONS\"],\n        resource=[\"RESOURCES\"]\n    )\n\n    result = s3_bucket_policy_utility.execute()\n    ```\n\n    Parameters:\n        bucket_name (str): S3 Bucket Name\n        aws_access_key_id (str): AWS Access Key\n        aws_secret_access_key (str): AWS Secret Key\n        aws_session_token (str): AWS Session Token\n        sid (str): S3 Bucket Policy Sid to be updated\n        effect (str): Effect to be applied to the policy\n        principal (str): Principal to be applied to Policy\n        action (list[str]): List of actions to be applied to the policy\n        resource (list[str]): List of resources to be applied to the policy\n    \"\"\"\n\n    bucket_name: str\n    aws_access_key_id: str\n    aws_secret_access_key: str\n    aws_session_token: str\n    sid: str\n    effect: str\n    principal: str\n    action: List[str]\n    resource: List[str]\n\n    def __init__(\n        self,\n        bucket_name: str,\n        aws_access_key_id: str,\n        aws_secret_access_key: str,\n        aws_session_token: str,\n        sid: str,\n        principal: str,\n        effect: str,\n        action: List[str],\n        resource: List[str],\n    ) -&gt; None:\n        self.bucket_name = bucket_name\n        self.aws_access_key_id = aws_access_key_id\n        self.aws_secret_access_key = aws_secret_access_key\n        self.aws_session_token = aws_session_token\n        self.sid = sid\n        self.effect = effect\n        self.principal = principal\n        self.action = action\n        self.resource = resource\n\n    @staticmethod\n    def system_type():\n\"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYTHON\n        \"\"\"\n        return SystemType.PYTHON\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        libraries.add_pypi_library(get_default_package(\"aws_boto3\"))\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def execute(self) -&gt; bool:\n        try:\n            s3_client = boto3.client(\n                \"s3\",\n                aws_access_key_id=self.aws_access_key_id,\n                aws_secret_access_key=self.aws_secret_access_key,\n                aws_session_token=self.aws_session_token,\n            )\n\n            bucket_policy = s3_client.get_bucket_policy(Bucket=self.bucket_name)\n\n            policy_statement = None\n            if \"Policy\" in bucket_policy and bucket_policy[\"Policy\"] != None:\n                policy_statement = json.loads(bucket_policy[\"Policy\"])\n\n            if policy_statement is None:\n                policy_statement = {\"Version\": \"2012-10-17\", \"Statement\": []}\n\n            sid_found = False\n            for statement in policy_statement[\"Statement\"]:\n                if statement[\"Sid\"] == self.sid:\n                    sid_found = True\n                    statement[\"Effect\"] = self.effect\n                    statement[\"Principal\"] = self.principal\n                    statement[\"Action\"] = self.action\n                    if isinstance(statement[\"Resource\"], list):\n                        statement[\"Resource\"] + self.resource\n                    else:\n                        self.resource.append(statement[\"Resource\"])\n                        statement[\"Resource\"] = self.resource\n                    statement[\"Resource\"] = list(set(statement[\"Resource\"]))\n\n            if not sid_found:\n                policy_statement[\"Statement\"].append(\n                    {\n                        \"Sid\": self.sid,\n                        \"Effect\": self.effect,\n                        \"Principal\": self.principal,\n                        \"Action\": self.action,\n                        \"Resource\": self.resource,\n                    }\n                )\n\n            policy = json.dumps(policy_statement)\n            s3_client.put_bucket_policy(Bucket=self.bucket_name, Policy=policy)\n\n            return True\n\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/utilities/aws/s3_bucket_policy/#src.sdk.python.rtdip_sdk.pipelines.utilities.aws.s3_bucket_policy.S3BucketPolicyUtility.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYTHON</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/utilities/aws/s3_bucket_policy.py</code> <pre><code>@staticmethod\ndef system_type():\n\"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYTHON\n    \"\"\"\n    return SystemType.PYTHON\n</code></pre>"},{"location":"sdk/code-reference/pipelines/utilities/azure/adls_gen2_acl/","title":"ADLS Gen 2 ACLs","text":""},{"location":"sdk/code-reference/pipelines/utilities/azure/adls_gen2_acl/#src.sdk.python.rtdip_sdk.pipelines.utilities.azure.adls_gen2_acl.ADLSGen2DirectoryACLUtility","title":"<code>ADLSGen2DirectoryACLUtility</code>","text":"<p>             Bases: <code>UtilitiesInterface</code></p> <p>Assigns Azure AD Groups to ACLs on directories in an Azure Data Lake Store Gen 2 storage account.</p>"},{"location":"sdk/code-reference/pipelines/utilities/azure/adls_gen2_acl/#src.sdk.python.rtdip_sdk.pipelines.utilities.azure.adls_gen2_acl.ADLSGen2DirectoryACLUtility--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.utilities import ADLSGen2DirectoryACLUtility\n\nadls_gen2_directory_acl_utility = ADLSGen2DirectoryACLUtility(\n    storage_account=\"YOUR-STORAGAE-ACCOUNT-NAME\",\n    container=\"YOUR-ADLS_CONTAINER_NAME\",\n    credential=\"YOUR-TOKEN-CREDENTIAL\",\n    directory=\"DIRECTORY\",\n    group_object_id=\"GROUP-OBJECT\",\n    folder_permissions=\"r-x\",\n    parent_folder_permissions=\"r-x\",\n    root_folder_permissions=\"r-x\",\n    set_as_default_acl=True,\n    create_directory_if_not_exists=True\n)\n\nresult = adls_gen2_directory_acl_utility.execute()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>storage_account</code> <code>str</code> <p>ADLS Gen 2 Storage Account Name</p> required <code>container</code> <code>str</code> <p>ADLS Gen 2 Container Name</p> required <code>credential</code> <code>TokenCredential</code> <p>Credentials to authenticate with ADLS Gen 2 Storage Account</p> required <code>directory</code> <code>str</code> <p>Directory to be assign ACLS to in an ADLS Gen 2</p> required <code>group_object_id</code> <code>str</code> <p>Azure AD Group Object ID to be assigned to Directory</p> required <code>folder_permissions</code> <code>(optional, str)</code> <p>Folder Permissions to Assign to directory</p> <code>'r-x'</code> <code>parent_folder_permissions</code> <code>(optional, str)</code> <p>Folder Permissions to Assign to parent directories. Parent Folder ACLs not set if None</p> <code>'r-x'</code> <code>root_folder_permissions</code> <code>(optional, str)</code> <p>Folder Permissions to Assign to root directory. Root Folder ACL not set if None</p> <code>'r-x'</code> <code>set_as_default_acl</code> <code>bool</code> <p>Sets the ACL as the default ACL on the folder</p> <code>True</code> <code>create_directory_if_not_exists</code> <code>bool</code> <p>Creates the directory(and Parent Directories) if it does not exist</p> <code>True</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/utilities/azure/adls_gen2_acl.py</code> <pre><code>class ADLSGen2DirectoryACLUtility(UtilitiesInterface):\n\"\"\"\n    Assigns Azure AD Groups to ACLs on directories in an Azure Data Lake Store Gen 2 storage account.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.utilities import ADLSGen2DirectoryACLUtility\n\n    adls_gen2_directory_acl_utility = ADLSGen2DirectoryACLUtility(\n        storage_account=\"YOUR-STORAGAE-ACCOUNT-NAME\",\n        container=\"YOUR-ADLS_CONTAINER_NAME\",\n        credential=\"YOUR-TOKEN-CREDENTIAL\",\n        directory=\"DIRECTORY\",\n        group_object_id=\"GROUP-OBJECT\",\n        folder_permissions=\"r-x\",\n        parent_folder_permissions=\"r-x\",\n        root_folder_permissions=\"r-x\",\n        set_as_default_acl=True,\n        create_directory_if_not_exists=True\n    )\n\n    result = adls_gen2_directory_acl_utility.execute()\n    ```\n\n    Parameters:\n        storage_account (str): ADLS Gen 2 Storage Account Name\n        container (str): ADLS Gen 2 Container Name\n        credential (TokenCredential): Credentials to authenticate with ADLS Gen 2 Storage Account\n        directory (str): Directory to be assign ACLS to in an ADLS Gen 2\n        group_object_id (str): Azure AD Group Object ID to be assigned to Directory\n        folder_permissions (optional, str): Folder Permissions to Assign to directory\n        parent_folder_permissions (optional, str): Folder Permissions to Assign to parent directories. Parent Folder ACLs not set if None\n        root_folder_permissions (optional, str): Folder Permissions to Assign to root directory. Root Folder ACL not set if None\n        set_as_default_acl (bool, optional): Sets the ACL as the default ACL on the folder\n        create_directory_if_not_exists (bool, optional): Creates the directory(and Parent Directories) if it does not exist\n    \"\"\"\n\n    storage_account: str\n    container: str\n    credential: Union[\n        str,\n        Dict[str, str],\n        AzureNamedKeyCredential,\n        AzureSasCredential,\n        TokenCredential,\n        None,\n    ]\n    directory: str\n    group_object_id: str\n    folder_permissions: str\n    parent_folder_permissions: str\n    root_folder_permissions: str\n    set_as_default_acl: bool\n    create_directory_if_not_exists: bool\n\n    def __init__(\n        self,\n        storage_account: str,\n        container: str,\n        credential: Union[\n            str,\n            Dict[str, str],\n            AzureNamedKeyCredential,\n            AzureSasCredential,\n            TokenCredential,\n            None,\n        ],\n        directory: str,\n        group_object_id: str,\n        folder_permissions: str = \"r-x\",\n        parent_folder_permissions: Union[str, None] = \"r-x\",\n        root_folder_permissions: Union[str, None] = \"r-x\",\n        set_as_default_acl: bool = True,\n        create_directory_if_not_exists: bool = True,\n    ) -&gt; None:\n        self.storage_account = storage_account\n        self.container = container\n        self.credential = credential\n        self.directory = directory\n        self.group_object_id = group_object_id\n        self.folder_permissions = folder_permissions\n        self.parent_folder_permissions = parent_folder_permissions\n        self.root_folder_permissions = root_folder_permissions\n        self.set_as_default_acl = set_as_default_acl\n        self.create_directory_if_not_exists = create_directory_if_not_exists\n\n    @staticmethod\n    def system_type():\n\"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYTHON\n        \"\"\"\n        return SystemType.PYTHON\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        libraries.add_pypi_library(get_default_package(\"azure_adls_gen_2\"))\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def _set_acl(\n        self,\n        file_system_client: FileSystemClient,\n        path: str,\n        group_object_id: str,\n        folder_permissions: str,\n        set_as_default_acl: bool,\n    ):\n        acl_directory_client = file_system_client.get_directory_client(path)\n\n        group_id_acl = \"group:{}:{}\".format(group_object_id, folder_permissions)\n        acl_props = acl_directory_client.get_access_control().get(\"acl\")\n        acl_props_list = acl_props.split(\",\")\n\n        for acl in acl_props_list:\n            if group_object_id in acl:\n                acl_props_list.remove(acl)\n\n        if set_as_default_acl == True:\n            acl_props_list.append(\"default:{}\".format(group_id_acl))\n        else:\n            acl_props_list.append(group_id_acl)\n\n        new_acl_props = \",\".join(acl_props_list)\n        acl_directory_client.set_access_control(acl=new_acl_props)\n\n    def execute(self) -&gt; bool:\n        try:\n            # Setup file system client\n            service_client = DataLakeServiceClient(\n                account_url=\"{}://{}.dfs.core.windows.net\".format(\n                    \"https\", self.storage_account\n                ),\n                credential=self.credential,\n            )\n            file_system_client = service_client.get_file_system_client(\n                file_system=self.container\n            )\n\n            # Create directory if it doesn't already exist\n            if self.create_directory_if_not_exists:\n                directory_client = file_system_client.get_directory_client(\n                    self.directory\n                )\n                if not directory_client.exists():\n                    file_system_client.create_directory(self.directory)\n\n            group_object_id = str(self.group_object_id)\n            acl_path = \"\"\n            directory_list = self.directory.split(\"/\")\n\n            # Set Root Folder ACLs if specified\n            if self.root_folder_permissions != None:\n                self._set_acl(\n                    file_system_client,\n                    \"/\",\n                    group_object_id,\n                    self.root_folder_permissions,\n                    False,\n                )\n\n            # Set Parent Folders ACLs if specified\n            if self.parent_folder_permissions != None:\n                for directory in directory_list[:-1]:\n                    if directory == \"\":\n                        acl_path = \"/\"\n                        continue\n                    elif acl_path == \"/\":\n                        acl_path += directory\n                    else:\n                        acl_path += \"/\" + directory\n\n                    self._set_acl(\n                        file_system_client,\n                        acl_path,\n                        group_object_id,\n                        self.parent_folder_permissions,\n                        False,\n                    )\n\n            # Set Folder ACLs\n            self._set_acl(\n                file_system_client,\n                self.directory,\n                group_object_id,\n                self.folder_permissions,\n                self.set_as_default_acl,\n            )\n\n            return True\n\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/utilities/azure/adls_gen2_acl/#src.sdk.python.rtdip_sdk.pipelines.utilities.azure.adls_gen2_acl.ADLSGen2DirectoryACLUtility.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYTHON</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/utilities/azure/adls_gen2_acl.py</code> <pre><code>@staticmethod\ndef system_type():\n\"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYTHON\n    \"\"\"\n    return SystemType.PYTHON\n</code></pre>"},{"location":"sdk/code-reference/pipelines/utilities/azure/autoloader_resources/","title":"Autoloader Resources","text":""},{"location":"sdk/code-reference/pipelines/utilities/azure/autoloader_resources/#src.sdk.python.rtdip_sdk.pipelines.utilities.azure.autoloader_resources.AzureAutoloaderResourcesUtility","title":"<code>AzureAutoloaderResourcesUtility</code>","text":"<p>             Bases: <code>UtilitiesInterface</code></p> <p>Creates the required Azure Resources for the Databricks Autoloader Notification Mode.</p>"},{"location":"sdk/code-reference/pipelines/utilities/azure/autoloader_resources/#src.sdk.python.rtdip_sdk.pipelines.utilities.azure.autoloader_resources.AzureAutoloaderResourcesUtility--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.utilities import AzureAutoloaderResourcesUtility\n\nazure_autoloader_resources_utility = AzureAutoloaderResourcesUtility(\n    subscription_id=\"YOUR-SUBSCRIPTION-ID\",\n    resource_group_name=\"YOUR-RESOURCE-GROUP\",\n    storage_account=\"YOUR-STORAGE-ACCOUNT-NAME\",\n    container=\"YOUR-CONTAINER-NAME\",\n    directory=\"DIRECTORY\",\n    credential=\"YOUR-CLIENT-ID\",\n    event_subscription_name=\"YOUR-EVENT-SUBSCRIPTION\",\n    queue_name=\"YOUR-QUEUE-NAME\",\n    system_topic_name=None\n)\n\nresult = azure_autoloader_resources_utility.execute()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>subscription_id</code> <code>str</code> <p>Azure Subscription ID</p> required <code>resource_group_name</code> <code>str</code> <p>Resource Group Name of Subscription</p> required <code>storage_account</code> <code>str</code> <p>Storage Account Name</p> required <code>container</code> <code>str</code> <p>Container Name</p> required <code>directory</code> <code>str</code> <p>Directory to be used for filtering messages in the Event Subscription. This will be equivalent to the Databricks Autoloader Path</p> required <code>credential</code> <code>TokenCredential</code> <p>Credentials to authenticate with Storage Account</p> required <code>event_subscription_name</code> <code>str</code> <p>Name of the Event Subscription</p> required <code>queue_name</code> <code>str</code> <p>Name of the queue that will be used for the Endpoint of the Messages</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/utilities/azure/autoloader_resources.py</code> <pre><code>class AzureAutoloaderResourcesUtility(UtilitiesInterface):\n\"\"\"\n    Creates the required Azure Resources for the Databricks Autoloader Notification Mode.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.utilities import AzureAutoloaderResourcesUtility\n\n    azure_autoloader_resources_utility = AzureAutoloaderResourcesUtility(\n        subscription_id=\"YOUR-SUBSCRIPTION-ID\",\n        resource_group_name=\"YOUR-RESOURCE-GROUP\",\n        storage_account=\"YOUR-STORAGE-ACCOUNT-NAME\",\n        container=\"YOUR-CONTAINER-NAME\",\n        directory=\"DIRECTORY\",\n        credential=\"YOUR-CLIENT-ID\",\n        event_subscription_name=\"YOUR-EVENT-SUBSCRIPTION\",\n        queue_name=\"YOUR-QUEUE-NAME\",\n        system_topic_name=None\n    )\n\n    result = azure_autoloader_resources_utility.execute()\n    ```\n\n    Parameters:\n        subscription_id (str): Azure Subscription ID\n        resource_group_name (str): Resource Group Name of Subscription\n        storage_account (str): Storage Account Name\n        container (str): Container Name\n        directory (str): Directory to be used for filtering messages in the Event Subscription. This will be equivalent to the Databricks Autoloader Path\n        credential (TokenCredential): Credentials to authenticate with Storage Account\n        event_subscription_name (str): Name of the Event Subscription\n        queue_name (str): Name of the queue that will be used for the Endpoint of the Messages\n    \"\"\"\n\n    subscription_id: str\n    resource_group_name: str\n    storage_account: str\n    container: str\n    directory: str\n    credential: TokenCredential\n    event_subscription_name: str\n    queue_name: str\n\n    def __init__(\n        self,\n        subscription_id: str,\n        resource_group_name: str,\n        storage_account: str,\n        container: str,\n        directory: str,\n        credential: TokenCredential,\n        event_subscription_name: str,\n        queue_name: str,\n    ) -&gt; None:\n        self.subscription_id = subscription_id\n        self.resource_group_name = resource_group_name\n        self.storage_account = storage_account\n        self.container = container\n        self.directory = directory\n        self.credential = credential\n        self.event_subscription_name = event_subscription_name\n        self.queue_name = queue_name\n\n    @staticmethod\n    def system_type():\n\"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYTHON\n        \"\"\"\n        return SystemType.PYTHON\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        libraries.add_pypi_library(get_default_package(\"azure_eventgrid_mgmt\"))\n        libraries.add_pypi_library(get_default_package(\"azure_storage_mgmt\"))\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def execute(self) -&gt; bool:\n        storage_mgmt_client = StorageManagementClient(\n            credential=self.credential, subscription_id=self.subscription_id\n        )\n\n        try:\n            queue_response = storage_mgmt_client.queue.get(\n                resource_group_name=self.resource_group_name,\n                account_name=self.storage_account,\n                queue_name=self.queue_name,\n            )\n        except ResourceNotFoundError:\n            queue_response = None\n\n        if queue_response == None:\n            storage_mgmt_client.queue.create(\n                resource_group_name=self.resource_group_name,\n                account_name=self.storage_account,\n                queue_name=self.queue_name,\n                queue=StorageQueue(),\n            )\n\n        eventgrid_client = EventGridManagementClient(\n            credential=self.credential, subscription_id=self.subscription_id\n        )\n\n        source = \"/subscriptions/{}/resourceGroups/{}/providers/Microsoft.Storage/StorageAccounts/{}\".format(\n            self.subscription_id, self.resource_group_name, self.storage_account\n        )\n\n        try:\n            event_subscription_response = eventgrid_client.event_subscriptions.get(\n                scope=source, event_subscription_name=self.event_subscription_name\n            )\n        except ResourceNotFoundError:\n            event_subscription_response = None\n\n        if event_subscription_response == None:\n            event_subscription_destination = StorageQueueEventSubscriptionDestination(\n                resource_id=source,\n                queue_name=self.queue_name,\n                queue_message_time_to_live_in_seconds=None,\n            )\n\n            event_subscription_filter = EventSubscriptionFilter(\n                subject_begins_with=\"/blobServices/default/containers/{}/blobs/{}\".format(\n                    self.container, self.directory\n                ),\n                included_event_types=[\n                    \"Microsoft.Storage.BlobCreated\",\n                    \"Microsoft.Storage.BlobRenamed\",\n                    \"Microsoft.Storage.DirectoryRenamed\",\n                ],\n                advanced_filters=[\n                    StringContainsAdvancedFilter(\n                        key=\"data.api\",\n                        values=[\n                            \"CopyBlob\",\n                            \"PutBlob\",\n                            \"PutBlockList\",\n                            \"FlushWithClose\",\n                            \"RenameFile\",\n                            \"RenameDirectory\",\n                        ],\n                    )\n                ],\n            )\n\n            retry_policy = RetryPolicy()\n\n            event_subscription_info = EventSubscription(\n                destination=event_subscription_destination,\n                filter=event_subscription_filter,\n                event_delivery_schema=EventDeliverySchema.EVENT_GRID_SCHEMA,\n                retry_policy=retry_policy,\n            )\n\n            eventgrid_client.event_subscriptions.begin_create_or_update(\n                scope=source,\n                event_subscription_name=self.event_subscription_name,\n                event_subscription_info=event_subscription_info,\n            ).result()\n\n            return True\n</code></pre>"},{"location":"sdk/code-reference/pipelines/utilities/azure/autoloader_resources/#src.sdk.python.rtdip_sdk.pipelines.utilities.azure.autoloader_resources.AzureAutoloaderResourcesUtility.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYTHON</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/utilities/azure/autoloader_resources.py</code> <pre><code>@staticmethod\ndef system_type():\n\"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYTHON\n    \"\"\"\n    return SystemType.PYTHON\n</code></pre>"},{"location":"sdk/code-reference/pipelines/utilities/spark/adls_gen2_spn_connect/","title":"ADLS Gen 2 Service Principal Connect","text":""},{"location":"sdk/code-reference/pipelines/utilities/spark/adls_gen2_spn_connect/#src.sdk.python.rtdip_sdk.pipelines.utilities.spark.adls_gen2_spn_connect.SparkADLSGen2SPNConnectUtility","title":"<code>SparkADLSGen2SPNConnectUtility</code>","text":"<p>             Bases: <code>UtilitiesInterface</code></p> <p>Configures Spark to Connect to an ADLS Gen 2 Storage Account using a Service Principal.</p>"},{"location":"sdk/code-reference/pipelines/utilities/spark/adls_gen2_spn_connect/#src.sdk.python.rtdip_sdk.pipelines.utilities.spark.adls_gen2_spn_connect.SparkADLSGen2SPNConnectUtility--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.utilities import SparkADLSGen2SPNConnectUtility\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\nadls_gen2_connect_utility = SparkADLSGen2SPNConnectUtility(\n    spark=spark,\n    storage_account=\"YOUR-STORAGAE-ACCOUNT-NAME\",\n    tenant_id=\"YOUR-TENANT-ID\",\n    client_id=\"YOUR-CLIENT-ID\",\n    client_secret=\"YOUR-CLIENT-SECRET\"\n)\n\nresult = adls_gen2_connect_utility.execute()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session required to read data from cloud storage</p> required <code>storage_account</code> <code>str</code> <p>Name of the ADLS Gen 2 Storage Account</p> required <code>tenant_id</code> <code>str</code> <p>Tenant ID of the Service Principal</p> required <code>client_id</code> <code>str</code> <p>Service Principal Client ID</p> required <code>client_secret</code> <code>str</code> <p>Service Principal Client Secret</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/utilities/spark/adls_gen2_spn_connect.py</code> <pre><code>class SparkADLSGen2SPNConnectUtility(UtilitiesInterface):\n\"\"\"\n    Configures Spark to Connect to an ADLS Gen 2 Storage Account using a Service Principal.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.utilities import SparkADLSGen2SPNConnectUtility\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    adls_gen2_connect_utility = SparkADLSGen2SPNConnectUtility(\n        spark=spark,\n        storage_account=\"YOUR-STORAGAE-ACCOUNT-NAME\",\n        tenant_id=\"YOUR-TENANT-ID\",\n        client_id=\"YOUR-CLIENT-ID\",\n        client_secret=\"YOUR-CLIENT-SECRET\"\n    )\n\n    result = adls_gen2_connect_utility.execute()\n    ```\n\n    Parameters:\n        spark (SparkSession): Spark Session required to read data from cloud storage\n        storage_account (str): Name of the ADLS Gen 2 Storage Account\n        tenant_id (str): Tenant ID of the Service Principal\n        client_id (str): Service Principal Client ID\n        client_secret (str): Service Principal Client Secret\n    \"\"\"\n\n    spark: SparkSession\n    storage_account: str\n    tenant_id: str\n    client_id: str\n    client_secret: str\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        storage_account: str,\n        tenant_id: str,\n        client_id: str,\n        client_secret: str,\n    ) -&gt; None:\n        self.spark = spark\n        self.storage_account = storage_account\n        self.tenant_id = tenant_id\n        self.client_id = client_id\n        self.client_secret = client_secret\n\n    @staticmethod\n    def system_type():\n\"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def execute(self) -&gt; bool:\n\"\"\"Executes spark configuration to connect to an ADLS Gen 2 Storage Account using a service principal\"\"\"\n        try:\n            adls_gen2_config = SparkConfigurationUtility(\n                spark=self.spark,\n                config={\n                    \"fs.azure.account.auth.type.{}.dfs.core.windows.net\".format(\n                        self.storage_account\n                    ): \"OAuth\",\n                    \"fs.azure.account.oauth.provider.type.{}.dfs.core.windows.net\".format(\n                        self.storage_account\n                    ): \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n                    \"fs.azure.account.oauth2.client.id.{}.dfs.core.windows.net\".format(\n                        self.storage_account\n                    ): self.client_id,\n                    \"fs.azure.account.oauth2.client.secret.{}.dfs.core.windows.net\".format(\n                        self.storage_account\n                    ): self.client_secret,\n                    \"fs.azure.account.oauth2.client.endpoint.{}.dfs.core.windows.net\".format(\n                        self.storage_account\n                    ): \"https://login.microsoftonline.com/{}/oauth2/token\".format(\n                        self.tenant_id\n                    ),\n                },\n            )\n            adls_gen2_config.execute()\n            return True\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/utilities/spark/adls_gen2_spn_connect/#src.sdk.python.rtdip_sdk.pipelines.utilities.spark.adls_gen2_spn_connect.SparkADLSGen2SPNConnectUtility.execute","title":"<code>execute()</code>","text":"<p>Executes spark configuration to connect to an ADLS Gen 2 Storage Account using a service principal</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/utilities/spark/adls_gen2_spn_connect.py</code> <pre><code>def execute(self) -&gt; bool:\n\"\"\"Executes spark configuration to connect to an ADLS Gen 2 Storage Account using a service principal\"\"\"\n    try:\n        adls_gen2_config = SparkConfigurationUtility(\n            spark=self.spark,\n            config={\n                \"fs.azure.account.auth.type.{}.dfs.core.windows.net\".format(\n                    self.storage_account\n                ): \"OAuth\",\n                \"fs.azure.account.oauth.provider.type.{}.dfs.core.windows.net\".format(\n                    self.storage_account\n                ): \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n                \"fs.azure.account.oauth2.client.id.{}.dfs.core.windows.net\".format(\n                    self.storage_account\n                ): self.client_id,\n                \"fs.azure.account.oauth2.client.secret.{}.dfs.core.windows.net\".format(\n                    self.storage_account\n                ): self.client_secret,\n                \"fs.azure.account.oauth2.client.endpoint.{}.dfs.core.windows.net\".format(\n                    self.storage_account\n                ): \"https://login.microsoftonline.com/{}/oauth2/token\".format(\n                    self.tenant_id\n                ),\n            },\n        )\n        adls_gen2_config.execute()\n        return True\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/utilities/spark/adls_gen2_spn_connect/#src.sdk.python.rtdip_sdk.pipelines.utilities.spark.adls_gen2_spn_connect.SparkADLSGen2SPNConnectUtility.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/utilities/spark/adls_gen2_spn_connect.py</code> <pre><code>@staticmethod\ndef system_type():\n\"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/utilities/spark/configuration/","title":"Configuration","text":""},{"location":"sdk/code-reference/pipelines/utilities/spark/configuration/#src.sdk.python.rtdip_sdk.pipelines.utilities.spark.configuration.SparkConfigurationUtility","title":"<code>SparkConfigurationUtility</code>","text":"<p>             Bases: <code>UtilitiesInterface</code></p> <p>Sets configuration key value pairs to a Spark Session</p>"},{"location":"sdk/code-reference/pipelines/utilities/spark/configuration/#src.sdk.python.rtdip_sdk.pipelines.utilities.spark.configuration.SparkConfigurationUtility--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.sources import SparkConfigurationUtility\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\nconfiguration_utility = SparkConfigurationUtility(\n    spark=spark,\n    config={}\n)\n\nresult = configuration_utility.execute()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session required to read data from cloud storage</p> required <code>config</code> <code>dict</code> <p>Dictionary of spark configuration to be applied to the spark session</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/utilities/spark/configuration.py</code> <pre><code>class SparkConfigurationUtility(UtilitiesInterface):\n\"\"\"\n    Sets configuration key value pairs to a Spark Session\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.sources import SparkConfigurationUtility\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    configuration_utility = SparkConfigurationUtility(\n        spark=spark,\n        config={}\n    )\n\n    result = configuration_utility.execute()\n    ```\n\n    Parameters:\n        spark (SparkSession): Spark Session required to read data from cloud storage\n        config (dict): Dictionary of spark configuration to be applied to the spark session\n    \"\"\"\n\n    spark: SparkSession\n    config: dict\n    columns: List[StructField]\n    partitioned_by: List[str]\n    location: str\n    properties: dict\n    comment: str\n\n    def __init__(self, spark: SparkSession, config: dict) -&gt; None:\n        self.spark = spark\n        self.config = config\n\n    @staticmethod\n    def system_type():\n\"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def execute(self) -&gt; bool:\n\"\"\"Executes configuration key value pairs to a Spark Session\"\"\"\n        try:\n            for configuration in self.config.items():\n                self.spark.conf.set(configuration[0], configuration[1])\n            return True\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/utilities/spark/configuration/#src.sdk.python.rtdip_sdk.pipelines.utilities.spark.configuration.SparkConfigurationUtility.execute","title":"<code>execute()</code>","text":"<p>Executes configuration key value pairs to a Spark Session</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/utilities/spark/configuration.py</code> <pre><code>def execute(self) -&gt; bool:\n\"\"\"Executes configuration key value pairs to a Spark Session\"\"\"\n    try:\n        for configuration in self.config.items():\n            self.spark.conf.set(configuration[0], configuration[1])\n        return True\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/utilities/spark/configuration/#src.sdk.python.rtdip_sdk.pipelines.utilities.spark.configuration.SparkConfigurationUtility.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/utilities/spark/configuration.py</code> <pre><code>@staticmethod\ndef system_type():\n\"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/utilities/spark/delta_table_create/","title":"Delta Table Create","text":""},{"location":"sdk/code-reference/pipelines/utilities/spark/delta_table_create/#src.sdk.python.rtdip_sdk.pipelines.utilities.spark.delta_table_create.DeltaTableCreateUtility","title":"<code>DeltaTableCreateUtility</code>","text":"<p>             Bases: <code>UtilitiesInterface</code></p> <p>Creates a Delta Table in a Hive Metastore or in Databricks Unity Catalog.</p>"},{"location":"sdk/code-reference/pipelines/utilities/spark/delta_table_create/#src.sdk.python.rtdip_sdk.pipelines.utilities.spark.delta_table_create.DeltaTableCreateUtility--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.utilities.spark.delta_table_create import DeltaTableCreateUtility, DeltaTableColumn\n\ntable_create_utility = DeltaTableCreateUtility(\n    spark=spark_session,\n    table_name=\"delta_table\",\n    columns=[\n        DeltaTableColumn(name=\"EventDate\", type=\"date\", nullable=False, metadata={\"delta.generationExpression\": \"CAST(EventTime AS DATE)\"}),\n        DeltaTableColumn(name=\"TagName\", type=\"string\", nullable=False),\n        DeltaTableColumn(name=\"EventTime\", type=\"timestamp\", nullable=False),\n        DeltaTableColumn(name=\"Status\", type=\"string\", nullable=True),\n        DeltaTableColumn(name=\"Value\", type=\"float\", nullable=True)\n    ],\n    partitioned_by=[\"EventDate\"],\n    properties={\"delta.logRetentionDuration\": \"7 days\", \"delta.enableChangeDataFeed\": \"true\"},\n    comment=\"Creation of Delta Table\"\n)\n\nresult = table_create_utility.execute()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session required to read data from cloud storage</p> required <code>table_name</code> <code>str</code> <p>Name of the table, including catalog and schema if table is to be created in Unity Catalog</p> required <code>columns</code> <code>list[DeltaTableColumn]</code> <p>List of columns and their related column properties</p> required <code>partitioned_by</code> <code>list[str]</code> <p>List of column names to partition the table by</p> <code>None</code> <code>location</code> <code>str</code> <p>Path to storage location</p> <code>None</code> <code>properties</code> <code>dict</code> <p>Propoerties that can be specified for a Delta Table. Further information on the options available are here</p> <code>None</code> <code>comment</code> <code>str</code> <p>Provides a comment on the table metadata</p> <code>None</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/utilities/spark/delta_table_create.py</code> <pre><code>class DeltaTableCreateUtility(UtilitiesInterface):\n\"\"\"\n    Creates a Delta Table in a Hive Metastore or in Databricks Unity Catalog.\n\n    Example\n    -------\n    ```python\n    from rtdip_sdk.pipelines.utilities.spark.delta_table_create import DeltaTableCreateUtility, DeltaTableColumn\n\n    table_create_utility = DeltaTableCreateUtility(\n        spark=spark_session,\n        table_name=\"delta_table\",\n        columns=[\n            DeltaTableColumn(name=\"EventDate\", type=\"date\", nullable=False, metadata={\"delta.generationExpression\": \"CAST(EventTime AS DATE)\"}),\n            DeltaTableColumn(name=\"TagName\", type=\"string\", nullable=False),\n            DeltaTableColumn(name=\"EventTime\", type=\"timestamp\", nullable=False),\n            DeltaTableColumn(name=\"Status\", type=\"string\", nullable=True),\n            DeltaTableColumn(name=\"Value\", type=\"float\", nullable=True)\n        ],\n        partitioned_by=[\"EventDate\"],\n        properties={\"delta.logRetentionDuration\": \"7 days\", \"delta.enableChangeDataFeed\": \"true\"},\n        comment=\"Creation of Delta Table\"\n    )\n\n    result = table_create_utility.execute()\n    ```\n\n    Parameters:\n        spark (SparkSession): Spark Session required to read data from cloud storage\n        table_name (str): Name of the table, including catalog and schema if table is to be created in Unity Catalog\n        columns (list[DeltaTableColumn]): List of columns and their related column properties\n        partitioned_by (list[str], optional): List of column names to partition the table by\n        location (str, optional): Path to storage location\n        properties (dict, optional): Propoerties that can be specified for a Delta Table. Further information on the options available are [here](https://docs.databricks.com/delta/table-properties.html#delta-table-properties)\n        comment (str, optional): Provides a comment on the table metadata\n\n\n    \"\"\"\n\n    spark: SparkSession\n    table_name: str\n    columns: List[DeltaTableColumn]\n    partitioned_by: List[str]\n    location: str\n    properties: dict\n    comment: str\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        table_name: str,\n        columns: List[StructField],\n        partitioned_by: List[str] = None,\n        location: str = None,\n        properties: dict = None,\n        comment: str = None,\n    ) -&gt; None:\n        self.spark = spark\n        self.table_name = table_name\n        self.columns = columns\n        self.partitioned_by = partitioned_by\n        self.location = location\n        self.properties = properties\n        self.comment = comment\n\n    @staticmethod\n    def system_type():\n\"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        libraries.add_maven_library(get_default_package(\"spark_delta_core\"))\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def execute(self) -&gt; bool:\n        try:\n            columns = [StructField.fromJson(column.dict()) for column in self.columns]\n\n            delta_table = (\n                DeltaTable.createIfNotExists(self.spark)\n                .tableName(self.table_name)\n                .addColumns(columns)\n            )\n\n            if self.partitioned_by is not None:\n                delta_table = delta_table.partitionedBy(self.partitioned_by)\n\n            if self.location is not None:\n                delta_table = delta_table.location(self.location)\n\n            if self.properties is not None:\n                for key, value in self.properties.items():\n                    delta_table = delta_table.property(key, value)\n\n            if self.comment is not None:\n                delta_table = delta_table.comment(self.comment)\n\n            delta_table.execute()\n            return True\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/utilities/spark/delta_table_create/#src.sdk.python.rtdip_sdk.pipelines.utilities.spark.delta_table_create.DeltaTableCreateUtility.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/utilities/spark/delta_table_create.py</code> <pre><code>@staticmethod\ndef system_type():\n\"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/utilities/spark/delta_table_optimize/","title":"Delta Table Optimize","text":""},{"location":"sdk/code-reference/pipelines/utilities/spark/delta_table_optimize/#src.sdk.python.rtdip_sdk.pipelines.utilities.spark.delta_table_optimize.DeltaTableOptimizeUtility","title":"<code>DeltaTableOptimizeUtility</code>","text":"<p>             Bases: <code>UtilitiesInterface</code></p> <p>Optimizes a Delta Table.</p>"},{"location":"sdk/code-reference/pipelines/utilities/spark/delta_table_optimize/#src.sdk.python.rtdip_sdk.pipelines.utilities.spark.delta_table_optimize.DeltaTableOptimizeUtility--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.utilities.spark.delta_table_optimize import DeltaTableOptimizeUtility\n\ntable_optimize_utility = DeltaTableOptimizeUtility(\n    spark=spark_session,\n    table_name=\"delta_table\",\n    where=\"EventDate&lt;=current_date()\",\n    zorder_by=[\"EventDate\"]\n)\n\nresult = table_optimize_utility.execute()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session required to read data from cloud storage</p> required <code>table_name</code> <code>str</code> <p>Name of the table, including catalog and schema if table is to be created in Unity Catalog</p> required <code>where</code> <code>str</code> <p>Apply a partition filter to limit optimize to specific partitions. Example, \"date='2021-11-18'\" or \"EventDate&lt;=current_date()\"</p> <code>None</code> <code>zorder_by</code> <code>list[str]</code> <p>List of column names to zorder the table by. For more information, see here.</p> <code>None</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/utilities/spark/delta_table_optimize.py</code> <pre><code>class DeltaTableOptimizeUtility(UtilitiesInterface):\n\"\"\"\n    [Optimizes](https://docs.delta.io/latest/optimizations-oss.html) a Delta Table.\n\n    Example\n    -------\n    ```python\n    from rtdip_sdk.pipelines.utilities.spark.delta_table_optimize import DeltaTableOptimizeUtility\n\n    table_optimize_utility = DeltaTableOptimizeUtility(\n        spark=spark_session,\n        table_name=\"delta_table\",\n        where=\"EventDate&lt;=current_date()\",\n        zorder_by=[\"EventDate\"]\n    )\n\n    result = table_optimize_utility.execute()\n    ```\n\n    Parameters:\n        spark (SparkSession): Spark Session required to read data from cloud storage\n        table_name (str): Name of the table, including catalog and schema if table is to be created in Unity Catalog\n        where (str, optional): Apply a partition filter to limit optimize to specific partitions. Example, \"date='2021-11-18'\" or \"EventDate&lt;=current_date()\"\n        zorder_by (list[str], optional): List of column names to zorder the table by. For more information, see [here.](https://docs.delta.io/latest/optimizations-oss.html#optimize-performance-with-file-management&amp;language-python)\n    \"\"\"\n\n    spark: SparkSession\n    table_name: str\n    where: Optional[str]\n    zorder_by: Optional[List[str]]\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        table_name: str,\n        where: str = None,\n        zorder_by: List[str] = None,\n    ) -&gt; None:\n        self.spark = spark\n        self.table_name = table_name\n        self.where = where\n        self.zorder_by = zorder_by\n\n    @staticmethod\n    def system_type():\n\"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        libraries.add_maven_library(get_default_package(\"spark_delta_core\"))\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def execute(self) -&gt; bool:\n        try:\n            delta_table = DeltaTable.forName(self.spark, self.table_name).optimize()\n\n            if self.where is not None:\n                delta_table = delta_table.where(self.where)\n\n            if self.zorder_by is not None:\n                delta_table = delta_table.executeZOrderBy(self.zorder_by)\n            else:\n                delta_table.executeCompaction()\n\n            return True\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/utilities/spark/delta_table_optimize/#src.sdk.python.rtdip_sdk.pipelines.utilities.spark.delta_table_optimize.DeltaTableOptimizeUtility.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/utilities/spark/delta_table_optimize.py</code> <pre><code>@staticmethod\ndef system_type():\n\"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/utilities/spark/delta_table_vacuum/","title":"Delta Table Vacuum","text":""},{"location":"sdk/code-reference/pipelines/utilities/spark/delta_table_vacuum/#src.sdk.python.rtdip_sdk.pipelines.utilities.spark.delta_table_vacuum.DeltaTableVacuumUtility","title":"<code>DeltaTableVacuumUtility</code>","text":"<p>             Bases: <code>UtilitiesInterface</code></p> <p>Vacuums a Delta Table.</p>"},{"location":"sdk/code-reference/pipelines/utilities/spark/delta_table_vacuum/#src.sdk.python.rtdip_sdk.pipelines.utilities.spark.delta_table_vacuum.DeltaTableVacuumUtility--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.utilities.spark.delta_table_vacuum import DeltaTableVacuumUtility\n\ntable_vacuum_utility =  DeltaTableVacuumUtility(\n    spark=spark_session,\n    table_name=\"delta_table\",\n    retention_hours=\"168\"\n)\n\nresult = table_vacuum_utility.execute()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session required to read data from cloud storage</p> required <code>table_name</code> <code>str</code> <p>Name of the table, including catalog and schema if table is to be created in Unity Catalog</p> required <code>retention_hours</code> <code>int</code> <p>Sets the retention threshold in hours.</p> <code>None</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/utilities/spark/delta_table_vacuum.py</code> <pre><code>class DeltaTableVacuumUtility(UtilitiesInterface):\n\"\"\"\n    [Vacuums](https://docs.delta.io/latest/delta-utility.html#-delta-vacuum) a Delta Table.\n\n    Example\n    -------\n    ```python\n    from rtdip_sdk.pipelines.utilities.spark.delta_table_vacuum import DeltaTableVacuumUtility\n\n    table_vacuum_utility =  DeltaTableVacuumUtility(\n        spark=spark_session,\n        table_name=\"delta_table\",\n        retention_hours=\"168\"\n    )\n\n    result = table_vacuum_utility.execute()\n    ```\n\n    Parameters:\n        spark (SparkSession): Spark Session required to read data from cloud storage\n        table_name (str): Name of the table, including catalog and schema if table is to be created in Unity Catalog\n        retention_hours (int, optional): Sets the retention threshold in hours.\n    \"\"\"\n\n    spark: SparkSession\n    table_name: str\n    retention_hours: Optional[int]\n\n    def __init__(\n        self, spark: SparkSession, table_name: str, retention_hours: int = None\n    ) -&gt; None:\n        self.spark = spark\n        self.table_name = table_name\n        self.retention_hours = retention_hours\n\n    @staticmethod\n    def system_type():\n\"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        libraries.add_maven_library(get_default_package(\"spark_delta_core\"))\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def execute(self) -&gt; bool:\n        try:\n            delta_table = DeltaTable.forName(self.spark, self.table_name)\n\n            delta_table.vacuum(retentionHours=self.retention_hours)\n\n            return True\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/utilities/spark/delta_table_vacuum/#src.sdk.python.rtdip_sdk.pipelines.utilities.spark.delta_table_vacuum.DeltaTableVacuumUtility.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/utilities/spark/delta_table_vacuum.py</code> <pre><code>@staticmethod\ndef system_type():\n\"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/utilities/spark/session/","title":"Session","text":""},{"location":"sdk/code-reference/pipelines/utilities/spark/session/#src.sdk.python.rtdip_sdk.pipelines.utilities.spark.session.SparkSessionUtility","title":"<code>SparkSessionUtility</code>","text":"<p>             Bases: <code>UtilitiesInterface</code></p> <p>Creates or Gets a Spark Session and uses settings and libraries of the imported RTDIP components to populate the spark configuration and jars in the spark session.</p> <p>Call this component after all imports of the RTDIP components to ensure that the spark session is configured correctly.</p>"},{"location":"sdk/code-reference/pipelines/utilities/spark/session/#src.sdk.python.rtdip_sdk.pipelines.utilities.spark.session.SparkSessionUtility--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\nspark_session_utility = SparkConfigurationUtility(\n    config={},\n    module=None,\n    remote=None\n)\n\nresult = spark_session_utility.execute()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>optional dict</code> <p>Dictionary of spark configuration to be applied to the spark session</p> <code>None</code> <code>module</code> <code>optional str</code> <p>Provide the module to use for imports of rtdip-sdk components. If not populated, it will use the calling module to check for imports</p> <code>None</code> <code>remote</code> <code>optional str</code> <p>Specify the remote parameters if intending to use Spark Connect</p> <code>None</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/utilities/spark/session.py</code> <pre><code>class SparkSessionUtility(UtilitiesInterface):\n\"\"\"\n    Creates or Gets a Spark Session and uses settings and libraries of the imported RTDIP components to populate the spark configuration and jars in the spark session.\n\n    Call this component after all imports of the RTDIP components to ensure that the spark session is configured correctly.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    spark_session_utility = SparkConfigurationUtility(\n        config={},\n        module=None,\n        remote=None\n    )\n\n    result = spark_session_utility.execute()\n    ```\n\n    Parameters:\n        config (optional dict): Dictionary of spark configuration to be applied to the spark session\n        module (optional str): Provide the module to use for imports of rtdip-sdk components. If not populated, it will use the calling module to check for imports\n        remote (optional str): Specify the remote parameters if intending to use Spark Connect\n    \"\"\"\n\n    spark: SparkSession\n    config: dict\n    module: str\n\n    def __init__(\n        self, config: dict = None, module: str = None, remote: str = None\n    ) -&gt; None:\n        self.config = config\n        if module == None:\n            frm = inspect.stack()[1]\n            mod = inspect.getmodule(frm[0])\n            self.module = mod.__name__\n        else:\n            self.module = module\n        self.remote = remote\n\n    @staticmethod\n    def system_type():\n\"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def execute(self) -&gt; SparkSession:\n\"\"\"To execute\"\"\"\n        try:\n            (task_libraries, spark_configuration) = PipelineComponentsGetUtility(\n                self.module, self.config\n            ).execute()\n            self.spark = SparkClient(\n                spark_configuration=spark_configuration,\n                spark_libraries=task_libraries,\n                spark_remote=self.remote,\n            ).spark_session\n            return self.spark\n\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/utilities/spark/session/#src.sdk.python.rtdip_sdk.pipelines.utilities.spark.session.SparkSessionUtility.execute","title":"<code>execute()</code>","text":"<p>To execute</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/utilities/spark/session.py</code> <pre><code>def execute(self) -&gt; SparkSession:\n\"\"\"To execute\"\"\"\n    try:\n        (task_libraries, spark_configuration) = PipelineComponentsGetUtility(\n            self.module, self.config\n        ).execute()\n        self.spark = SparkClient(\n            spark_configuration=spark_configuration,\n            spark_libraries=task_libraries,\n            spark_remote=self.remote,\n        ).spark_session\n        return self.spark\n\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/utilities/spark/session/#src.sdk.python.rtdip_sdk.pipelines.utilities.spark.session.SparkSessionUtility.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/utilities/spark/session.py</code> <pre><code>@staticmethod\ndef system_type():\n\"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/query/connectors/chatopenai_databricks_connector/","title":"Chat Open AI Databricks LLM Connector","text":""},{"location":"sdk/code-reference/query/connectors/chatopenai_databricks_connector/#src.sdk.python.rtdip_sdk.connectors.llm.chatopenai_databricks_connector.ChatOpenAIDatabricksConnection","title":"<code>ChatOpenAIDatabricksConnection</code>","text":"<p>             Bases: <code>ConnectionInterface</code></p> <p>The Chat Open AI(Chat GPT) Databricks LLM Connector enables you to connect to a Databricks SQL Warehouse and use the Chat Open AI(Chat GPT) LLM to generate SQL queries.</p> <p>The connection class represents a connection to a database and uses the Databricks SQL Connector API's for Python to interact with cluster/jobs and langchain to connect to Chat Open AI(Chat GPT) LLM. To find details for SQL warehouses server_hostname and http_path location to the SQL Warehouse tab in the documentation.</p> <p>Parameters:</p> Name Type Description Default <code>catalog</code> <code>str</code> <p>Catalog name in Databricks</p> required <code>schema</code> <code>str</code> <p>Schema name in Databricks</p> required <code>server_hostname</code> <code>str</code> <p>Server hostname for the cluster or SQL Warehouse</p> required <code>http_path</code> <code>str</code> <p>Http path for the cluster or SQL Warehouse</p> required <code>access_token</code> <code>str</code> <p>Azure AD or Databricks PAT token</p> required <code>openai_api_key</code> <code>str</code> <p>OpenAI API key</p> required <code>openai_model</code> <code>str</code> <p>OpenAI model name</p> <code>'gpt-4'</code> <code>sample_rows_in_table_info</code> <code>int</code> <p>Number of rows to sample when getting table information</p> <code>3</code> <code>verbose_logging</code> <code>bool</code> <p>Whether to log verbose messages</p> <code>False</code> Source code in <code>src/sdk/python/rtdip_sdk/connectors/llm/chatopenai_databricks_connector.py</code> <pre><code>class ChatOpenAIDatabricksConnection(ConnectionInterface):\n\"\"\"\n    The Chat Open AI(Chat GPT) Databricks LLM Connector enables you to connect to a Databricks SQL Warehouse and use the Chat Open AI(Chat GPT) LLM to generate SQL queries.\n\n    The connection class represents a connection to a database and uses the Databricks SQL Connector API's for Python to interact with cluster/jobs and langchain to connect to Chat Open AI(Chat GPT) LLM.\n    To find details for SQL warehouses server_hostname and http_path location to the SQL Warehouse tab in the documentation.\n\n    Args:\n        catalog: Catalog name in Databricks\n        schema: Schema name in Databricks\n        server_hostname: Server hostname for the cluster or SQL Warehouse\n        http_path: Http path for the cluster or SQL Warehouse\n        access_token: Azure AD or Databricks PAT token\n        openai_api_key: OpenAI API key\n        openai_model: OpenAI model name\n        sample_rows_in_table_info: Number of rows to sample when getting table information\n        verbose_logging: Whether to log verbose messages\n    \"\"\"\n\n    def __init__(\n        self,\n        catalog: str,\n        schema: str,\n        server_hostname: str,\n        http_path: str,\n        access_token: str,\n        openai_api_key: str,\n        openai_model: str = \"gpt-4\",\n        sample_rows_in_table_info: int = 3,\n        verbose_logging: bool = False,\n    ) -&gt; None:\n        _package_version_meets_minimum(\"langchain\", \"0.0.196\")\n        # connect to llm\n        llm = ChatOpenAI(\n            temperature=0, model_name=openai_model, openai_api_key=openai_api_key\n        )\n\n        # connect to Databricks\n        db = SQLDatabase.from_databricks(\n            catalog=catalog,\n            schema=schema,\n            api_token=access_token,\n            host=server_hostname,\n            warehouse_id=http_path.split(\"/\")[-1],\n            sample_rows_in_table_info=sample_rows_in_table_info,\n        )\n\n        prefix = \"\"\"\n    ...\n    Always adhere to the format and don't return empty names or half responses.\n    \"\"\"\n        toolkit = SQLDatabaseToolkit(db=db, llm=llm)\n\n        model_agent_type = AgentType.ZERO_SHOT_REACT_DESCRIPTION\n        if \"0613\" in openai_model:\n            model_agent_type = AgentType.OPENAI_FUNCTIONS\n\n        self.connection = create_sql_agent(\n            llm=llm,\n            prefix=prefix,\n            toolkit=toolkit,\n            verbose=verbose_logging,\n            agent_type=model_agent_type,\n        )\n\n    def close(self) -&gt; None:\n\"\"\"Closes connection to database.\"\"\"\n        pass\n\n    def cursor(self) -&gt; object:\n\"\"\"\n        Initiates the cursor and returns it.\n\n        Returns:\n          ChatOpenAIDatabricksSQLCursor: Object to represent a connection to Databricks and Open AI with methods to interact with clusters/jobs and ChatGPT.\n        \"\"\"\n        try:\n            return ChatOpenAIDatabricksSQLCursor(self.connection)\n        except Exception as e:\n            logging.exception(\"error with cursor object\")\n            raise e\n\n    def run(self, query: str) -&gt; str:\n\"\"\"\n        Runs a query on the ChatGPT and the Databricks Cluster or SQL Warehouse.\n\n        Returns:\n            str: response from ChatGPT and the Databricks Cluster or SQL Warehouse\n        \"\"\"\n        cursor = self.cursor()\n        cursor.execute(query)\n        return cursor.fetch_all()\n</code></pre>"},{"location":"sdk/code-reference/query/connectors/chatopenai_databricks_connector/#src.sdk.python.rtdip_sdk.connectors.llm.chatopenai_databricks_connector.ChatOpenAIDatabricksConnection.close","title":"<code>close()</code>","text":"<p>Closes connection to database.</p> Source code in <code>src/sdk/python/rtdip_sdk/connectors/llm/chatopenai_databricks_connector.py</code> <pre><code>def close(self) -&gt; None:\n\"\"\"Closes connection to database.\"\"\"\n    pass\n</code></pre>"},{"location":"sdk/code-reference/query/connectors/chatopenai_databricks_connector/#src.sdk.python.rtdip_sdk.connectors.llm.chatopenai_databricks_connector.ChatOpenAIDatabricksConnection.cursor","title":"<code>cursor()</code>","text":"<p>Initiates the cursor and returns it.</p> <p>Returns:</p> Name Type Description <code>ChatOpenAIDatabricksSQLCursor</code> <code>object</code> <p>Object to represent a connection to Databricks and Open AI with methods to interact with clusters/jobs and ChatGPT.</p> Source code in <code>src/sdk/python/rtdip_sdk/connectors/llm/chatopenai_databricks_connector.py</code> <pre><code>def cursor(self) -&gt; object:\n\"\"\"\n    Initiates the cursor and returns it.\n\n    Returns:\n      ChatOpenAIDatabricksSQLCursor: Object to represent a connection to Databricks and Open AI with methods to interact with clusters/jobs and ChatGPT.\n    \"\"\"\n    try:\n        return ChatOpenAIDatabricksSQLCursor(self.connection)\n    except Exception as e:\n        logging.exception(\"error with cursor object\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/connectors/chatopenai_databricks_connector/#src.sdk.python.rtdip_sdk.connectors.llm.chatopenai_databricks_connector.ChatOpenAIDatabricksConnection.run","title":"<code>run(query)</code>","text":"<p>Runs a query on the ChatGPT and the Databricks Cluster or SQL Warehouse.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>response from ChatGPT and the Databricks Cluster or SQL Warehouse</p> Source code in <code>src/sdk/python/rtdip_sdk/connectors/llm/chatopenai_databricks_connector.py</code> <pre><code>def run(self, query: str) -&gt; str:\n\"\"\"\n    Runs a query on the ChatGPT and the Databricks Cluster or SQL Warehouse.\n\n    Returns:\n        str: response from ChatGPT and the Databricks Cluster or SQL Warehouse\n    \"\"\"\n    cursor = self.cursor()\n    cursor.execute(query)\n    return cursor.fetch_all()\n</code></pre>"},{"location":"sdk/code-reference/query/connectors/chatopenai_databricks_connector/#src.sdk.python.rtdip_sdk.connectors.llm.chatopenai_databricks_connector.ChatOpenAIDatabricksSQLCursor","title":"<code>ChatOpenAIDatabricksSQLCursor</code>","text":"<p>             Bases: <code>CursorInterface</code></p> <p>Object to represent a connection to Databricks and Open AI with methods to interact with clusters/jobs and ChatGPT.</p> <p>Parameters:</p> Name Type Description Default <code>cursor</code> <code>object</code> <p>controls execution of commands on cluster or SQL Warehouse</p> required Source code in <code>src/sdk/python/rtdip_sdk/connectors/llm/chatopenai_databricks_connector.py</code> <pre><code>class ChatOpenAIDatabricksSQLCursor(CursorInterface):\n\"\"\"\n    Object to represent a connection to Databricks and Open AI with methods to interact with clusters/jobs and ChatGPT.\n\n    Args:\n        cursor: controls execution of commands on cluster or SQL Warehouse\n    \"\"\"\n\n    response = None\n\n    def __init__(self, cursor: object) -&gt; None:\n        self.cursor = cursor\n\n    def execute(self, query: str) -&gt; None:\n\"\"\"\n        Prepares and runs a database query.\n\n        Args:\n            query: sql query to execute on ChatGPT and the Databricks Cluster or SQL Warehouse\n        \"\"\"\n        try:\n            self.response = self.cursor.run(query)\n        except Exception as e:\n            logging.exception(\"error while executing the query\")\n            raise e\n\n    def fetch_all(self) -&gt; list:\n\"\"\"\n        Gets all rows of a query.\n\n        Returns:\n            list: list of results\n        \"\"\"\n        try:\n            return self.response\n        except Exception as e:\n            logging.exception(\"error while fetching the rows of a query\")\n            raise e\n\n    def close(self) -&gt; None:\n\"\"\"Closes the cursor.\"\"\"\n        pass\n</code></pre>"},{"location":"sdk/code-reference/query/connectors/chatopenai_databricks_connector/#src.sdk.python.rtdip_sdk.connectors.llm.chatopenai_databricks_connector.ChatOpenAIDatabricksSQLCursor.close","title":"<code>close()</code>","text":"<p>Closes the cursor.</p> Source code in <code>src/sdk/python/rtdip_sdk/connectors/llm/chatopenai_databricks_connector.py</code> <pre><code>def close(self) -&gt; None:\n\"\"\"Closes the cursor.\"\"\"\n    pass\n</code></pre>"},{"location":"sdk/code-reference/query/connectors/chatopenai_databricks_connector/#src.sdk.python.rtdip_sdk.connectors.llm.chatopenai_databricks_connector.ChatOpenAIDatabricksSQLCursor.execute","title":"<code>execute(query)</code>","text":"<p>Prepares and runs a database query.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>sql query to execute on ChatGPT and the Databricks Cluster or SQL Warehouse</p> required Source code in <code>src/sdk/python/rtdip_sdk/connectors/llm/chatopenai_databricks_connector.py</code> <pre><code>def execute(self, query: str) -&gt; None:\n\"\"\"\n    Prepares and runs a database query.\n\n    Args:\n        query: sql query to execute on ChatGPT and the Databricks Cluster or SQL Warehouse\n    \"\"\"\n    try:\n        self.response = self.cursor.run(query)\n    except Exception as e:\n        logging.exception(\"error while executing the query\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/connectors/chatopenai_databricks_connector/#src.sdk.python.rtdip_sdk.connectors.llm.chatopenai_databricks_connector.ChatOpenAIDatabricksSQLCursor.fetch_all","title":"<code>fetch_all()</code>","text":"<p>Gets all rows of a query.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>list of results</p> Source code in <code>src/sdk/python/rtdip_sdk/connectors/llm/chatopenai_databricks_connector.py</code> <pre><code>def fetch_all(self) -&gt; list:\n\"\"\"\n    Gets all rows of a query.\n\n    Returns:\n        list: list of results\n    \"\"\"\n    try:\n        return self.response\n    except Exception as e:\n        logging.exception(\"error while fetching the rows of a query\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/connectors/db-sql-connector/","title":"Databricks SQL Connector","text":""},{"location":"sdk/code-reference/query/connectors/db-sql-connector/#src.sdk.python.rtdip_sdk.connectors.odbc.db_sql_connector.DatabricksSQLConnection","title":"<code>DatabricksSQLConnection</code>","text":"<p>             Bases: <code>ConnectionInterface</code></p> <p>The Databricks SQL Connector for Python is a Python library that allows you to use Python code to run SQL commands on Databricks clusters and Databricks SQL warehouses.</p> <p>The connection class represents a connection to a database and uses the Databricks SQL Connector API's for Python to intereact with cluster/jobs. To find details for SQL warehouses server_hostname and http_path location to the SQL Warehouse tab in the documentation.</p> <p>Parameters:</p> Name Type Description Default <code>server_hostname</code> <code>str</code> <p>Server hostname for the cluster or SQL Warehouse</p> required <code>http_path</code> <code>str</code> <p>Http path for the cluster or SQL Warehouse</p> required <code>access_token</code> <code>str</code> <p>Azure AD or Databricks PAT token</p> required Source code in <code>src/sdk/python/rtdip_sdk/connectors/odbc/db_sql_connector.py</code> <pre><code>class DatabricksSQLConnection(ConnectionInterface):\n\"\"\"\n    The Databricks SQL Connector for Python is a Python library that allows you to use Python code to run SQL commands on Databricks clusters and Databricks SQL warehouses.\n\n    The connection class represents a connection to a database and uses the Databricks SQL Connector API's for Python to intereact with cluster/jobs.\n    To find details for SQL warehouses server_hostname and http_path location to the SQL Warehouse tab in the documentation.\n\n    Args:\n        server_hostname: Server hostname for the cluster or SQL Warehouse\n        http_path: Http path for the cluster or SQL Warehouse\n        access_token: Azure AD or Databricks PAT token\n    \"\"\"\n\n    def __init__(self, server_hostname: str, http_path: str, access_token: str) -&gt; None:\n        self.server_hostname = server_hostname\n        self.http_path = http_path\n        self.access_token = access_token\n        # call auth method\n        self.connection = self._connect()\n\n    def _connect(self):\n\"\"\"Connects to the endpoint\"\"\"\n        try:\n            return sql.connect(\n                server_hostname=self.server_hostname,\n                http_path=self.http_path,\n                access_token=self.access_token,\n            )\n        except Exception as e:\n            logging.exception(\"error while connecting to the endpoint\")\n            raise e\n\n    def close(self) -&gt; None:\n\"\"\"Closes connection to database.\"\"\"\n        try:\n            self.connection.close()\n        except Exception as e:\n            logging.exception(\"error while closing connection\")\n            raise e\n\n    def cursor(self) -&gt; object:\n\"\"\"\n        Initiates the cursor and returns it.\n\n        Returns:\n          DatabricksSQLCursor: Object to represent a databricks workspace with methods to interact with clusters/jobs.\n        \"\"\"\n        try:\n            if self.connection.open == False:\n                self.connection = self._connect()\n            return DatabricksSQLCursor(self.connection.cursor())\n        except Exception as e:\n            logging.exception(\"error with cursor object\")\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/query/connectors/db-sql-connector/#src.sdk.python.rtdip_sdk.connectors.odbc.db_sql_connector.DatabricksSQLConnection.close","title":"<code>close()</code>","text":"<p>Closes connection to database.</p> Source code in <code>src/sdk/python/rtdip_sdk/connectors/odbc/db_sql_connector.py</code> <pre><code>def close(self) -&gt; None:\n\"\"\"Closes connection to database.\"\"\"\n    try:\n        self.connection.close()\n    except Exception as e:\n        logging.exception(\"error while closing connection\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/connectors/db-sql-connector/#src.sdk.python.rtdip_sdk.connectors.odbc.db_sql_connector.DatabricksSQLConnection.cursor","title":"<code>cursor()</code>","text":"<p>Initiates the cursor and returns it.</p> <p>Returns:</p> Name Type Description <code>DatabricksSQLCursor</code> <code>object</code> <p>Object to represent a databricks workspace with methods to interact with clusters/jobs.</p> Source code in <code>src/sdk/python/rtdip_sdk/connectors/odbc/db_sql_connector.py</code> <pre><code>def cursor(self) -&gt; object:\n\"\"\"\n    Initiates the cursor and returns it.\n\n    Returns:\n      DatabricksSQLCursor: Object to represent a databricks workspace with methods to interact with clusters/jobs.\n    \"\"\"\n    try:\n        if self.connection.open == False:\n            self.connection = self._connect()\n        return DatabricksSQLCursor(self.connection.cursor())\n    except Exception as e:\n        logging.exception(\"error with cursor object\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/connectors/db-sql-connector/#src.sdk.python.rtdip_sdk.connectors.odbc.db_sql_connector.DatabricksSQLCursor","title":"<code>DatabricksSQLCursor</code>","text":"<p>             Bases: <code>CursorInterface</code></p> <p>Object to represent a databricks workspace with methods to interact with clusters/jobs.</p> <p>Parameters:</p> Name Type Description Default <code>cursor</code> <code>object</code> <p>controls execution of commands on cluster or SQL Warehouse</p> required Source code in <code>src/sdk/python/rtdip_sdk/connectors/odbc/db_sql_connector.py</code> <pre><code>class DatabricksSQLCursor(CursorInterface):\n\"\"\"\n    Object to represent a databricks workspace with methods to interact with clusters/jobs.\n\n    Args:\n        cursor: controls execution of commands on cluster or SQL Warehouse\n    \"\"\"\n\n    def __init__(self, cursor: object) -&gt; None:\n        self.cursor = cursor\n\n    def execute(self, query: str) -&gt; None:\n\"\"\"\n        Prepares and runs a database query.\n\n        Args:\n            query: sql query to execute on the cluster or SQL Warehouse\n        \"\"\"\n        try:\n            self.cursor.execute(query)\n        except Exception as e:\n            logging.exception(\"error while executing the query\")\n            raise e\n\n    def fetch_all(self) -&gt; list:\n\"\"\"\n        Gets all rows of a query.\n\n        Returns:\n            list: list of results\n        \"\"\"\n        try:\n            result = self.cursor.fetchall_arrow()\n            df = result.to_pandas()\n            return df\n        except Exception as e:\n            logging.exception(\"error while fetching the rows of a query\")\n            raise e\n\n    def close(self) -&gt; None:\n\"\"\"Closes the cursor.\"\"\"\n        try:\n            self.cursor.close()\n        except Exception as e:\n            logging.exception(\"error while closing the cursor\")\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/query/connectors/db-sql-connector/#src.sdk.python.rtdip_sdk.connectors.odbc.db_sql_connector.DatabricksSQLCursor.close","title":"<code>close()</code>","text":"<p>Closes the cursor.</p> Source code in <code>src/sdk/python/rtdip_sdk/connectors/odbc/db_sql_connector.py</code> <pre><code>def close(self) -&gt; None:\n\"\"\"Closes the cursor.\"\"\"\n    try:\n        self.cursor.close()\n    except Exception as e:\n        logging.exception(\"error while closing the cursor\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/connectors/db-sql-connector/#src.sdk.python.rtdip_sdk.connectors.odbc.db_sql_connector.DatabricksSQLCursor.execute","title":"<code>execute(query)</code>","text":"<p>Prepares and runs a database query.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>sql query to execute on the cluster or SQL Warehouse</p> required Source code in <code>src/sdk/python/rtdip_sdk/connectors/odbc/db_sql_connector.py</code> <pre><code>def execute(self, query: str) -&gt; None:\n\"\"\"\n    Prepares and runs a database query.\n\n    Args:\n        query: sql query to execute on the cluster or SQL Warehouse\n    \"\"\"\n    try:\n        self.cursor.execute(query)\n    except Exception as e:\n        logging.exception(\"error while executing the query\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/connectors/db-sql-connector/#src.sdk.python.rtdip_sdk.connectors.odbc.db_sql_connector.DatabricksSQLCursor.fetch_all","title":"<code>fetch_all()</code>","text":"<p>Gets all rows of a query.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>list of results</p> Source code in <code>src/sdk/python/rtdip_sdk/connectors/odbc/db_sql_connector.py</code> <pre><code>def fetch_all(self) -&gt; list:\n\"\"\"\n    Gets all rows of a query.\n\n    Returns:\n        list: list of results\n    \"\"\"\n    try:\n        result = self.cursor.fetchall_arrow()\n        df = result.to_pandas()\n        return df\n    except Exception as e:\n        logging.exception(\"error while fetching the rows of a query\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/connectors/pyodbc-sql-connector/","title":"PYODBC SQL Connector","text":""},{"location":"sdk/code-reference/query/connectors/pyodbc-sql-connector/#pyodbc-driver-paths","title":"PYODBC Driver Paths","text":"<p>To use PYODBC SQL Connect you will require the driver path specified below per operating system.</p> Operating Systems Driver Paths Windows <code>C:\\Program Files\\Simba Spark ODBC Driver</code> MacOS <code>/Library/simba/spark/lib/libsparkodbc_sbu.dylib</code> Linux 64-bit <code>/opt/simba/spark/lib/64/libsparkodbc_sb64.so</code> Linux 32-bit <code>/opt/simba/spark/lib/32/libsparkodbc_sb32.so</code>"},{"location":"sdk/code-reference/query/connectors/pyodbc-sql-connector/#src.sdk.python.rtdip_sdk.connectors.odbc.pyodbc_sql_connector.PYODBCSQLConnection","title":"<code>PYODBCSQLConnection</code>","text":"<p>             Bases: <code>ConnectionInterface</code></p> <p>PYODBC is an open source python module which allows access to ODBC databases. This allows the user to connect through ODBC to data in azure databricks clusters or sql warehouses.</p> <p>Uses the databricks API's (2.0) to connect to the sql server.</p> <p>Parameters:</p> Name Type Description Default <code>driver_path</code> <code>str</code> <p>Driver installed to work with PYODBC</p> required <code>server_hostname</code> <code>str</code> <p>Server hostname for the cluster or SQL Warehouse</p> required <code>http_path</code> <code>str</code> <p>Http path for the cluster or SQL Warehouse</p> required <code>access_token</code> <code>str</code> <p>Azure AD Token</p> required Note 1 <p>More fields can be configured here in the connection ie PORT, Schema, etc.</p> Note 2 <p>When using Unix, Linux or Mac OS brew installation of PYODBC is required for connection.</p> Source code in <code>src/sdk/python/rtdip_sdk/connectors/odbc/pyodbc_sql_connector.py</code> <pre><code>class PYODBCSQLConnection(ConnectionInterface):\n\"\"\"\n    PYODBC is an open source python module which allows access to ODBC databases.\n    This allows the user to connect through ODBC to data in azure databricks clusters or sql warehouses.\n\n    Uses the databricks API's (2.0) to connect to the sql server.\n\n    Args:\n        driver_path: Driver installed to work with PYODBC\n        server_hostname: Server hostname for the cluster or SQL Warehouse\n        http_path: Http path for the cluster or SQL Warehouse\n        access_token: Azure AD Token\n\n    Note 1:\n        More fields can be configured here in the connection ie PORT, Schema, etc.\n\n    Note 2:\n        When using Unix, Linux or Mac OS brew installation of PYODBC is required for connection.\n    \"\"\"\n\n    def __init__(\n        self, driver_path: str, server_hostname: str, http_path: str, access_token: str\n    ) -&gt; None:\n        self.driver_path = driver_path\n        self.server_hostname = server_hostname\n        self.http_path = http_path\n        self.access_token = access_token\n        # call auth method\n        self.connection = self._connect()\n        self.open = True\n\n    def _connect(self):\n\"\"\"Connects to the endpoint\"\"\"\n        try:\n            return pyodbc.connect(\n                \"Driver=\"\n                + self.driver_path\n                + \";\"\n                + \"HOST=\"\n                + self.server_hostname\n                + \";\"\n                + \"PORT=443;\"\n                + \"Schema=default;\"\n                + \"SparkServerType=3;\"\n                + \"AuthMech=11;\"\n                + \"UID=token;\"\n                +\n                #'PWD=' + access_token+ \";\" +\n                \"Auth_AccessToken=\" + self.access_token + \";\"\n                \"ThriftTransport=2;\" + \"SSL=1;\" + \"HTTPPath=\" + self.http_path,\n                autocommit=True,\n            )\n        except Exception as e:\n            logging.exception(\"error while connecting to the endpoint\")\n            raise e\n\n    def close(self) -&gt; None:\n\"\"\"Closes connection to database.\"\"\"\n        try:\n            self.connection.close()\n            self.open = False\n        except Exception as e:\n            logging.exception(\"error while closing the connection\")\n            raise e\n\n    def cursor(self) -&gt; object:\n\"\"\"\n        Intiates the cursor and returns it.\n\n        Returns:\n          PYODBCSQLCursor: Object to represent a databricks workspace with methods to interact with clusters/jobs.\n        \"\"\"\n        try:\n            if self.open == False:\n                self.connection = self._connect()\n            return PYODBCSQLCursor(self.connection.cursor())\n        except Exception as e:\n            logging.exception(\"error with cursor object\")\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/query/connectors/pyodbc-sql-connector/#src.sdk.python.rtdip_sdk.connectors.odbc.pyodbc_sql_connector.PYODBCSQLConnection.close","title":"<code>close()</code>","text":"<p>Closes connection to database.</p> Source code in <code>src/sdk/python/rtdip_sdk/connectors/odbc/pyodbc_sql_connector.py</code> <pre><code>def close(self) -&gt; None:\n\"\"\"Closes connection to database.\"\"\"\n    try:\n        self.connection.close()\n        self.open = False\n    except Exception as e:\n        logging.exception(\"error while closing the connection\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/connectors/pyodbc-sql-connector/#src.sdk.python.rtdip_sdk.connectors.odbc.pyodbc_sql_connector.PYODBCSQLConnection.cursor","title":"<code>cursor()</code>","text":"<p>Intiates the cursor and returns it.</p> <p>Returns:</p> Name Type Description <code>PYODBCSQLCursor</code> <code>object</code> <p>Object to represent a databricks workspace with methods to interact with clusters/jobs.</p> Source code in <code>src/sdk/python/rtdip_sdk/connectors/odbc/pyodbc_sql_connector.py</code> <pre><code>def cursor(self) -&gt; object:\n\"\"\"\n    Intiates the cursor and returns it.\n\n    Returns:\n      PYODBCSQLCursor: Object to represent a databricks workspace with methods to interact with clusters/jobs.\n    \"\"\"\n    try:\n        if self.open == False:\n            self.connection = self._connect()\n        return PYODBCSQLCursor(self.connection.cursor())\n    except Exception as e:\n        logging.exception(\"error with cursor object\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/connectors/pyodbc-sql-connector/#src.sdk.python.rtdip_sdk.connectors.odbc.pyodbc_sql_connector.PYODBCSQLCursor","title":"<code>PYODBCSQLCursor</code>","text":"<p>             Bases: <code>CursorInterface</code></p> <p>Object to represent a databricks workspace with methods to interact with clusters/jobs.</p> <p>Parameters:</p> Name Type Description Default <code>cursor</code> <code>object</code> <p>controls execution of commands on cluster or SQL Warehouse</p> required Source code in <code>src/sdk/python/rtdip_sdk/connectors/odbc/pyodbc_sql_connector.py</code> <pre><code>class PYODBCSQLCursor(CursorInterface):\n\"\"\"\n    Object to represent a databricks workspace with methods to interact with clusters/jobs.\n\n    Args:\n        cursor: controls execution of commands on cluster or SQL Warehouse\n    \"\"\"\n\n    def __init__(self, cursor: object) -&gt; None:\n        self.cursor = cursor\n\n    def execute(self, query: str) -&gt; None:\n\"\"\"\n        Prepares and runs a database query.\n\n        Args:\n            query: sql query to execute on the cluster or SQL Warehouse\n        \"\"\"\n        try:\n            self.cursor.execute(query)\n\n        except Exception as e:\n            logging.exception(\"error while executing the query\")\n            raise e\n\n    def fetch_all(self) -&gt; list:\n\"\"\"\n        Gets all rows of a query.\n\n        Returns:\n            list: list of results\n        \"\"\"\n        try:\n            result = self.cursor.fetchall()\n            cols = [column[0] for column in self.cursor.description]\n            result = [list(x) for x in result]\n            df = pd.DataFrame(result)\n            df.columns = cols\n            return df\n        except Exception as e:\n            logging.exception(\"error while fetching rows from the query\")\n            raise e\n\n    def close(self) -&gt; None:\n\"\"\"Closes the cursor.\"\"\"\n        try:\n            self.cursor.close()\n        except Exception as e:\n            logging.exception(\"error while closing the cursor\")\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/query/connectors/pyodbc-sql-connector/#src.sdk.python.rtdip_sdk.connectors.odbc.pyodbc_sql_connector.PYODBCSQLCursor.close","title":"<code>close()</code>","text":"<p>Closes the cursor.</p> Source code in <code>src/sdk/python/rtdip_sdk/connectors/odbc/pyodbc_sql_connector.py</code> <pre><code>def close(self) -&gt; None:\n\"\"\"Closes the cursor.\"\"\"\n    try:\n        self.cursor.close()\n    except Exception as e:\n        logging.exception(\"error while closing the cursor\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/connectors/pyodbc-sql-connector/#src.sdk.python.rtdip_sdk.connectors.odbc.pyodbc_sql_connector.PYODBCSQLCursor.execute","title":"<code>execute(query)</code>","text":"<p>Prepares and runs a database query.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>sql query to execute on the cluster or SQL Warehouse</p> required Source code in <code>src/sdk/python/rtdip_sdk/connectors/odbc/pyodbc_sql_connector.py</code> <pre><code>def execute(self, query: str) -&gt; None:\n\"\"\"\n    Prepares and runs a database query.\n\n    Args:\n        query: sql query to execute on the cluster or SQL Warehouse\n    \"\"\"\n    try:\n        self.cursor.execute(query)\n\n    except Exception as e:\n        logging.exception(\"error while executing the query\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/connectors/pyodbc-sql-connector/#src.sdk.python.rtdip_sdk.connectors.odbc.pyodbc_sql_connector.PYODBCSQLCursor.fetch_all","title":"<code>fetch_all()</code>","text":"<p>Gets all rows of a query.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>list of results</p> Source code in <code>src/sdk/python/rtdip_sdk/connectors/odbc/pyodbc_sql_connector.py</code> <pre><code>def fetch_all(self) -&gt; list:\n\"\"\"\n    Gets all rows of a query.\n\n    Returns:\n        list: list of results\n    \"\"\"\n    try:\n        result = self.cursor.fetchall()\n        cols = [column[0] for column in self.cursor.description]\n        result = [list(x) for x in result]\n        df = pd.DataFrame(result)\n        df.columns = cols\n        return df\n    except Exception as e:\n        logging.exception(\"error while fetching rows from the query\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/connectors/spark-connector/","title":"Spark Connector","text":""},{"location":"sdk/code-reference/query/connectors/spark-connector/#src.sdk.python.rtdip_sdk.connectors.grpc.spark_connector.SparkConnection","title":"<code>SparkConnection</code>","text":"<p>             Bases: <code>ConnectionInterface</code></p> <p>The Spark Connector enables running Spark Sql queries via a Spark Session.</p> <p>Additionally, this connector supports Spark Connect which was introduced in Pyspark 3.4.0 and allows Spark Sessions to connect to remote Spark Clusters. This enables Spark SQL to be constructed locally, but executed remotely. To find out more about Spark Connect and the connection string to be provided to the <code>spark_remote</code> parameter of the Spark Session, please see here.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>(optional, SparkSession)</code> <p>Provide an existing spark session if one exists. A new Spark Session will be created if not populated</p> <code>None</code> <code>spark_configuration</code> <code>(optional, dict)</code> <p>Spark configuration to be provided to the spark session</p> <code>None</code> <code>spark_libraries</code> <code>(optional, Libraries)</code> <p>Additional JARs to be included in the Spark Session.</p> <code>None</code> <code>spark_remote</code> <code>(optional, str)</code> <p>Remote connection string of Spark Server and any authentication details. The Spark Connect introduced in Pyspark 3.4.0 allows Spark Sessions to connect to remote Spark Clusters. This enables Spark SQL to be constructed locally, but executed remotely.</p> <code>None</code> Source code in <code>src/sdk/python/rtdip_sdk/connectors/grpc/spark_connector.py</code> <pre><code>class SparkConnection(ConnectionInterface):\n\"\"\"\n    The Spark Connector enables running Spark Sql queries via a Spark Session.\n\n    Additionally, this connector supports Spark Connect which was introduced in Pyspark 3.4.0 and allows Spark Sessions to connect to remote Spark Clusters. This enables Spark SQL to be constructed locally, but executed remotely.\n    To find out more about Spark Connect and the connection string to be provided to the `spark_remote` parameter of the Spark Session, please see [here.](https://spark.apache.org/docs/latest/spark-connect-overview.html#specify-spark-connect-when-creating-spark-session)\n\n    Args:\n        spark (optional, SparkSession): Provide an existing spark session if one exists. A new Spark Session will be created if not populated\n        spark_configuration (optional, dict): Spark configuration to be provided to the spark session\n        spark_libraries (optional, Libraries): Additional JARs to be included in the Spark Session.\n        spark_remote (optional, str): Remote connection string of Spark Server and any authentication details. The Spark Connect introduced in Pyspark 3.4.0 allows Spark Sessions to connect to remote Spark Clusters. This enables Spark SQL to be constructed locally, but executed remotely.\n    \"\"\"\n\n    def __init__(\n        self,\n        spark: SparkSession = None,\n        spark_configuration: dict = None,\n        spark_libraries: Libraries = None,\n        spark_remote: str = None,\n    ) -&gt; None:\n        if spark_remote != None:\n            _package_version_meets_minimum(\"pyspark\", \"3.4.0\")\n\n        if spark is None:\n            self.connection = SparkClient(\n                spark_configuration={}\n                if spark_configuration is None\n                else spark_configuration,\n                spark_libraries=Libraries()\n                if spark_libraries is None\n                else spark_libraries,\n                spark_remote=spark_remote,\n            ).spark_session\n        else:\n            self.connection = spark\n\n    def close(self) -&gt; None:\n\"\"\"Not relevant for spark sessions\"\"\"\n        pass\n\n    def cursor(self) -&gt; object:\n\"\"\"\n        Intiates the cursor and returns it.\n\n        Returns:\n          DatabricksSQLCursor: Object to represent a databricks workspace with methods to interact with clusters/jobs.\n        \"\"\"\n        try:\n            return SparkCursor(self.connection)\n        except Exception as e:\n            logging.exception(\"error with cursor object\")\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/query/connectors/spark-connector/#src.sdk.python.rtdip_sdk.connectors.grpc.spark_connector.SparkConnection.close","title":"<code>close()</code>","text":"<p>Not relevant for spark sessions</p> Source code in <code>src/sdk/python/rtdip_sdk/connectors/grpc/spark_connector.py</code> <pre><code>def close(self) -&gt; None:\n\"\"\"Not relevant for spark sessions\"\"\"\n    pass\n</code></pre>"},{"location":"sdk/code-reference/query/connectors/spark-connector/#src.sdk.python.rtdip_sdk.connectors.grpc.spark_connector.SparkConnection.cursor","title":"<code>cursor()</code>","text":"<p>Intiates the cursor and returns it.</p> <p>Returns:</p> Name Type Description <code>DatabricksSQLCursor</code> <code>object</code> <p>Object to represent a databricks workspace with methods to interact with clusters/jobs.</p> Source code in <code>src/sdk/python/rtdip_sdk/connectors/grpc/spark_connector.py</code> <pre><code>def cursor(self) -&gt; object:\n\"\"\"\n    Intiates the cursor and returns it.\n\n    Returns:\n      DatabricksSQLCursor: Object to represent a databricks workspace with methods to interact with clusters/jobs.\n    \"\"\"\n    try:\n        return SparkCursor(self.connection)\n    except Exception as e:\n        logging.exception(\"error with cursor object\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/connectors/spark-connector/#src.sdk.python.rtdip_sdk.connectors.grpc.spark_connector.SparkCursor","title":"<code>SparkCursor</code>","text":"<p>             Bases: <code>CursorInterface</code></p> <p>Object to represent a spark session with methods to interact with clusters/jobs using the remote connection information.</p> <p>Parameters:</p> Name Type Description Default <code>cursor</code> <code>object</code> <p>controls execution of commands on Spark Cluster</p> required Source code in <code>src/sdk/python/rtdip_sdk/connectors/grpc/spark_connector.py</code> <pre><code>class SparkCursor(CursorInterface):\n\"\"\"\n    Object to represent a spark session with methods to interact with clusters/jobs using the remote connection information.\n\n    Args:\n        cursor: controls execution of commands on Spark Cluster\n    \"\"\"\n\n    execute_result: DataFrame\n\n    def __init__(self, cursor: object) -&gt; None:\n        self.cursor = cursor\n\n    def execute(self, query: str) -&gt; None:\n\"\"\"\n        Prepares and runs a database query.\n\n        Args:\n            query: sql query to execute on the cluster or SQL Warehouse\n        \"\"\"\n        try:\n            self.execute_result = self.cursor.sql(query)\n        except Exception as e:\n            logging.exception(\"error while executing the query\")\n            raise e\n\n    def fetch_all(self) -&gt; DataFrame:\n\"\"\"\n        Gets all rows of a query.\n\n        Returns:\n          DataFrame: Spark DataFrame of results\n        \"\"\"\n        try:\n            df = self.execute_result\n            return df\n        except Exception as e:\n            logging.exception(\"error while fetching the rows of a query\")\n            raise e\n\n    def close(self) -&gt; None:\n\"\"\"Not relevant for dataframes\"\"\"\n        pass\n</code></pre>"},{"location":"sdk/code-reference/query/connectors/spark-connector/#src.sdk.python.rtdip_sdk.connectors.grpc.spark_connector.SparkCursor.close","title":"<code>close()</code>","text":"<p>Not relevant for dataframes</p> Source code in <code>src/sdk/python/rtdip_sdk/connectors/grpc/spark_connector.py</code> <pre><code>def close(self) -&gt; None:\n\"\"\"Not relevant for dataframes\"\"\"\n    pass\n</code></pre>"},{"location":"sdk/code-reference/query/connectors/spark-connector/#src.sdk.python.rtdip_sdk.connectors.grpc.spark_connector.SparkCursor.execute","title":"<code>execute(query)</code>","text":"<p>Prepares and runs a database query.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>sql query to execute on the cluster or SQL Warehouse</p> required Source code in <code>src/sdk/python/rtdip_sdk/connectors/grpc/spark_connector.py</code> <pre><code>def execute(self, query: str) -&gt; None:\n\"\"\"\n    Prepares and runs a database query.\n\n    Args:\n        query: sql query to execute on the cluster or SQL Warehouse\n    \"\"\"\n    try:\n        self.execute_result = self.cursor.sql(query)\n    except Exception as e:\n        logging.exception(\"error while executing the query\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/connectors/spark-connector/#src.sdk.python.rtdip_sdk.connectors.grpc.spark_connector.SparkCursor.fetch_all","title":"<code>fetch_all()</code>","text":"<p>Gets all rows of a query.</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Spark DataFrame of results</p> Source code in <code>src/sdk/python/rtdip_sdk/connectors/grpc/spark_connector.py</code> <pre><code>def fetch_all(self) -&gt; DataFrame:\n\"\"\"\n    Gets all rows of a query.\n\n    Returns:\n      DataFrame: Spark DataFrame of results\n    \"\"\"\n    try:\n        df = self.execute_result\n        return df\n    except Exception as e:\n        logging.exception(\"error while fetching the rows of a query\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/connectors/turbodbc-sql-connector/","title":"TURBODBC SQL Connector","text":""},{"location":"sdk/code-reference/query/connectors/turbodbc-sql-connector/#src.sdk.python.rtdip_sdk.connectors.odbc.turbodbc_sql_connector.TURBODBCSQLConnection","title":"<code>TURBODBCSQLConnection</code>","text":"<p>             Bases: <code>ConnectionInterface</code></p> <p>Turbodbc is a python module used to access relational databases through an ODBC interface. It will allow a user to connect to databricks clusters or sql warehouses.</p> <p>Turbodbc offers built-in NumPy support allowing it to be much faster for processing compared to other connectors. To find details for SQL warehouses server_hostname and http_path location to the SQL Warehouse tab in the documentation.</p> <p>Parameters:</p> Name Type Description Default <code>server_hostname</code> <code>str</code> <p>hostname for the cluster or SQL Warehouse</p> required <code>http_path</code> <code>str</code> <p>Http path for the cluster or SQL Warehouse</p> required <code>access_token</code> <code>str</code> <p>Azure AD Token</p> required Note <p>More fields such as driver can be configured upon extension.</p> Source code in <code>src/sdk/python/rtdip_sdk/connectors/odbc/turbodbc_sql_connector.py</code> <pre><code>class TURBODBCSQLConnection(ConnectionInterface):\n\"\"\"\n    Turbodbc is a python module used to access relational databases through an ODBC interface. It will allow a user to connect to databricks clusters or sql warehouses.\n\n    Turbodbc offers built-in NumPy support allowing it to be much faster for processing compared to other connectors.\n    To find details for SQL warehouses server_hostname and http_path location to the SQL Warehouse tab in the documentation.\n\n    Args:\n        server_hostname: hostname for the cluster or SQL Warehouse\n        http_path: Http path for the cluster or SQL Warehouse\n        access_token: Azure AD Token\n\n    Note:\n        More fields such as driver can be configured upon extension.\n    \"\"\"\n\n    def __init__(self, server_hostname: str, http_path: str, access_token: str) -&gt; None:\n        _package_version_meets_minimum(\"turbodbc\", \"4.0.0\")\n        self.server_hostname = server_hostname\n        self.http_path = http_path\n        self.access_token = access_token\n        # call auth method\n        self.connection = self._connect()\n        self.open = True\n\n    def _connect(self):\n\"\"\"Connects to the endpoint\"\"\"\n        try:\n            options = make_options(\n                autocommit=True, read_buffer_size=Megabytes(100), use_async_io=True\n            )\n\n            return connect(\n                Driver=\"Simba Spark ODBC Driver\",\n                Host=self.server_hostname,\n                Port=443,\n                SparkServerType=3,\n                ThriftTransport=2,\n                SSL=1,\n                AuthMech=11,\n                Auth_AccessToken=self.access_token,\n                Auth_Flow=0,\n                HTTPPath=self.http_path,\n                UseNativeQuery=1,\n                FastSQLPrepare=1,\n                ApplyFastSQLPrepareToAllQueries=1,\n                DisableLimitZero=1,\n                EnableAsyncExec=1,\n                RowsFetchedPerBlock=os.getenv(\"RTDIP_ODBC_ROW_BLOCK_SIZE\", 500000),\n                turbodbc_options=options,\n            )\n\n        except Exception as e:\n            logging.exception(\"error while connecting to the endpoint\")\n            raise e\n\n    def close(self) -&gt; None:\n\"\"\"Closes connection to database.\"\"\"\n        try:\n            self.connection.close()\n            self.open = False\n        except Exception as e:\n            logging.exception(\"error while closing the connection\")\n            raise e\n\n    def cursor(self) -&gt; object:\n\"\"\"\n        Intiates the cursor and returns it.\n\n        Returns:\n          TURBODBCSQLCursor: Object to represent a databricks workspace with methods to interact with clusters/jobs.\n        \"\"\"\n        try:\n            if self.open == False:\n                self.connection = self._connect()\n            return TURBODBCSQLCursor(self.connection.cursor())\n        except Exception as e:\n            logging.exception(\"error with cursor object\")\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/query/connectors/turbodbc-sql-connector/#src.sdk.python.rtdip_sdk.connectors.odbc.turbodbc_sql_connector.TURBODBCSQLConnection.close","title":"<code>close()</code>","text":"<p>Closes connection to database.</p> Source code in <code>src/sdk/python/rtdip_sdk/connectors/odbc/turbodbc_sql_connector.py</code> <pre><code>def close(self) -&gt; None:\n\"\"\"Closes connection to database.\"\"\"\n    try:\n        self.connection.close()\n        self.open = False\n    except Exception as e:\n        logging.exception(\"error while closing the connection\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/connectors/turbodbc-sql-connector/#src.sdk.python.rtdip_sdk.connectors.odbc.turbodbc_sql_connector.TURBODBCSQLConnection.cursor","title":"<code>cursor()</code>","text":"<p>Intiates the cursor and returns it.</p> <p>Returns:</p> Name Type Description <code>TURBODBCSQLCursor</code> <code>object</code> <p>Object to represent a databricks workspace with methods to interact with clusters/jobs.</p> Source code in <code>src/sdk/python/rtdip_sdk/connectors/odbc/turbodbc_sql_connector.py</code> <pre><code>def cursor(self) -&gt; object:\n\"\"\"\n    Intiates the cursor and returns it.\n\n    Returns:\n      TURBODBCSQLCursor: Object to represent a databricks workspace with methods to interact with clusters/jobs.\n    \"\"\"\n    try:\n        if self.open == False:\n            self.connection = self._connect()\n        return TURBODBCSQLCursor(self.connection.cursor())\n    except Exception as e:\n        logging.exception(\"error with cursor object\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/connectors/turbodbc-sql-connector/#src.sdk.python.rtdip_sdk.connectors.odbc.turbodbc_sql_connector.TURBODBCSQLCursor","title":"<code>TURBODBCSQLCursor</code>","text":"<p>             Bases: <code>CursorInterface</code></p> <p>Object to represent a databricks workspace with methods to interact with clusters/jobs.</p> <p>Parameters:</p> Name Type Description Default <code>cursor</code> <code>object</code> <p>controls execution of commands on cluster or SQL Warehouse</p> required Source code in <code>src/sdk/python/rtdip_sdk/connectors/odbc/turbodbc_sql_connector.py</code> <pre><code>class TURBODBCSQLCursor(CursorInterface):\n\"\"\"\n    Object to represent a databricks workspace with methods to interact with clusters/jobs.\n\n    Args:\n        cursor: controls execution of commands on cluster or SQL Warehouse\n    \"\"\"\n\n    def __init__(self, cursor: object) -&gt; None:\n        self.cursor = cursor\n\n    def execute(self, query: str) -&gt; None:\n\"\"\"\n        Prepares and runs a database query.\n\n        Args:\n            query: sql query to execute on the cluster or SQL Warehouse\n        \"\"\"\n        try:\n            self.cursor.execute(query)\n        except Exception as e:\n            logging.exception(\"error while executing the query\")\n            raise e\n\n    def fetch_all(self) -&gt; list:\n\"\"\"\n        Gets all rows of a query.\n\n        Returns:\n            list: list of results\n        \"\"\"\n        try:\n            result = self.cursor.fetchallarrow()\n            df = result.to_pandas()\n            return df\n        except Exception as e:\n            logging.exception(\"error while fetching the rows from the query\")\n            raise e\n\n    def close(self) -&gt; None:\n\"\"\"Closes the cursor.\"\"\"\n        try:\n            self.cursor.close()\n        except Exception as e:\n            logging.exception(\"error while closing the cursor\")\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/query/connectors/turbodbc-sql-connector/#src.sdk.python.rtdip_sdk.connectors.odbc.turbodbc_sql_connector.TURBODBCSQLCursor.close","title":"<code>close()</code>","text":"<p>Closes the cursor.</p> Source code in <code>src/sdk/python/rtdip_sdk/connectors/odbc/turbodbc_sql_connector.py</code> <pre><code>def close(self) -&gt; None:\n\"\"\"Closes the cursor.\"\"\"\n    try:\n        self.cursor.close()\n    except Exception as e:\n        logging.exception(\"error while closing the cursor\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/connectors/turbodbc-sql-connector/#src.sdk.python.rtdip_sdk.connectors.odbc.turbodbc_sql_connector.TURBODBCSQLCursor.execute","title":"<code>execute(query)</code>","text":"<p>Prepares and runs a database query.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>sql query to execute on the cluster or SQL Warehouse</p> required Source code in <code>src/sdk/python/rtdip_sdk/connectors/odbc/turbodbc_sql_connector.py</code> <pre><code>def execute(self, query: str) -&gt; None:\n\"\"\"\n    Prepares and runs a database query.\n\n    Args:\n        query: sql query to execute on the cluster or SQL Warehouse\n    \"\"\"\n    try:\n        self.cursor.execute(query)\n    except Exception as e:\n        logging.exception(\"error while executing the query\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/connectors/turbodbc-sql-connector/#src.sdk.python.rtdip_sdk.connectors.odbc.turbodbc_sql_connector.TURBODBCSQLCursor.fetch_all","title":"<code>fetch_all()</code>","text":"<p>Gets all rows of a query.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>list of results</p> Source code in <code>src/sdk/python/rtdip_sdk/connectors/odbc/turbodbc_sql_connector.py</code> <pre><code>def fetch_all(self) -&gt; list:\n\"\"\"\n    Gets all rows of a query.\n\n    Returns:\n        list: list of results\n    \"\"\"\n    try:\n        result = self.cursor.fetchallarrow()\n        df = result.to_pandas()\n        return df\n    except Exception as e:\n        logging.exception(\"error while fetching the rows from the query\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/functions/metadata/","title":"Metadata Function","text":""},{"location":"sdk/code-reference/query/functions/metadata/#src.sdk.python.rtdip_sdk.queries.metadata.get","title":"<code>get(connection, parameters_dict)</code>","text":"<p>A function to return back the metadata by querying databricks SQL Warehouse using a connection specified by the user.</p> <p>The available connectors by RTDIP are Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect.</p> <p>The available authentcation methods are Certificate Authentication, Client Secret Authentication or Default Authentication. See documentation.</p> <p>This function requires the user to input a dictionary of parameters. (See Attributes table below)</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>object</code> <p>Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)</p> required <code>parameters_dict</code> <code>dict</code> <p>A dictionary of parameters (see Attributes table below)</p> required <p>Attributes:</p> Name Type Description <code>business_unit</code> <code>str</code> <p>Business unit</p> <code>region</code> <code>str</code> <p>Region</p> <code>asset</code> <code>str</code> <p>Asset</p> <code>data_security_level</code> <code>str</code> <p>Level of data security</p> <code>tag_names</code> <code>(optional, list)</code> <p>Either pass a list of tagname/tagnames [\"tag_1\", \"tag_2\"] or leave the list blank [] or leave the parameter out completely</p> <code>limit</code> <code>optional int</code> <p>The number of rows to be returned</p> <code>offset</code> <code>optional int</code> <p>The number of rows to skip before returning rows</p> Source code in <code>src/sdk/python/rtdip_sdk/queries/metadata.py</code> <pre><code>def get(connection: object, parameters_dict: dict) -&gt; pd.DataFrame:\n\"\"\"\n    A function to return back the metadata by querying databricks SQL Warehouse using a connection specified by the user.\n\n    The available connectors by RTDIP are Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect.\n\n    The available authentcation methods are Certificate Authentication, Client Secret Authentication or Default Authentication. See documentation.\n\n    This function requires the user to input a dictionary of parameters. (See Attributes table below)\n\n    Args:\n        connection: Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)\n        parameters_dict: A dictionary of parameters (see Attributes table below)\n\n    Attributes:\n        business_unit (str): Business unit\n        region (str): Region\n        asset (str): Asset\n        data_security_level (str): Level of data security\n        tag_names (optional, list): Either pass a list of tagname/tagnames [\"tag_1\", \"tag_2\"] or leave the list blank [] or leave the parameter out completely\n        limit (optional int): The number of rows to be returned\n        offset (optional int): The number of rows to skip before returning rows\n    Returns:\n        DataFrame: A dataframe of metadata.\n    \"\"\"\n    try:\n        query = _query_builder(parameters_dict, \"metadata\")\n\n        try:\n            cursor = connection.cursor()\n            cursor.execute(query)\n            df = cursor.fetch_all()\n            cursor.close()\n            connection.close()\n            return df\n        except Exception as e:\n            logging.exception(\"error returning dataframe\")\n            raise e\n\n    except Exception as e:\n        logging.exception(\"error returning metadata function\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/functions/metadata/#example","title":"Example","text":"<pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import metadata\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\nparameters = {\n    \"business_unit\": \"{business_unit}\",\n    \"region\": \"{region}\", \n    \"asset\": \"{asset_name}\", \n    \"data_security_level\": \"{security_level}\", \n    \"tag_names\": [\"{tag_name_1}\", \"{tag_name_2}\"],\n}\nx = metadata.get(connection, parameters)\nprint(x)\n</code></pre> <p>This example is using <code>DefaultAuth()</code> and <code>DatabricksSQLConnection()</code> to authenticate and connect. You can find other ways to authenticate here. The alternative built in connection methods are either by <code>PYODBCSQLConnection()</code>, <code>TURBODBCSQLConnection()</code> or <code>SparkConnection()</code>.</p> <p>Note</p> <p>See Samples Repository for full list of examples.</p> <p>Note</p> <p><code>server_hostname</code> and <code>http_path</code> can be found on the SQL Warehouses Page. </p>"},{"location":"sdk/code-reference/query/functions/time_series/circular-average/","title":"Circular Average Function","text":""},{"location":"sdk/code-reference/query/functions/time_series/circular-average/#src.sdk.python.rtdip_sdk.queries.time_series.circular_average.get","title":"<code>get(connection, parameters_dict)</code>","text":"<p>A function that receives a dataframe of raw tag data and computes the circular mean for samples in a range, returning the results.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>object</code> <p>Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)</p> required <code>parameters_dict</code> <code>dict</code> <p>A dictionary of parameters (see Attributes table below)</p> required <p>Attributes:</p> Name Type Description <code>business_unit</code> <code>str</code> <p>Business unit</p> <code>region</code> <code>str</code> <p>Region</p> <code>asset</code> <code>str</code> <p>Asset</p> <code>data_security_level</code> <code>str</code> <p>Level of data security</p> <code>data_type</code> <code>str</code> <p>Type of the data (float, integer, double, string)</p> <code>tag_names</code> <code>list</code> <p>List of tagname or tagnames</p> <code>start_date</code> <code>str</code> <p>Start date (Either a utc date in the format YYYY-MM-DD or a utc datetime in the format YYYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> <code>end_date</code> <code>str</code> <p>End date (Either a utc date in the format YYYY-MM-DD or a utc datetime in the format YYYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> <code>time_interval_rate</code> <code>str</code> <p>The time interval rate (numeric input)</p> <code>time_interval_unit</code> <code>str</code> <p>The time interval unit (second, minute, day, hour)</p> <code>lower_bound</code> <code>int</code> <p>Lower boundary for the sample range</p> <code>upper_bound</code> <code>int</code> <p>Upper boundary for the sample range</p> <code>include_bad_data</code> <code>bool</code> <p>Include \"Bad\" data points with True or remove \"Bad\" data points with False</p> <code>pivot</code> <code>bool</code> <p>Pivot the data on timestamp column with True or do not pivot the data with False</p> <code>limit</code> <code>optional int</code> <p>The number of rows to be returned</p> <code>offset</code> <code>optional int</code> <p>The number of rows to skip before returning rows</p> Source code in <code>src/sdk/python/rtdip_sdk/queries/time_series/circular_average.py</code> <pre><code>def get(connection: object, parameters_dict: dict) -&gt; pd.DataFrame:\n\"\"\"\n    A function that receives a dataframe of raw tag data and computes the circular mean for samples in a range, returning the results.\n\n    Args:\n        connection: Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)\n        parameters_dict (dict): A dictionary of parameters (see Attributes table below)\n\n    Attributes:\n        business_unit (str): Business unit\n        region (str): Region\n        asset (str): Asset\n        data_security_level (str): Level of data security\n        data_type (str): Type of the data (float, integer, double, string)\n        tag_names (list): List of tagname or tagnames\n        start_date (str): Start date (Either a utc date in the format YYYY-MM-DD or a utc datetime in the format YYYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        end_date (str): End date (Either a utc date in the format YYYY-MM-DD or a utc datetime in the format YYYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        time_interval_rate (str): The time interval rate (numeric input)\n        time_interval_unit (str): The time interval unit (second, minute, day, hour)\n        lower_bound (int): Lower boundary for the sample range\n        upper_bound (int): Upper boundary for the sample range\n        include_bad_data (bool): Include \"Bad\" data points with True or remove \"Bad\" data points with False\n        pivot (bool): Pivot the data on timestamp column with True or do not pivot the data with False\n        limit (optional int): The number of rows to be returned\n        offset (optional int): The number of rows to skip before returning rows\n    Returns:\n        DataFrame: A dataframe containing the circular averages.\n    \"\"\"\n    if isinstance(parameters_dict[\"tag_names\"], list) is False:\n        raise ValueError(\"tag_names must be a list\")\n\n    try:\n        query = _query_builder(parameters_dict, \"circular_average\")\n\n        try:\n            cursor = connection.cursor()\n            cursor.execute(query)\n            df = cursor.fetch_all()\n            cursor.close()\n            connection.close()\n            return df\n        except Exception as e:\n            logging.exception(\"error returning dataframe\")\n            raise e\n\n    except Exception as e:\n        logging.exception(\"error with circular average function\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/functions/time_series/circular-average/#example","title":"Example","text":"<pre><code> from rtdip_sdk.authentication.azure import DefaultAuth\n from rtdip_sdk.connectors import DatabricksSQLConnection\n from rtdip_sdk.queries import circular_average\n\n auth = DefaultAuth().authenticate()\n token = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\n connection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\n parameters = {\n     \"business_unit\": \"{business_unit}\",\n     \"region\": \"{region}\", \n     \"asset\": \"{asset_name}\", \n     \"data_security_level\": \"{security_level}\", \n     \"data_type\": \"float\",\n     \"tag_names\": [\"{tag_name_1}\", \"{tag_name_2}\"],\n     \"start_date\": \"2023-01-01\",\n     \"end_date\": \"2023-01-31\",\n     \"time_interval_rate\": \"15\",\n     \"time_interval_unit\": \"minute\",\n     \"lower_bound\": 0,\n     \"upper_bound\": 360,\n     \"include_bad_data\": True,\n }\n x = circular_average.get(connection, parameters)\n print(x)\n</code></pre> <p>This example is using <code>DefaultAuth()</code> and <code>DatabricksSQLConnection()</code> to authenticate and connect. You can find other ways to authenticate here. The alternative built in connection methods are either by <code>PYODBCSQLConnection()</code>, <code>TURBODBCSQLConnection()</code> or <code>SparkConnection()</code>.</p> <p>Note</p> <p>See Samples Repository for full list of examples.</p> <p>Note</p> <p><code>server_hostname</code> and <code>http_path</code> can be found on the SQL Warehouses Page. </p>"},{"location":"sdk/code-reference/query/functions/time_series/circular-standard-deviation/","title":"Circular Standard Deviation Function","text":""},{"location":"sdk/code-reference/query/functions/time_series/circular-standard-deviation/#src.sdk.python.rtdip_sdk.queries.time_series.circular_standard_deviation.get","title":"<code>get(connection, parameters_dict)</code>","text":"<p>A function that receives a dataframe of raw tag data and computes the circular standard deviation for samples assumed to be in the range, returning the results.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>object</code> <p>Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)</p> required <code>parameters_dict</code> <code>dict</code> <p>A dictionary of parameters (see Attributes table below)</p> required <p>Attributes:</p> Name Type Description <code>business_unit</code> <code>str</code> <p>Business unit</p> <code>region</code> <code>str</code> <p>Region</p> <code>asset</code> <code>str</code> <p>Asset</p> <code>data_security_level</code> <code>str</code> <p>Level of data security</p> <code>data_type</code> <code>str</code> <p>Type of the data (float, integer, double, string)</p> <code>tag_names</code> <code>list</code> <p>List of tagname or tagnames</p> <code>start_date</code> <code>str</code> <p>Start date (Either a utc date in the format YYYY-MM-DD or a utc datetime in the format YYYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> <code>end_date</code> <code>str</code> <p>End date (Either a utc date in the format YYYY-MM-DD or a utc datetime in the format YYYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> <code>time_interval_rate</code> <code>str</code> <p>The time interval rate (numeric input)</p> <code>time_interval_unit</code> <code>str</code> <p>The time interval unit (second, minute, day, hour)</p> <code>lower_bound</code> <code>int</code> <p>Lower boundary for the sample range</p> <code>upper_bound</code> <code>int</code> <p>Upper boundary for the sample range</p> <code>include_bad_data</code> <code>bool</code> <p>Include \"Bad\" data points with True or remove \"Bad\" data points with False</p> <code>pivot</code> <code>bool</code> <p>Pivot the data on timestamp column with True or do not pivot the data with False</p> <code>limit</code> <code>optional int</code> <p>The number of rows to be returned</p> <code>offset</code> <code>optional int</code> <p>The number of rows to skip before returning rows</p> Source code in <code>src/sdk/python/rtdip_sdk/queries/time_series/circular_standard_deviation.py</code> <pre><code>def get(connection: object, parameters_dict: dict) -&gt; pd.DataFrame:\n\"\"\"\n    A function that receives a dataframe of raw tag data and computes the circular standard deviation for samples assumed to be in the range, returning the results.\n\n    Args:\n        connection: Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)\n        parameters_dict (dict): A dictionary of parameters (see Attributes table below)\n\n    Attributes:\n        business_unit (str): Business unit\n        region (str): Region\n        asset (str): Asset\n        data_security_level (str): Level of data security\n        data_type (str): Type of the data (float, integer, double, string)\n        tag_names (list): List of tagname or tagnames\n        start_date (str): Start date (Either a utc date in the format YYYY-MM-DD or a utc datetime in the format YYYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        end_date (str): End date (Either a utc date in the format YYYY-MM-DD or a utc datetime in the format YYYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        time_interval_rate (str): The time interval rate (numeric input)\n        time_interval_unit (str): The time interval unit (second, minute, day, hour)\n        lower_bound (int): Lower boundary for the sample range\n        upper_bound (int): Upper boundary for the sample range\n        include_bad_data (bool): Include \"Bad\" data points with True or remove \"Bad\" data points with False\n        pivot (bool): Pivot the data on timestamp column with True or do not pivot the data with False\n        limit (optional int): The number of rows to be returned\n        offset (optional int): The number of rows to skip before returning rows\n    Returns:\n        DataFrame: A dataframe containing the circular standard deviations.\n    \"\"\"\n    if isinstance(parameters_dict[\"tag_names\"], list) is False:\n        raise ValueError(\"tag_names must be a list\")\n\n    try:\n        query = _query_builder(parameters_dict, \"circular_standard_deviation\")\n\n        try:\n            cursor = connection.cursor()\n            cursor.execute(query)\n            df = cursor.fetch_all()\n            cursor.close()\n            connection.close()\n            return df\n        except Exception as e:\n            logging.exception(\"error returning dataframe\")\n            raise e\n\n    except Exception as e:\n        logging.exception(\"error with circular standard_deviation function\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/functions/time_series/circular-standard-deviation/#example","title":"Example","text":"<pre><code> from rtdip_sdk.authentication.azure import DefaultAuth\n from rtdip_sdk.connectors import DatabricksSQLConnection\n from rtdip_sdk.queries import circular_standard_deviation\n\n auth = DefaultAuth().authenticate()\n token = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\n connection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\n parameters = {\n     \"business_unit\": \"{business_unit}\",\n     \"region\": \"{region}\", \n     \"asset\": \"{asset_name}\", \n     \"data_security_level\": \"{security_level}\", \n     \"data_type\": \"float\",\n     \"tag_names\": [\"{tag_name_1}\", \"{tag_name_2}\"],\n     \"start_date\": \"2023-01-01\",\n     \"end_date\": \"2023-01-31\",\n     \"time_interval_rate\": \"15\",\n     \"time_interval_unit\": \"minute\",\n     \"lower_bound\": 0,\n     \"upper_bound\": 360,\n     \"include_bad_data\": True,\n }\n x = circular_standard_deviation.get(connection, parameters)\n print(x)\n</code></pre> <p>This example is using <code>DefaultAuth()</code> and <code>DatabricksSQLConnection()</code> to authenticate and connect. You can find other ways to authenticate here. The alternative built in connection methods are either by <code>PYODBCSQLConnection()</code>, <code>TURBODBCSQLConnection()</code> or <code>SparkConnection()</code>.</p> <p>Note</p> <p>See Samples Repository for full list of examples.</p> <p>Note</p> <p><code>server_hostname</code> and <code>http_path</code> can be found on the SQL Warehouses Page. </p>"},{"location":"sdk/code-reference/query/functions/time_series/interpolate/","title":"Interpolate Function","text":""},{"location":"sdk/code-reference/query/functions/time_series/interpolate/#src.sdk.python.rtdip_sdk.queries.time_series.interpolate.get","title":"<code>get(connection, parameters_dict)</code>","text":"<p>An RTDIP interpolation function that is intertwined with the RTDIP Resampling function.</p> <p>The Interpolation function will forward fill, backward fill or linearly interpolate the resampled data depending users specified interpolation method.</p> <p>This function requires the user to input a dictionary of parameters. (See Attributes table below.)</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>object</code> <p>Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)</p> required <code>parameters_dict</code> <code>dict</code> <p>A dictionary of parameters (see Attributes table below)</p> required <p>Attributes:</p> Name Type Description <code>business_unit</code> <code>str</code> <p>Business unit of the data</p> <code>region</code> <code>str</code> <p>Region</p> <code>asset</code> <code>str</code> <p>Asset</p> <code>data_security_level</code> <code>str</code> <p>Level of data security</p> <code>data_type</code> <code>str</code> <p>Type of the data (float, integer, double, string)</p> <code>tag_names</code> <code>list</code> <p>List of tagname or tagnames [\"tag_1\", \"tag_2\"]</p> <code>start_date</code> <code>str</code> <p>Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> <code>end_date</code> <code>str</code> <p>End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> <code>sample_rate</code> <code>int</code> <p>(deprecated) Please use time_interval_rate instead. See below.</p> <code>sample_unit</code> <code>str</code> <p>(deprecated) Please use time_interval_unit instead. See below.</p> <code>time_interval_rate</code> <code>str</code> <p>The time interval rate (numeric input)</p> <code>time_interval_unit</code> <code>str</code> <p>The time interval unit (second, minute, day, hour)</p> <code>agg_method</code> <code>str</code> <p>Aggregation Method (first, last, avg, min, max)</p> <code>interpolation_method</code> <code>str</code> <p>Interpolation method (forward_fill, backward_fill, linear)</p> <code>include_bad_data</code> <code>bool</code> <p>Include \"Bad\" data points with True or remove \"Bad\" data points with False</p> <code>pivot</code> <code>bool</code> <p>Pivot the data on timestamp column with True or do not pivot the data with False</p> <code>limit</code> <code>optional int</code> <p>The number of rows to be returned</p> <code>offset</code> <code>optional int</code> <p>The number of rows to skip before returning rows</p> Source code in <code>src/sdk/python/rtdip_sdk/queries/time_series/interpolate.py</code> <pre><code>def get(connection: object, parameters_dict: dict) -&gt; pd.DataFrame:\n\"\"\"\n    An RTDIP interpolation function that is intertwined with the RTDIP Resampling function.\n\n    The Interpolation function will forward fill, backward fill or linearly interpolate the resampled data depending users specified interpolation method.\n\n    This function requires the user to input a dictionary of parameters. (See Attributes table below.)\n\n    Args:\n        connection: Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)\n        parameters_dict: A dictionary of parameters (see Attributes table below)\n\n    Attributes:\n        business_unit (str): Business unit of the data\n        region (str): Region\n        asset (str):  Asset\n        data_security_level (str): Level of data security\n        data_type (str): Type of the data (float, integer, double, string)\n        tag_names (list): List of tagname or tagnames [\"tag_1\", \"tag_2\"]\n        start_date (str): Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        end_date (str): End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        sample_rate (int): (deprecated) Please use time_interval_rate instead. See below.\n        sample_unit (str): (deprecated) Please use time_interval_unit instead. See below.\n        time_interval_rate (str): The time interval rate (numeric input)\n        time_interval_unit (str): The time interval unit (second, minute, day, hour)\n        agg_method (str): Aggregation Method (first, last, avg, min, max)\n        interpolation_method (str): Interpolation method (forward_fill, backward_fill, linear)\n        include_bad_data (bool): Include \"Bad\" data points with True or remove \"Bad\" data points with False\n        pivot (bool): Pivot the data on timestamp column with True or do not pivot the data with False\n        limit (optional int): The number of rows to be returned\n        offset (optional int): The number of rows to skip before returning rows\n    Returns:\n        DataFrame: A resampled and interpolated dataframe.\n    \"\"\"\n    if isinstance(parameters_dict[\"tag_names\"], list) is False:\n        raise ValueError(\"tag_names must be a list\")\n\n    if \"sample_rate\" in parameters_dict:\n        logging.warning(\n            \"Parameter sample_rate is deprecated and will be removed in v1.0.0. Please use time_interval_rate instead.\"\n        )\n        parameters_dict[\"time_interval_rate\"] = parameters_dict[\"sample_rate\"]\n\n    if \"sample_unit\" in parameters_dict:\n        logging.warning(\n            \"Parameter sample_unit is deprecated and will be removed in v1.0.0. Please use time_interval_unit instead.\"\n        )\n        parameters_dict[\"time_interval_unit\"] = parameters_dict[\"sample_unit\"]\n\n    try:\n        query = _query_builder(parameters_dict, \"interpolate\")\n\n        try:\n            cursor = connection.cursor()\n            cursor.execute(query)\n            df = cursor.fetch_all()\n            cursor.close()\n            connection.close()\n            return df\n        except Exception as e:\n            logging.exception(\"error returning dataframe\")\n            raise e\n\n    except Exception as e:\n        logging.exception(\"error with interpolate function\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/functions/time_series/interpolate/#example","title":"Example","text":"<pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import interpolate\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\nparameters = {\n    \"business_unit\": \"{business_unit}\",\n    \"region\": \"{region}\", \n    \"asset\": \"{asset_name}\", \n    \"data_security_level\": \"{security_level}\", \n    \"data_type\": \"float\",\n    \"tag_names\": [\"{tag_name_1}\", \"{tag_name_2}\"],\n    \"start_date\": \"2023-01-01\",\n    \"end_date\": \"2023-01-31\",\n    \"time_interval_rate\": \"15\",\n    \"time_interval_unit\": \"minute\",\n    \"agg_method\": \"first\",\n    \"interpolation_method\": \"forward_fill\",\n    \"include_bad_data\": True,\n}\nx = interpolate.get(connection, parameters)\nprint(x)\n</code></pre> <p>This example is using <code>DefaultAuth()</code> and <code>DatabricksSQLConnection()</code> to authenticate and connect. You can find other ways to authenticate here. The alternative built in connection methods are either by <code>PYODBCSQLConnection()</code>, <code>TURBODBCSQLConnection()</code> or <code>SparkConnection()</code>.</p> <p>Note</p> <p>See Samples Repository for full list of examples.</p> <p>Note</p> <p><code>server_hostname</code> and <code>http_path</code> can be found on the SQL Warehouses Page. </p>"},{"location":"sdk/code-reference/query/functions/time_series/interpolation-at-time/","title":"Interpolation at Time Function","text":""},{"location":"sdk/code-reference/query/functions/time_series/interpolation-at-time/#src.sdk.python.rtdip_sdk.queries.time_series.interpolation_at_time.get","title":"<code>get(connection, parameters_dict)</code>","text":"<p>An RTDIP interpolation at time function which works out the linear interpolation at a specific time based on the points before and after.</p> <p>This function requires the user to input a dictionary of parameters. (See Attributes table below.)</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>object</code> <p>Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)</p> required <code>parameters_dict</code> <code>dict</code> <p>A dictionary of parameters (see Attributes table below)</p> required <p>Attributes:</p> Name Type Description <code>business_unit</code> <code>str</code> <p>Business unit of the data</p> <code>region</code> <code>str</code> <p>Region</p> <code>asset</code> <code>str</code> <p>Asset</p> <code>data_security_level</code> <code>str</code> <p>Level of data security</p> <code>data_type</code> <code>str</code> <p>Type of the data (float, integer, double, string)</p> <code>tag_names</code> <code>str</code> <p>Name of the tag</p> <code>timestamps</code> <code>list</code> <p>List of timestamp or timestamps in the format YYY-MM-DDTHH:MM:SS or YYY-MM-DDTHH:MM:SS+zz:zz where %z is the timezone. (Example +00:00 is the UTC timezone)</p> <code>window_length</code> <code>int</code> <p>Add longer window time in days for the start or end of specified date to cater for edge cases.</p> <code>include_bad_data</code> <code>bool</code> <p>Include \"Bad\" data points with True or remove \"Bad\" data points with False</p> <code>pivot</code> <code>bool</code> <p>Pivot the data on timestamp column with True or do not pivot the data with False</p> <code>limit</code> <code>optional int</code> <p>The number of rows to be returned</p> <code>offset</code> <code>optional int</code> <p>The number of rows to skip before returning rows</p> Source code in <code>src/sdk/python/rtdip_sdk/queries/time_series/interpolation_at_time.py</code> <pre><code>def get(connection: object, parameters_dict: dict) -&gt; pd.DataFrame:\n\"\"\"\n    An RTDIP interpolation at time function which works out the linear interpolation at a specific time based on the points before and after.\n\n    This function requires the user to input a dictionary of parameters. (See Attributes table below.)\n\n    Args:\n        connection: Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)\n        parameters_dict: A dictionary of parameters (see Attributes table below)\n\n    Attributes:\n        business_unit (str): Business unit of the data\n        region (str): Region\n        asset (str):  Asset\n        data_security_level (str): Level of data security\n        data_type (str): Type of the data (float, integer, double, string)\n        tag_names (str): Name of the tag\n        timestamps (list): List of timestamp or timestamps in the format YYY-MM-DDTHH:MM:SS or YYY-MM-DDTHH:MM:SS+zz:zz where %z is the timezone. (Example +00:00 is the UTC timezone)\n        window_length (int): Add longer window time in days for the start or end of specified date to cater for edge cases.\n        include_bad_data (bool): Include \"Bad\" data points with True or remove \"Bad\" data points with False\n        pivot (bool): Pivot the data on timestamp column with True or do not pivot the data with False\n        limit (optional int): The number of rows to be returned\n        offset (optional int): The number of rows to skip before returning rows\n    Returns:\n        DataFrame: A interpolated at time dataframe.\n    \"\"\"\n    if isinstance(parameters_dict[\"tag_names\"], list) is False:\n        raise ValueError(\"tag_names must be a list\")\n\n    if isinstance(parameters_dict[\"timestamps\"], list) is False:\n        raise ValueError(\"timestamps must be a list\")\n\n    try:\n        query = _query_builder(parameters_dict, \"interpolation_at_time\")\n\n        try:\n            cursor = connection.cursor()\n            cursor.execute(query)\n            df = cursor.fetch_all()\n            cursor.close()\n            connection.close()\n            return df\n        except Exception as e:\n            logging.exception(\"error returning dataframe\")\n            raise e\n\n    except Exception as e:\n        logging.exception(\"error with interpolation at time function\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/functions/time_series/interpolation-at-time/#example","title":"Example","text":"<pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import interpolation_at_time\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\nparameters = {\n    \"business_unit\": \"{business_unit}\",\n    \"region\": \"{region}\", \n    \"asset\": \"{asset_name}\", \n    \"data_security_level\": \"{security_level}\", \n    \"data_type\": \"float\",\n    \"tag_names\": [\"{tag_name_1}\", \"{tag_name_2}\"],\n    \"timestamps\": [\"2023-01-01\", \"2023-01-02\"], \n    \"window_length\": 1,\n}\nx = interpolation_at_time.get(connection, parameters)\nprint(x)\n</code></pre> <p>This example is using <code>DefaultAuth()</code> and <code>DatabricksSQLConnection()</code> to authenticate and connect. You can find other ways to authenticate here. The alternative built in connection methods are either by <code>PYODBCSQLConnection()</code>, <code>TURBODBCSQLConnection()</code> or <code>SparkConnection()</code>.</p> <p>Note</p> <p>See Samples Repository for full list of examples.</p> <p>Note</p> <p><code>server_hostname</code> and <code>http_path</code> can be found on the SQL Warehouses Page. </p>"},{"location":"sdk/code-reference/query/functions/time_series/latest/","title":"Latest Function","text":""},{"location":"sdk/code-reference/query/functions/time_series/latest/#src.sdk.python.rtdip_sdk.queries.time_series.latest.get","title":"<code>get(connection, parameters_dict)</code>","text":"<p>A function to return the latest event values by querying databricks SQL Warehouse using a connection specified by the user.</p> <p>The available connectors by RTDIP are Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect.</p> <p>The available authentcation methods are Certificate Authentication, Client Secret Authentication or Default Authentication. See documentation.</p> <p>This function requires the user to input a dictionary of parameters. (See Attributes table below)</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>object</code> <p>Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)</p> required <code>parameters_dict</code> <code>dict</code> <p>A dictionary of parameters (see Attributes table below)</p> required <p>Attributes:</p> Name Type Description <code>business_unit</code> <code>str</code> <p>Business unit</p> <code>region</code> <code>str</code> <p>Region</p> <code>asset</code> <code>str</code> <p>Asset</p> <code>data_security_level</code> <code>str</code> <p>Level of data security</p> <code>tag_names</code> <code>(optional, list)</code> <p>Either pass a list of tagname/tagnames [\"tag_1\", \"tag_2\"] or leave the list blank [] or leave the parameter out completely</p> <code>limit</code> <code>optional int</code> <p>The number of rows to be returned</p> <code>offset</code> <code>optional int</code> <p>The number of rows to skip before returning rows</p> Source code in <code>src/sdk/python/rtdip_sdk/queries/time_series/latest.py</code> <pre><code>def get(connection: object, parameters_dict: dict) -&gt; pd.DataFrame:\n\"\"\"\n    A function to return the latest event values by querying databricks SQL Warehouse using a connection specified by the user.\n\n    The available connectors by RTDIP are Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect.\n\n    The available authentcation methods are Certificate Authentication, Client Secret Authentication or Default Authentication. See documentation.\n\n    This function requires the user to input a dictionary of parameters. (See Attributes table below)\n\n    Args:\n        connection: Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)\n        parameters_dict: A dictionary of parameters (see Attributes table below)\n\n    Attributes:\n        business_unit (str): Business unit\n        region (str): Region\n        asset (str): Asset\n        data_security_level (str): Level of data security\n        tag_names (optional, list): Either pass a list of tagname/tagnames [\"tag_1\", \"tag_2\"] or leave the list blank [] or leave the parameter out completely\n        limit (optional int): The number of rows to be returned\n        offset (optional int): The number of rows to skip before returning rows\n    Returns:\n        DataFrame: A dataframe of event latest values.\n    \"\"\"\n    try:\n        query = _query_builder(parameters_dict, \"latest\")\n\n        try:\n            cursor = connection.cursor()\n            cursor.execute(query)\n            df = cursor.fetch_all()\n            cursor.close()\n            connection.close()\n            return df\n        except Exception as e:\n            logging.exception(\"error returning dataframe\")\n            raise e\n\n    except Exception as e:\n        logging.exception(\"error returning latest function\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/functions/time_series/latest/#example","title":"Example","text":"<pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import latest\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\nparameters = {\n    \"business_unit\": \"{business_unit}\",\n    \"region\": \"{region}\", \n    \"asset\": \"{asset_name}\", \n    \"data_security_level\": \"{security_level}\", \n    \"tag_names\": [\"{tag_name_1}\", \"{tag_name_2}\"],\n}\nx = latest.get(connection, parameters)\nprint(x)\n</code></pre> <p>This example is using <code>DefaultAuth()</code> and <code>DatabricksSQLConnection()</code> to authenticate and connect. You can find other ways to authenticate here. The alternative built in connection methods are either by <code>PYODBCSQLConnection()</code>, <code>TURBODBCSQLConnection()</code> or <code>SparkConnection()</code>.</p> <p>Note</p> <p>See Samples Repository for full list of examples.</p> <p>Note</p> <p><code>server_hostname</code> and <code>http_path</code> can be found on the SQL Warehouses Page. </p>"},{"location":"sdk/code-reference/query/functions/time_series/raw/","title":"Raw Function","text":""},{"location":"sdk/code-reference/query/functions/time_series/raw/#src.sdk.python.rtdip_sdk.queries.time_series.raw.get","title":"<code>get(connection, parameters_dict)</code>","text":"<p>A function to return back raw data by querying databricks SQL Warehouse using a connection specified by the user.</p> <p>The available connectors by RTDIP are Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect.</p> <p>The available authentication methods are Certificate Authentication, Client Secret Authentication or Default Authentication. See documentation.</p> <p>This function requires the user to input a dictionary of parameters. (See Attributes table below)</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>object</code> <p>Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)</p> required <code>parameters_dict</code> <code>dict</code> <p>A dictionary of parameters (see Attributes table below)</p> required <p>Attributes:</p> Name Type Description <code>business_unit</code> <code>str</code> <p>Business unit</p> <code>region</code> <code>str</code> <p>Region</p> <code>asset</code> <code>str</code> <p>Asset</p> <code>data_security_level</code> <code>str</code> <p>Level of data security</p> <code>data_type</code> <code>str</code> <p>Type of the data (float, integer, double, string)</p> <code>tag_names</code> <code>list</code> <p>List of tagname or tagnames [\"tag_1\", \"tag_2\"]</p> <code>start_date</code> <code>str</code> <p>Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> <code>end_date</code> <code>str</code> <p>End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> <code>include_bad_data</code> <code>bool</code> <p>Include \"Bad\" data points with True or remove \"Bad\" data points with False</p> <code>limit</code> <code>optional int</code> <p>The number of rows to be returned</p> <code>offset</code> <code>optional int</code> <p>The number of rows to skip before returning rows</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe of raw timeseries data.</p> Source code in <code>src/sdk/python/rtdip_sdk/queries/time_series/raw.py</code> <pre><code>def get(connection: object, parameters_dict: dict) -&gt; pd.DataFrame:\n\"\"\"\n    A function to return back raw data by querying databricks SQL Warehouse using a connection specified by the user.\n\n    The available connectors by RTDIP are Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect.\n\n    The available authentication methods are Certificate Authentication, Client Secret Authentication or Default Authentication. See documentation.\n\n    This function requires the user to input a dictionary of parameters. (See Attributes table below)\n\n    Args:\n        connection: Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)\n        parameters_dict: A dictionary of parameters (see Attributes table below)\n\n    Attributes:\n        business_unit (str): Business unit\n        region (str): Region\n        asset (str): Asset\n        data_security_level (str): Level of data security\n        data_type (str): Type of the data (float, integer, double, string)\n        tag_names (list): List of tagname or tagnames [\"tag_1\", \"tag_2\"]\n        start_date (str): Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        end_date (str): End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        include_bad_data (bool): Include \"Bad\" data points with True or remove \"Bad\" data points with False\n        limit (optional int): The number of rows to be returned\n        offset (optional int): The number of rows to skip before returning rows\n\n    Returns:\n        DataFrame: A dataframe of raw timeseries data.\n    \"\"\"\n    try:\n        query = _query_builder(parameters_dict, \"raw\")\n\n        try:\n            cursor = connection.cursor()\n            cursor.execute(query)\n            df = cursor.fetch_all()\n            cursor.close()\n            connection.close()\n            return df\n        except Exception as e:\n            logging.exception(\"error returning dataframe\")\n            raise e\n\n    except Exception as e:\n        logging.exception(\"error with raw function\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/functions/time_series/raw/#example","title":"Example","text":"<pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import raw\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\nparameters = {\n    \"business_unit\": \"{business_unit}\",\n    \"region\": \"{region}\", \n    \"asset\": \"{asset_name}\", \n    \"data_security_level\": \"{security_level}\", \n    \"data_type\": \"float\",\n    \"tag_names\": [\"{tag_name_1}\", \"{tag_name_2}\"],\n    \"start_date\": \"2023-01-01\",\n    \"end_date\": \"2023-01-31\",\n    \"include_bad_data\": True,\n}\nx = raw.get(connection, parameters)\nprint(x)\n</code></pre> <p>This example is using <code>DefaultAuth()</code> and <code>DatabricksSQLConnection()</code> to authenticate and connect. You can find other ways to authenticate here. The alternative built in connection methods are either by <code>PYODBCSQLConnection()</code>, <code>TURBODBCSQLConnection()</code> or <code>SparkConnection()</code>.</p> <p>Note</p> <p>See Samples Repository for full list of examples.</p> <p>Note</p> <p><code>server_hostname</code> and <code>http_path</code> can be found on the SQL Warehouses Page. </p>"},{"location":"sdk/code-reference/query/functions/time_series/resample/","title":"Resample Function","text":""},{"location":"sdk/code-reference/query/functions/time_series/resample/#src.sdk.python.rtdip_sdk.queries.time_series.resample.get","title":"<code>get(connection, parameters_dict)</code>","text":"<p>An RTDIP Resampling function in spark to resample data by querying databricks SQL warehouses using a connection and authentication method specified by the user. This spark resample function will return a resampled dataframe.</p> <p>The available connectors by RTDIP are Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect.</p> <p>The available authentication methods are Certificate Authentication, Client Secret Authentication or Default Authentication. See documentation.</p> <p>This function requires the user to input a dictionary of parameters. (See Attributes table below)</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>object</code> <p>Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)</p> required <code>parameters_dict</code> <code>dict</code> <p>A dictionary of parameters (see Attributes table below)</p> required <p>Attributes:</p> Name Type Description <code>business_unit</code> <code>str</code> <p>Business unit of the data</p> <code>region</code> <code>str</code> <p>Region</p> <code>asset</code> <code>str</code> <p>Asset</p> <code>data_security_level</code> <code>str</code> <p>Level of data security</p> <code>data_type</code> <code>str</code> <p>Type of the data (float, integer, double, string)</p> <code>tag_names</code> <code>list</code> <p>List of tagname or tagnames [\"tag_1\", \"tag_2\"]</p> <code>start_date</code> <code>str</code> <p>Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> <code>end_date</code> <code>str</code> <p>End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> <code>sample_rate</code> <code>int</code> <p>(deprecated) Please use time_interval_rate instead. See below.</p> <code>sample_unit</code> <code>str</code> <p>(deprecated) Please use time_interval_unit instead. See below.</p> <code>time_interval_rate</code> <code>str</code> <p>The time interval rate (numeric input)</p> <code>time_interval_unit</code> <code>str</code> <p>The time interval unit (second, minute, day, hour)</p> <code>agg_method</code> <code>str</code> <p>Aggregation Method (first, last, avg, min, max)</p> <code>include_bad_data</code> <code>bool</code> <p>Include \"Bad\" data points with True or remove \"Bad\" data points with False</p> <code>pivot</code> <code>bool</code> <p>Pivot the data on timestamp column with True or do not pivot the data with False</p> <code>limit</code> <code>optional int</code> <p>The number of rows to be returned</p> <code>offset</code> <code>optional int</code> <p>The number of rows to skip before returning rows</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A resampled dataframe.</p> Source code in <code>src/sdk/python/rtdip_sdk/queries/time_series/resample.py</code> <pre><code>def get(connection: object, parameters_dict: dict) -&gt; pd.DataFrame:\n\"\"\"\n    An RTDIP Resampling function in spark to resample data by querying databricks SQL warehouses using a connection and authentication method specified by the user. This spark resample function will return a resampled dataframe.\n\n    The available connectors by RTDIP are Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect.\n\n    The available authentication methods are Certificate Authentication, Client Secret Authentication or Default Authentication. See documentation.\n\n    This function requires the user to input a dictionary of parameters. (See Attributes table below)\n\n    Args:\n        connection: Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)\n        parameters_dict: A dictionary of parameters (see Attributes table below)\n\n    Attributes:\n        business_unit (str): Business unit of the data\n        region (str): Region\n        asset (str):  Asset\n        data_security_level (str): Level of data security\n        data_type (str): Type of the data (float, integer, double, string)\n        tag_names (list): List of tagname or tagnames [\"tag_1\", \"tag_2\"]\n        start_date (str): Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        end_date (str): End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        sample_rate (int): (deprecated) Please use time_interval_rate instead. See below.\n        sample_unit (str): (deprecated) Please use time_interval_unit instead. See below.\n        time_interval_rate (str): The time interval rate (numeric input)\n        time_interval_unit (str): The time interval unit (second, minute, day, hour)\n        agg_method (str): Aggregation Method (first, last, avg, min, max)\n        include_bad_data (bool): Include \"Bad\" data points with True or remove \"Bad\" data points with False\n        pivot (bool): Pivot the data on timestamp column with True or do not pivot the data with False\n        limit (optional int): The number of rows to be returned\n        offset (optional int): The number of rows to skip before returning rows\n\n    Returns:\n        DataFrame: A resampled dataframe.\n    \"\"\"\n    if isinstance(parameters_dict[\"tag_names\"], list) is False:\n        raise ValueError(\"tag_names must be a list\")\n\n    if \"sample_rate\" in parameters_dict:\n        logging.warning(\n            \"Parameter sample_rate is deprecated and will be removed in v1.0.0. Please use time_interval_rate instead.\"\n        )\n        parameters_dict[\"time_interval_rate\"] = parameters_dict[\"sample_rate\"]\n\n    if \"sample_unit\" in parameters_dict:\n        logging.warning(\n            \"Parameter sample_unit is deprecated and will be removed in v1.0.0. Please use time_interval_unit instead.\"\n        )\n        parameters_dict[\"time_interval_unit\"] = parameters_dict[\"sample_unit\"]\n\n    try:\n        query = _query_builder(parameters_dict, \"resample\")\n\n        try:\n            cursor = connection.cursor()\n            cursor.execute(query)\n            df = cursor.fetch_all()\n            cursor.close()\n            connection.close()\n            return df\n        except Exception as e:\n            logging.exception(\"error returning dataframe\")\n            raise e\n\n    except Exception as e:\n        logging.exception(\"error with resampling function\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/functions/time_series/resample/#example","title":"Example","text":"<pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import resample\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\nparameters = {\n    \"business_unit\": \"{business_unit}\",\n    \"region\": \"{region}\", \n    \"asset\": \"{asset_name}\", \n    \"data_security_level\": \"{security_level}\", \n    \"data_type\": \"float\",\n    \"tag_names\": [\"{tag_name_1}\", \"{tag_name_2}\"],\n    \"start_date\": \"2023-01-01\",\n    \"end_date\": \"2023-01-31\",\n    \"time_interval_rate\": \"15\",\n    \"time_interval_unit\": \"minute\",\n    \"agg_method\": \"first\",\n    \"include_bad_data\": True,\n}\nx = resample.get(connection, parameters)\nprint(x)\n</code></pre> <p>This example is using <code>DefaultAuth()</code> and <code>DatabricksSQLConnection()</code> to authenticate and connect. You can find other ways to authenticate here. The alternative built in connection methods are either by <code>PYODBCSQLConnection()</code>, <code>TURBODBCSQLConnection()</code> or <code>SparkConnection()</code>.</p> <p>Note</p> <p>See Samples Repository for full list of examples.</p> <p>Note</p> <p><code>server_hostname</code> and <code>http_path</code> can be found on the SQL Warehouses Page. </p>"},{"location":"sdk/code-reference/query/functions/time_series/summary/","title":"Summary Function","text":""},{"location":"sdk/code-reference/query/functions/time_series/summary/#src.sdk.python.rtdip_sdk.queries.time_series.summary.get","title":"<code>get(connection, parameters_dict)</code>","text":"<p>A function to return back a summary of statistics (Avg, Min, Max, Count, StDev, Sum, Variance) by querying databricks SQL Warehouse using a connection specified by the user.</p> <p>The available connectors by RTDIP are Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect.</p> <p>The available authentication methods are Certificate Authentication, Client Secret Authentication or Default Authentication. See documentation.</p> <p>This function requires the user to input a dictionary of parameters. (See Attributes table below)</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>object</code> <p>Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)</p> required <code>parameters_dict</code> <code>dict</code> <p>A dictionary of parameters (see Attributes table below)</p> required <p>Attributes:</p> Name Type Description <code>business_unit</code> <code>str</code> <p>Business unit</p> <code>region</code> <code>str</code> <p>Region</p> <code>asset</code> <code>str</code> <p>Asset</p> <code>data_security_level</code> <code>str</code> <p>Level of data security</p> <code>data_type</code> <code>str</code> <p>Type of the data (float, integer, double, string)</p> <code>tag_names</code> <code>list</code> <p>List of tagname or tagnames [\"tag_1\", \"tag_2\"]</p> <code>start_date</code> <code>str</code> <p>Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> <code>end_date</code> <code>str</code> <p>End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> <code>include_bad_data</code> <code>bool</code> <p>Include \"Bad\" data points with True or remove \"Bad\" data points with False</p> <code>limit</code> <code>optional int</code> <p>The number of rows to be returned</p> <code>offset</code> <code>optional int</code> <p>The number of rows to skip before returning rows</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe of summary statistics.</p> Source code in <code>src/sdk/python/rtdip_sdk/queries/time_series/summary.py</code> <pre><code>def get(connection: object, parameters_dict: dict) -&gt; pd.DataFrame:\n\"\"\"\n    A function to return back a summary of statistics (Avg, Min, Max, Count, StDev, Sum, Variance) by querying databricks SQL Warehouse using a connection specified by the user.\n\n    The available connectors by RTDIP are Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect.\n\n    The available authentication methods are Certificate Authentication, Client Secret Authentication or Default Authentication. See documentation.\n\n    This function requires the user to input a dictionary of parameters. (See Attributes table below)\n\n    Args:\n        connection: Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)\n        parameters_dict: A dictionary of parameters (see Attributes table below)\n\n    Attributes:\n        business_unit (str): Business unit\n        region (str): Region\n        asset (str): Asset\n        data_security_level (str): Level of data security\n        data_type (str): Type of the data (float, integer, double, string)\n        tag_names (list): List of tagname or tagnames [\"tag_1\", \"tag_2\"]\n        start_date (str): Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        end_date (str): End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        include_bad_data (bool): Include \"Bad\" data points with True or remove \"Bad\" data points with False\n        limit (optional int): The number of rows to be returned\n        offset (optional int): The number of rows to skip before returning rows\n\n    Returns:\n        DataFrame: A dataframe of summary statistics.\n    \"\"\"\n    try:\n        query = _query_builder(parameters_dict, \"summary\")\n\n        try:\n            cursor = connection.cursor()\n            cursor.execute(query)\n            df = cursor.fetch_all()\n            cursor.close()\n            connection.close()\n            return df\n        except Exception as e:\n            logging.exception(\"error returning summary dataframe\")\n            raise e\n\n    except Exception as e:\n        logging.exception(\"error with summary function\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/functions/time_series/summary/#example","title":"Example","text":"<pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import summary\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\nparameters = {\n    \"business_unit\": \"{business_unit}\",\n    \"region\": \"{region}\", \n    \"asset\": \"{asset_name}\", \n    \"data_security_level\": \"{security_level}\", \n    \"data_type\": \"float\",\n    \"tag_names\": [\"{tag_name_1}\", \"{tag_name_2}\"],\n    \"start_date\": \"2023-01-01\",\n    \"end_date\": \"2023-01-31\",\n    \"include_bad_data\": True,\n}\nx = summary.get(connection, parameters)\nprint(x)\n</code></pre> <p>This example is using <code>DefaultAuth()</code> and <code>DatabricksSQLConnection()</code> to authenticate and connect. You can find other ways to authenticate here. The alternative built in connection methods are either by <code>PYODBCSQLConnection()</code>, <code>TURBODBCSQLConnection()</code> or <code>SparkConnection()</code>.</p> <p>Note</p> <p>See Samples Repository for full list of examples.</p> <p>Note</p> <p><code>server_hostname</code> and <code>http_path</code> can be found on the SQL Warehouses Page. </p>"},{"location":"sdk/code-reference/query/functions/time_series/time-weighted-average/","title":"Time Weighted Average","text":""},{"location":"sdk/code-reference/query/functions/time_series/time-weighted-average/#src.sdk.python.rtdip_sdk.queries.time_series.time_weighted_average.get","title":"<code>get(connection, parameters_dict)</code>","text":"<p>A function that receives a dataframe of raw tag data and performs a time weighted averages, returning the results.</p> <p>This function requires the input of a pandas dataframe acquired via the rtdip.functions.raw() method and the user to input a dictionary of parameters. (See Attributes table below)</p> <p>Pi data points will either have step enabled (True) or step disabled (False). You can specify whether you want step to be fetched by \"Pi\" or you can set the step parameter to True/False in the dictionary below.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>object</code> <p>Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)</p> required <code>parameters_dict</code> <code>dict</code> <p>A dictionary of parameters (see Attributes table below)</p> required Source code in <code>src/sdk/python/rtdip_sdk/queries/time_series/time_weighted_average.py</code> <pre><code>def get(connection: object, parameters_dict: dict) -&gt; pd.DataFrame:\n\"\"\"\n    A function that receives a dataframe of raw tag data and performs a time weighted averages, returning the results.\n\n    This function requires the input of a pandas dataframe acquired via the rtdip.functions.raw() method and the user to input a dictionary of parameters. (See Attributes table below)\n\n    Pi data points will either have step enabled (True) or step disabled (False). You can specify whether you want step to be fetched by \"Pi\" or you can set the step parameter to True/False in the dictionary below.\n\n    Args:\n        connection: Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)\n        parameters_dict (dict): A dictionary of parameters (see Attributes table below)\n    Attributes:\n        business_unit (str): Business unit\n        region (str): Region\n        asset (str): Asset\n        data_security_level (str): Level of data security\n        data_type (str): Type of the data (float, integer, double, string)\n        tag_names (list): List of tagname or tagnames\n        start_date (str): Start date (Either a utc date in the format YYYY-MM-DD or a utc datetime in the format YYYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        end_date (str): End date (Either a utc date in the format YYYY-MM-DD or a utc datetime in the format YYYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        window_size_mins (int): (deprecated) Window size in minutes. Please use time_interval_rate and time_interval_unit below instead.\n        time_interval_rate (str): The time interval rate (numeric input)\n        time_interval_unit (str): The time interval unit (second, minute, day, hour)\n        window_length (int): Add longer window time in days for the start or end of specified date to cater for edge cases.\n        include_bad_data (bool): Include \"Bad\" data points with True or remove \"Bad\" data points with False\n        step (str): data points with step \"enabled\" or \"disabled\". The options for step are \"true\", \"false\" or \"metadata\". \"metadata\" will retrieve the step value from the metadata table.\n        pivot (bool): Pivot the data on timestamp column with True or do not pivot the data with False\n        limit (optional int): The number of rows to be returned\n        offset (optional int): The number of rows to skip before returning rows\n    Returns:\n        DataFrame: A dataframe containing the time weighted averages.\n    \"\"\"\n    if isinstance(parameters_dict[\"tag_names\"], list) is False:\n        raise ValueError(\"tag_names must be a list\")\n\n    if \"window_size_mins\" in parameters_dict:\n        logging.warning(\n            \"Parameter window_size_mins is deprecated and will be removed in v1.0.0. Please use time_interval_rate and time_interval_unit instead.\"\n        )\n        parameters_dict[\"time_interval_rate\"] = str(parameters_dict[\"window_size_mins\"])\n        parameters_dict[\"time_interval_unit\"] = \"minute\"\n\n    try:\n        query = _query_builder(parameters_dict, \"time_weighted_average\")\n\n        try:\n            cursor = connection.cursor()\n            cursor.execute(query)\n            df = cursor.fetch_all()\n            cursor.close()\n            connection.close()\n            return df\n        except Exception as e:\n            logging.exception(\"error returning dataframe\")\n            raise e\n\n    except Exception as e:\n        logging.exception(\"error with time weighted average function\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/functions/time_series/time-weighted-average/#example","title":"Example","text":"<pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import time_weighted_average\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\nparameters = {\n    \"business_unit\": \"{business_unit}\",\n    \"region\": \"{region}\", \n    \"asset\": \"{asset_name}\", \n    \"data_security_level\": \"{security_level}\", \n    \"data_type\": \"float\",\n    \"tag_names\": [\"{tag_name_1}\", \"{tag_name_2}\"],\n    \"start_date\": \"2023-01-01\",\n    \"end_date\": \"2023-01-31\",\n    \"time_interval_rate\": \"15\",\n    \"time_interval_unit\": \"minute\",\n    \"window_length\": 1,\n    \"include_bad_data\": True,\n    \"step\": \"true\"\n}\nx = time_weighted_average.get(connection, parameters)\nprint(x)\n</code></pre> <p>This example is using <code>DefaultAuth()</code> and <code>DatabricksSQLConnection()</code> to authenticate and connect. You can find other ways to authenticate here. The alternative built in connection methods are either by <code>PYODBCSQLConnection()</code>, <code>TURBODBCSQLConnection()</code> or <code>SparkConnection()</code>.</p> <p>Note</p> <p>See Samples Repository for full list of examples.</p> <p>Note</p> <p><code>server_hostname</code> and <code>http_path</code> can be found on the SQL Warehouses Page. </p>"},{"location":"sdk/code-reference/query/functions/time_series/time_series_query_builder/","title":"Query Builder","text":""},{"location":"sdk/code-reference/query/functions/time_series/time_series_query_builder/#src.sdk.python.rtdip_sdk.queries.time_series.time_series_query_builder.TimeSeriesQueryBuilder","title":"<code>TimeSeriesQueryBuilder</code>","text":"<p>A builder for developing RTDIP queries using any delta table.</p> Source code in <code>src/sdk/python/rtdip_sdk/queries/time_series/time_series_query_builder.py</code> <pre><code>class TimeSeriesQueryBuilder:\n\"\"\"\n    A builder for developing RTDIP queries using any delta table.\n    \"\"\"\n\n    parameters: dict\n    connection: ConnectionInterface\n    close_connection: bool\n    data_source: str\n    tagname_column: str\n    timestamp_column: str\n    status_column: str\n    value_column: str\n\n    def connect(self, connection: ConnectionInterface):\n\"\"\"\n        Specifies the connection to be used for the query.\n\n        Args:\n            connection: Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)\n        \"\"\"\n        self.connection = connection\n        return self\n\n    def source(\n        self,\n        source: str,\n        tagname_column: str = \"TagName\",\n        timestamp_column: str = \"EventTime\",\n        status_column: Union[str, None] = \"Status\",\n        value_column: str = \"Value\",\n    ):\n\"\"\"\n        Specifies the source of the query.\n\n        Args:\n            source (str): Source of the query can be a Unity Catalog table, Hive metastore table or path\n            tagname_column (optional str): The column name in the source that contains the tagnames or series\n            timestamp_column (optional str): The timestamp column name in the source\n            status_column (optional str): The status column name in the source indicating `Good` or `Bad`. If this is not available, specify `None`\n            value_column (optional str): The value column name in the source which is normally a float or string value for the time series event\n        \"\"\"\n        self.data_source = \"`.`\".join(source.split(\".\"))\n        self.tagname_column = tagname_column\n        self.timestamp_column = timestamp_column\n        self.status_column = status_column\n        self.value_column = value_column\n        return self\n\n    def raw(\n        self,\n        tagname_filter: [str],\n        start_date: str,\n        end_date: str,\n        include_bad_data: bool = False,\n        limit: int = None,\n        offset: int = None,\n    ) -&gt; DataFrame:\n\"\"\"\n        A function to return back raw data.\n\n        Args:\n            tagname_filter (list str): List of tagnames to filter on the source\n            start_date (str): Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n            end_date (str): End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n            include_bad_data (optional bool): Include \"Bad\" data points with True or remove \"Bad\" data points with False\n            limit (optional int): The number of rows to be returned\n            offset (optional int): The number of rows to skip before returning rows\n\n        Returns:\n            DataFrame: A dataframe of raw timeseries data.\n        \"\"\"\n        raw_parameters = {\n            \"source\": self.data_source,\n            \"tag_names\": tagname_filter,\n            \"start_date\": start_date,\n            \"end_date\": end_date,\n            \"include_bad_data\": include_bad_data,\n            \"limit\": limit,\n            \"offset\": offset,\n            \"tagname_column\": self.tagname_column,\n            \"timestamp_column\": self.timestamp_column,\n            \"status_column\": self.status_column,\n            \"value_column\": self.value_column,\n            \"supress_warning\": True,\n        }\n        return raw.get(self.connection, raw_parameters)\n\n    def resample(\n        self,\n        tagname_filter: [str],\n        start_date: str,\n        end_date: str,\n        time_interval_rate: str,\n        time_interval_unit: str,\n        agg_method: str,\n        include_bad_data: bool = False,\n        pivot: bool = False,\n        limit: int = None,\n        offset: int = None,\n    ) -&gt; DataFrame:\n\"\"\"\n        A query to resample the source data.\n\n        Args:\n            tagname_filter (list str): List of tagnames to filter on the source\n            start_date (str): Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n            end_date (str): End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n            time_interval_rate (str): The time interval rate (numeric input)\n            time_interval_unit (str): The time interval unit (second, minute, day, hour)\n            agg_method (str): Aggregation Method (first, last, avg, min, max)\n            include_bad_data (optional bool): Include \"Bad\" data points with True or remove \"Bad\" data points with False\n            pivot (optional bool): Pivot the data on the timestamp column with True or do not pivot the data with False\n            limit (optional int): The number of rows to be returned\n            offset (optional int): The number of rows to skip before returning rows\n\n        Returns:\n            DataFrame: A dataframe of resampled timeseries data.\n        \"\"\"\n\n        resample_parameters = {\n            \"source\": self.data_source,\n            \"tag_names\": tagname_filter,\n            \"start_date\": start_date,\n            \"end_date\": end_date,\n            \"include_bad_data\": include_bad_data,\n            \"time_interval_rate\": time_interval_rate,\n            \"time_interval_unit\": time_interval_unit,\n            \"agg_method\": agg_method,\n            \"pivot\": pivot,\n            \"limit\": limit,\n            \"offset\": offset,\n            \"tagname_column\": self.tagname_column,\n            \"timestamp_column\": self.timestamp_column,\n            \"status_column\": self.status_column,\n            \"value_column\": self.value_column,\n            \"supress_warning\": True,\n        }\n\n        return resample.get(self.connection, resample_parameters)\n\n    def interpolate(\n        self,\n        tagname_filter: [str],\n        start_date: str,\n        end_date: str,\n        time_interval_rate: str,\n        time_interval_unit: str,\n        agg_method: str,\n        interpolation_method: str,\n        include_bad_data: bool = False,\n        pivot: bool = False,\n        limit: int = None,\n        offset: int = None,\n    ) -&gt; DataFrame:\n\"\"\"\n        The Interpolate function will forward fill, backward fill or linearly interpolate the resampled data depending on the parameters specified.\n\n        Args:\n            tagname_filter (list str): List of tagnames to filter on the source\n            start_date (str): Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n            end_date (str): End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n            time_interval_rate (str): The time interval rate (numeric input)\n            time_interval_unit (str): The time interval unit (second, minute, day, hour)\n            agg_method (str): Aggregation Method (first, last, avg, min, max)\n            interpolation_method (str): Interpolation method (forward_fill, backward_fill, linear)\n            include_bad_data (optional bool): Include \"Bad\" data points with True or remove \"Bad\" data points with False\n            pivot (optional bool): Pivot the data on the timestamp column with True or do not pivot the data with False\n            limit (optional int): The number of rows to be returned\n            offset (optional int): The number of rows to skip before returning rows\n\n        Returns:\n            DataFrame: A dataframe of interpolated timeseries data.\n        \"\"\"\n        interpolation_parameters = {\n            \"source\": self.data_source,\n            \"tag_names\": tagname_filter,\n            \"start_date\": start_date,\n            \"end_date\": end_date,\n            \"include_bad_data\": include_bad_data,\n            \"time_interval_rate\": time_interval_rate,\n            \"time_interval_unit\": time_interval_unit,\n            \"agg_method\": agg_method,\n            \"interpolation_method\": interpolation_method,\n            \"pivot\": pivot,\n            \"limit\": limit,\n            \"offset\": offset,\n            \"tagname_column\": self.tagname_column,\n            \"timestamp_column\": self.timestamp_column,\n            \"status_column\": self.status_column,\n            \"value_column\": self.value_column,\n            \"supress_warning\": True,\n        }\n\n        return interpolate.get(self.connection, interpolation_parameters)\n\n    def interpolation_at_time(\n        self,\n        tagname_filter: [str],\n        timestamp_filter: [str],\n        include_bad_data: bool = False,\n        window_length: int = 1,\n        pivot: bool = False,\n        limit: int = None,\n        offset: int = None,\n    ) -&gt; DataFrame:\n\"\"\"\n        A interpolation at time function which works out the linear interpolation at a specific time based on the points before and after.\n\n        Args:\n            tagname_filter (list str): List of tagnames to filter on the source\n            timestamp_filter (list): List of timestamp or timestamps in the format YYY-MM-DDTHH:MM:SS or YYY-MM-DDTHH:MM:SS+zz:zz where %z is the timezone. (Example +00:00 is the UTC timezone)\n            include_bad_data (optional bool): Include \"Bad\" data points with True or remove \"Bad\" data points with False\n            window_length (optional int): Add longer window time in days for the start or end of specified date to cater for edge cases\n            pivot (optional bool): Pivot the data on the timestamp column with True or do not pivot the data with False\n            limit (optional int): The number of rows to be returned\n            offset (optional int): The number of rows to skip before returning rows\n\n        Returns:\n            DataFrame: A dataframe of interpolation at time timeseries data\n        \"\"\"\n        interpolation_at_time_parameters = {\n            \"source\": self.data_source,\n            \"tag_names\": tagname_filter,\n            \"timestamps\": timestamp_filter,\n            \"include_bad_data\": include_bad_data,\n            \"window_length\": window_length,\n            \"pivot\": pivot,\n            \"limit\": limit,\n            \"offset\": offset,\n            \"tagname_column\": self.tagname_column,\n            \"timestamp_column\": self.timestamp_column,\n            \"status_column\": self.status_column,\n            \"value_column\": self.value_column,\n            \"supress_warning\": True,\n        }\n\n        return interpolation_at_time.get(\n            self.connection, interpolation_at_time_parameters\n        )\n\n    def time_weighted_average(\n        self,\n        tagname_filter: [str],\n        start_date: str,\n        end_date: str,\n        time_interval_rate: str,\n        time_interval_unit: str,\n        step: str,\n        source_metadata: str = None,\n        include_bad_data: bool = False,\n        window_length: int = 1,\n        pivot: bool = False,\n        limit: int = None,\n        offset: int = None,\n    ) -&gt; DataFrame:\n\"\"\"\n        A function that receives a dataframe of raw tag data and performs a time weighted averages.\n\n        Args:\n            tagname_filter (list str): List of tagnames to filter on the source\n            start_date (str): Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n            end_date (str): End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n            time_interval_rate (str): The time interval rate (numeric input)\n            time_interval_unit (str): The time interval unit (second, minute, day, hour)\n            step (str): data points with step \"enabled\" or \"disabled\". The options for step are \"true\", \"false\" or \"metadata\". \"metadata\" will retrieve the step value from the metadata table\n            source_metadata (optional str): if step is set to \"metadata\", then this parameter must be populated with the source containing the tagname metadata with a column called \"Step\"\n            include_bad_data (optional bool): Include \"Bad\" data points with True or remove \"Bad\" data points with False\n            window_length (optional int): Add longer window time in days for the start or end of specified date to cater for edge cases\n            pivot (optional bool): Pivot the data on the timestamp column with True or do not pivot the data with False\n            limit (optional int): The number of rows to be returned\n            offset (optional int): The number of rows to skip before returning rows\n\n        Returns:\n            DataFrame: A dataframe of time weighted averages timeseries data\n        \"\"\"\n        time_weighted_average_parameters = {\n            \"source\": self.data_source,\n            \"tag_names\": tagname_filter,\n            \"start_date\": start_date,\n            \"end_date\": end_date,\n            \"include_bad_data\": include_bad_data,\n            \"time_interval_rate\": time_interval_rate,\n            \"time_interval_unit\": time_interval_unit,\n            \"step\": step,\n            \"source_metadata\": None\n            if source_metadata is None\n            else \"`.`\".join(source_metadata.split(\".\")),\n            \"window_length\": window_length,\n            \"pivot\": pivot,\n            \"limit\": limit,\n            \"offset\": offset,\n            \"tagname_column\": self.tagname_column,\n            \"timestamp_column\": self.timestamp_column,\n            \"status_column\": self.status_column,\n            \"value_column\": self.value_column,\n            \"supress_warning\": True,\n        }\n\n        return time_weighted_average.get(\n            self.connection, time_weighted_average_parameters\n        )\n\n    def metadata(\n        self,\n        tagname_filter: [str] = None,\n        limit: int = None,\n        offset: int = None,\n    ) -&gt; DataFrame:\n\"\"\"\n        A query to retrieve metadata.\n\n        Args:\n            tagname_filter (list str): List of tagnames to filter on the source\n            limit (optional int): The number of rows to be returned\n            offset (optional int): The number of rows to skip before returning rows\n\n        Returns:\n            DataFrame: A dataframe of metadata\n        \"\"\"\n        metadata_parameters = {\n            \"source\": self.data_source,\n            \"tag_names\": [] if tagname_filter is None else tagname_filter,\n            \"tagname_column\": self.tagname_column,\n            \"limit\": limit,\n            \"offset\": offset,\n            \"supress_warning\": True,\n        }\n\n        return metadata.get(self.connection, metadata_parameters)\n\n    def latest(\n        self,\n        tagname_filter: [str] = None,\n        limit: int = None,\n        offset: int = None,\n    ) -&gt; DataFrame:\n\"\"\"\n        A query to retrieve latest event_values.\n\n        Args:\n            tagname_filter (list str): List of tagnames to filter on the source\n            limit (optional int): The number of rows to be returned\n            offset (optional int): The number of rows to skip before returning rows\n\n        Returns:\n            DataFrame: A dataframe of events latest_values\n        \"\"\"\n        latest_parameters = {\n            \"source\": self.data_source,\n            \"tag_names\": [] if tagname_filter is None else tagname_filter,\n            \"tagname_column\": self.tagname_column,\n            \"limit\": limit,\n            \"offset\": offset,\n            \"supress_warning\": True,\n        }\n\n        return latest.get(self.connection, latest_parameters)\n\n    def circular_average(\n        self,\n        tagname_filter: [str],\n        start_date: str,\n        end_date: str,\n        time_interval_rate: str,\n        time_interval_unit: str,\n        lower_bound: int,\n        upper_bound: int,\n        include_bad_data: bool = False,\n        pivot: bool = False,\n        limit: int = None,\n        offset: int = None,\n    ) -&gt; DataFrame:\n\"\"\"\n        A function that receives a dataframe of raw tag data and computes the circular mean for samples in a range.\n\n        Args:\n            tagname_filter (list str): List of tagnames to filter on the source\n            start_date (str): Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n            end_date (str): End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n            time_interval_rate (str): The time interval rate (numeric input)\n            time_interval_unit (str): The time interval unit (second, minute, day, hour)\n            lower_bound (int): Lower boundary for the sample range\n            upper_bound (int): Upper boundary for the sample range\n            include_bad_data (optional bool): Include \"Bad\" data points with True or remove \"Bad\" data points with False\n            pivot (optional bool): Pivot the data on the timestamp column with True or do not pivot the data with False\n            limit (optional int): The number of rows to be returned\n            offset (optional int): The number of rows to skip before returning rows\n\n        Returns:\n            DataFrame: A dataframe containing the circular averages\n        \"\"\"\n        circular_average_parameters = {\n            \"source\": self.data_source,\n            \"tag_names\": tagname_filter,\n            \"start_date\": start_date,\n            \"end_date\": end_date,\n            \"include_bad_data\": include_bad_data,\n            \"time_interval_rate\": time_interval_rate,\n            \"time_interval_unit\": time_interval_unit,\n            \"lower_bound\": lower_bound,\n            \"upper_bound\": upper_bound,\n            \"pivot\": pivot,\n            \"limit\": limit,\n            \"offset\": offset,\n            \"tagname_column\": self.tagname_column,\n            \"timestamp_column\": self.timestamp_column,\n            \"status_column\": self.status_column,\n            \"value_column\": self.value_column,\n            \"supress_warning\": True,\n        }\n\n        return circular_average.get(self.connection, circular_average_parameters)\n\n    def circular_standard_deviation(\n        self,\n        tagname_filter: [str],\n        start_date: str,\n        end_date: str,\n        time_interval_rate: str,\n        time_interval_unit: str,\n        lower_bound: int,\n        upper_bound: int,\n        include_bad_data: bool = False,\n        pivot: bool = False,\n        limit: int = None,\n        offset: int = None,\n    ) -&gt; DataFrame:\n\"\"\"\n        A function that receives a dataframe of raw tag data and computes the circular standard deviation for samples assumed to be in the range.\n\n        Args:\n            tagname_filter (list str): List of tagnames to filter on the source\n            start_date (str): Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n            end_date (str): End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n            time_interval_rate (str): The time interval rate (numeric input)\n            time_interval_unit (str): The time interval unit (second, minute, day, hour)\n            lower_bound (int): Lower boundary for the sample range\n            upper_bound (int): Upper boundary for the sample range\n            include_bad_data (optional bool): Include \"Bad\" data points with True or remove \"Bad\" data points with False\n            pivot (optional bool): Pivot the data on the timestamp column with True or do not pivot the data with False\n            limit (optional int): The number of rows to be returned\n            offset (optional int): The number of rows to skip before returning rows\n\n        Returns:\n            DataFrame: A dataframe containing the circular standard deviations\n        \"\"\"\n        circular_stdev_parameters = {\n            \"source\": self.data_source,\n            \"tag_names\": tagname_filter,\n            \"start_date\": start_date,\n            \"end_date\": end_date,\n            \"include_bad_data\": include_bad_data,\n            \"time_interval_rate\": time_interval_rate,\n            \"time_interval_unit\": time_interval_unit,\n            \"lower_bound\": lower_bound,\n            \"upper_bound\": upper_bound,\n            \"pivot\": pivot,\n            \"limit\": limit,\n            \"offset\": offset,\n            \"tagname_column\": self.tagname_column,\n            \"timestamp_column\": self.timestamp_column,\n            \"status_column\": self.status_column,\n            \"value_column\": self.value_column,\n            \"supress_warning\": True,\n        }\n\n        return circular_standard_deviation.get(\n            self.connection, circular_stdev_parameters\n        )\n\n    def summary(\n        self,\n        tagname_filter: [str],\n        start_date: str,\n        end_date: str,\n        include_bad_data: bool = False,\n        limit: int = None,\n        offset: int = None,\n    ) -&gt; DataFrame:\n\"\"\"\n        A function to return back a summary of statistics.\n\n        Args:\n            tagname_filter (list str): List of tagnames to filter on the source\n            start_date (str): Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n            end_date (str): End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n            include_bad_data (optional bool): Include \"Bad\" data points with True or remove \"Bad\" data points with False\n            limit (optional int): The number of rows to be returned\n            offset (optional int): The number of rows to skip before returning rows\n\n        Returns:\n            DataFrame: A dataframe of raw timeseries data.\n        \"\"\"\n        summary_parameters = {\n            \"source\": self.data_source,\n            \"tag_names\": tagname_filter,\n            \"start_date\": start_date,\n            \"end_date\": end_date,\n            \"include_bad_data\": include_bad_data,\n            \"limit\": limit,\n            \"offset\": offset,\n            \"tagname_column\": self.tagname_column,\n            \"timestamp_column\": self.timestamp_column,\n            \"status_column\": self.status_column,\n            \"value_column\": self.value_column,\n            \"supress_warning\": True,\n        }\n        return summary.get(self.connection, summary_parameters)\n</code></pre>"},{"location":"sdk/code-reference/query/functions/time_series/time_series_query_builder/#src.sdk.python.rtdip_sdk.queries.time_series.time_series_query_builder.TimeSeriesQueryBuilder.circular_average","title":"<code>circular_average(tagname_filter, start_date, end_date, time_interval_rate, time_interval_unit, lower_bound, upper_bound, include_bad_data=False, pivot=False, limit=None, offset=None)</code>","text":"<p>A function that receives a dataframe of raw tag data and computes the circular mean for samples in a range.</p> <p>Parameters:</p> Name Type Description Default <code>tagname_filter</code> <code>list str</code> <p>List of tagnames to filter on the source</p> required <code>start_date</code> <code>str</code> <p>Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> required <code>end_date</code> <code>str</code> <p>End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> required <code>time_interval_rate</code> <code>str</code> <p>The time interval rate (numeric input)</p> required <code>time_interval_unit</code> <code>str</code> <p>The time interval unit (second, minute, day, hour)</p> required <code>lower_bound</code> <code>int</code> <p>Lower boundary for the sample range</p> required <code>upper_bound</code> <code>int</code> <p>Upper boundary for the sample range</p> required <code>include_bad_data</code> <code>optional bool</code> <p>Include \"Bad\" data points with True or remove \"Bad\" data points with False</p> <code>False</code> <code>pivot</code> <code>optional bool</code> <p>Pivot the data on the timestamp column with True or do not pivot the data with False</p> <code>False</code> <code>limit</code> <code>optional int</code> <p>The number of rows to be returned</p> <code>None</code> <code>offset</code> <code>optional int</code> <p>The number of rows to skip before returning rows</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe containing the circular averages</p> Source code in <code>src/sdk/python/rtdip_sdk/queries/time_series/time_series_query_builder.py</code> <pre><code>def circular_average(\n    self,\n    tagname_filter: [str],\n    start_date: str,\n    end_date: str,\n    time_interval_rate: str,\n    time_interval_unit: str,\n    lower_bound: int,\n    upper_bound: int,\n    include_bad_data: bool = False,\n    pivot: bool = False,\n    limit: int = None,\n    offset: int = None,\n) -&gt; DataFrame:\n\"\"\"\n    A function that receives a dataframe of raw tag data and computes the circular mean for samples in a range.\n\n    Args:\n        tagname_filter (list str): List of tagnames to filter on the source\n        start_date (str): Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        end_date (str): End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        time_interval_rate (str): The time interval rate (numeric input)\n        time_interval_unit (str): The time interval unit (second, minute, day, hour)\n        lower_bound (int): Lower boundary for the sample range\n        upper_bound (int): Upper boundary for the sample range\n        include_bad_data (optional bool): Include \"Bad\" data points with True or remove \"Bad\" data points with False\n        pivot (optional bool): Pivot the data on the timestamp column with True or do not pivot the data with False\n        limit (optional int): The number of rows to be returned\n        offset (optional int): The number of rows to skip before returning rows\n\n    Returns:\n        DataFrame: A dataframe containing the circular averages\n    \"\"\"\n    circular_average_parameters = {\n        \"source\": self.data_source,\n        \"tag_names\": tagname_filter,\n        \"start_date\": start_date,\n        \"end_date\": end_date,\n        \"include_bad_data\": include_bad_data,\n        \"time_interval_rate\": time_interval_rate,\n        \"time_interval_unit\": time_interval_unit,\n        \"lower_bound\": lower_bound,\n        \"upper_bound\": upper_bound,\n        \"pivot\": pivot,\n        \"limit\": limit,\n        \"offset\": offset,\n        \"tagname_column\": self.tagname_column,\n        \"timestamp_column\": self.timestamp_column,\n        \"status_column\": self.status_column,\n        \"value_column\": self.value_column,\n        \"supress_warning\": True,\n    }\n\n    return circular_average.get(self.connection, circular_average_parameters)\n</code></pre>"},{"location":"sdk/code-reference/query/functions/time_series/time_series_query_builder/#src.sdk.python.rtdip_sdk.queries.time_series.time_series_query_builder.TimeSeriesQueryBuilder.circular_standard_deviation","title":"<code>circular_standard_deviation(tagname_filter, start_date, end_date, time_interval_rate, time_interval_unit, lower_bound, upper_bound, include_bad_data=False, pivot=False, limit=None, offset=None)</code>","text":"<p>A function that receives a dataframe of raw tag data and computes the circular standard deviation for samples assumed to be in the range.</p> <p>Parameters:</p> Name Type Description Default <code>tagname_filter</code> <code>list str</code> <p>List of tagnames to filter on the source</p> required <code>start_date</code> <code>str</code> <p>Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> required <code>end_date</code> <code>str</code> <p>End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> required <code>time_interval_rate</code> <code>str</code> <p>The time interval rate (numeric input)</p> required <code>time_interval_unit</code> <code>str</code> <p>The time interval unit (second, minute, day, hour)</p> required <code>lower_bound</code> <code>int</code> <p>Lower boundary for the sample range</p> required <code>upper_bound</code> <code>int</code> <p>Upper boundary for the sample range</p> required <code>include_bad_data</code> <code>optional bool</code> <p>Include \"Bad\" data points with True or remove \"Bad\" data points with False</p> <code>False</code> <code>pivot</code> <code>optional bool</code> <p>Pivot the data on the timestamp column with True or do not pivot the data with False</p> <code>False</code> <code>limit</code> <code>optional int</code> <p>The number of rows to be returned</p> <code>None</code> <code>offset</code> <code>optional int</code> <p>The number of rows to skip before returning rows</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe containing the circular standard deviations</p> Source code in <code>src/sdk/python/rtdip_sdk/queries/time_series/time_series_query_builder.py</code> <pre><code>def circular_standard_deviation(\n    self,\n    tagname_filter: [str],\n    start_date: str,\n    end_date: str,\n    time_interval_rate: str,\n    time_interval_unit: str,\n    lower_bound: int,\n    upper_bound: int,\n    include_bad_data: bool = False,\n    pivot: bool = False,\n    limit: int = None,\n    offset: int = None,\n) -&gt; DataFrame:\n\"\"\"\n    A function that receives a dataframe of raw tag data and computes the circular standard deviation for samples assumed to be in the range.\n\n    Args:\n        tagname_filter (list str): List of tagnames to filter on the source\n        start_date (str): Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        end_date (str): End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        time_interval_rate (str): The time interval rate (numeric input)\n        time_interval_unit (str): The time interval unit (second, minute, day, hour)\n        lower_bound (int): Lower boundary for the sample range\n        upper_bound (int): Upper boundary for the sample range\n        include_bad_data (optional bool): Include \"Bad\" data points with True or remove \"Bad\" data points with False\n        pivot (optional bool): Pivot the data on the timestamp column with True or do not pivot the data with False\n        limit (optional int): The number of rows to be returned\n        offset (optional int): The number of rows to skip before returning rows\n\n    Returns:\n        DataFrame: A dataframe containing the circular standard deviations\n    \"\"\"\n    circular_stdev_parameters = {\n        \"source\": self.data_source,\n        \"tag_names\": tagname_filter,\n        \"start_date\": start_date,\n        \"end_date\": end_date,\n        \"include_bad_data\": include_bad_data,\n        \"time_interval_rate\": time_interval_rate,\n        \"time_interval_unit\": time_interval_unit,\n        \"lower_bound\": lower_bound,\n        \"upper_bound\": upper_bound,\n        \"pivot\": pivot,\n        \"limit\": limit,\n        \"offset\": offset,\n        \"tagname_column\": self.tagname_column,\n        \"timestamp_column\": self.timestamp_column,\n        \"status_column\": self.status_column,\n        \"value_column\": self.value_column,\n        \"supress_warning\": True,\n    }\n\n    return circular_standard_deviation.get(\n        self.connection, circular_stdev_parameters\n    )\n</code></pre>"},{"location":"sdk/code-reference/query/functions/time_series/time_series_query_builder/#src.sdk.python.rtdip_sdk.queries.time_series.time_series_query_builder.TimeSeriesQueryBuilder.connect","title":"<code>connect(connection)</code>","text":"<p>Specifies the connection to be used for the query.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>ConnectionInterface</code> <p>Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)</p> required Source code in <code>src/sdk/python/rtdip_sdk/queries/time_series/time_series_query_builder.py</code> <pre><code>def connect(self, connection: ConnectionInterface):\n\"\"\"\n    Specifies the connection to be used for the query.\n\n    Args:\n        connection: Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)\n    \"\"\"\n    self.connection = connection\n    return self\n</code></pre>"},{"location":"sdk/code-reference/query/functions/time_series/time_series_query_builder/#src.sdk.python.rtdip_sdk.queries.time_series.time_series_query_builder.TimeSeriesQueryBuilder.interpolate","title":"<code>interpolate(tagname_filter, start_date, end_date, time_interval_rate, time_interval_unit, agg_method, interpolation_method, include_bad_data=False, pivot=False, limit=None, offset=None)</code>","text":"<p>The Interpolate function will forward fill, backward fill or linearly interpolate the resampled data depending on the parameters specified.</p> <p>Parameters:</p> Name Type Description Default <code>tagname_filter</code> <code>list str</code> <p>List of tagnames to filter on the source</p> required <code>start_date</code> <code>str</code> <p>Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> required <code>end_date</code> <code>str</code> <p>End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> required <code>time_interval_rate</code> <code>str</code> <p>The time interval rate (numeric input)</p> required <code>time_interval_unit</code> <code>str</code> <p>The time interval unit (second, minute, day, hour)</p> required <code>agg_method</code> <code>str</code> <p>Aggregation Method (first, last, avg, min, max)</p> required <code>interpolation_method</code> <code>str</code> <p>Interpolation method (forward_fill, backward_fill, linear)</p> required <code>include_bad_data</code> <code>optional bool</code> <p>Include \"Bad\" data points with True or remove \"Bad\" data points with False</p> <code>False</code> <code>pivot</code> <code>optional bool</code> <p>Pivot the data on the timestamp column with True or do not pivot the data with False</p> <code>False</code> <code>limit</code> <code>optional int</code> <p>The number of rows to be returned</p> <code>None</code> <code>offset</code> <code>optional int</code> <p>The number of rows to skip before returning rows</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe of interpolated timeseries data.</p> Source code in <code>src/sdk/python/rtdip_sdk/queries/time_series/time_series_query_builder.py</code> <pre><code>def interpolate(\n    self,\n    tagname_filter: [str],\n    start_date: str,\n    end_date: str,\n    time_interval_rate: str,\n    time_interval_unit: str,\n    agg_method: str,\n    interpolation_method: str,\n    include_bad_data: bool = False,\n    pivot: bool = False,\n    limit: int = None,\n    offset: int = None,\n) -&gt; DataFrame:\n\"\"\"\n    The Interpolate function will forward fill, backward fill or linearly interpolate the resampled data depending on the parameters specified.\n\n    Args:\n        tagname_filter (list str): List of tagnames to filter on the source\n        start_date (str): Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        end_date (str): End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        time_interval_rate (str): The time interval rate (numeric input)\n        time_interval_unit (str): The time interval unit (second, minute, day, hour)\n        agg_method (str): Aggregation Method (first, last, avg, min, max)\n        interpolation_method (str): Interpolation method (forward_fill, backward_fill, linear)\n        include_bad_data (optional bool): Include \"Bad\" data points with True or remove \"Bad\" data points with False\n        pivot (optional bool): Pivot the data on the timestamp column with True or do not pivot the data with False\n        limit (optional int): The number of rows to be returned\n        offset (optional int): The number of rows to skip before returning rows\n\n    Returns:\n        DataFrame: A dataframe of interpolated timeseries data.\n    \"\"\"\n    interpolation_parameters = {\n        \"source\": self.data_source,\n        \"tag_names\": tagname_filter,\n        \"start_date\": start_date,\n        \"end_date\": end_date,\n        \"include_bad_data\": include_bad_data,\n        \"time_interval_rate\": time_interval_rate,\n        \"time_interval_unit\": time_interval_unit,\n        \"agg_method\": agg_method,\n        \"interpolation_method\": interpolation_method,\n        \"pivot\": pivot,\n        \"limit\": limit,\n        \"offset\": offset,\n        \"tagname_column\": self.tagname_column,\n        \"timestamp_column\": self.timestamp_column,\n        \"status_column\": self.status_column,\n        \"value_column\": self.value_column,\n        \"supress_warning\": True,\n    }\n\n    return interpolate.get(self.connection, interpolation_parameters)\n</code></pre>"},{"location":"sdk/code-reference/query/functions/time_series/time_series_query_builder/#src.sdk.python.rtdip_sdk.queries.time_series.time_series_query_builder.TimeSeriesQueryBuilder.interpolation_at_time","title":"<code>interpolation_at_time(tagname_filter, timestamp_filter, include_bad_data=False, window_length=1, pivot=False, limit=None, offset=None)</code>","text":"<p>A interpolation at time function which works out the linear interpolation at a specific time based on the points before and after.</p> <p>Parameters:</p> Name Type Description Default <code>tagname_filter</code> <code>list str</code> <p>List of tagnames to filter on the source</p> required <code>timestamp_filter</code> <code>list</code> <p>List of timestamp or timestamps in the format YYY-MM-DDTHH:MM:SS or YYY-MM-DDTHH:MM:SS+zz:zz where %z is the timezone. (Example +00:00 is the UTC timezone)</p> required <code>include_bad_data</code> <code>optional bool</code> <p>Include \"Bad\" data points with True or remove \"Bad\" data points with False</p> <code>False</code> <code>window_length</code> <code>optional int</code> <p>Add longer window time in days for the start or end of specified date to cater for edge cases</p> <code>1</code> <code>pivot</code> <code>optional bool</code> <p>Pivot the data on the timestamp column with True or do not pivot the data with False</p> <code>False</code> <code>limit</code> <code>optional int</code> <p>The number of rows to be returned</p> <code>None</code> <code>offset</code> <code>optional int</code> <p>The number of rows to skip before returning rows</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe of interpolation at time timeseries data</p> Source code in <code>src/sdk/python/rtdip_sdk/queries/time_series/time_series_query_builder.py</code> <pre><code>def interpolation_at_time(\n    self,\n    tagname_filter: [str],\n    timestamp_filter: [str],\n    include_bad_data: bool = False,\n    window_length: int = 1,\n    pivot: bool = False,\n    limit: int = None,\n    offset: int = None,\n) -&gt; DataFrame:\n\"\"\"\n    A interpolation at time function which works out the linear interpolation at a specific time based on the points before and after.\n\n    Args:\n        tagname_filter (list str): List of tagnames to filter on the source\n        timestamp_filter (list): List of timestamp or timestamps in the format YYY-MM-DDTHH:MM:SS or YYY-MM-DDTHH:MM:SS+zz:zz where %z is the timezone. (Example +00:00 is the UTC timezone)\n        include_bad_data (optional bool): Include \"Bad\" data points with True or remove \"Bad\" data points with False\n        window_length (optional int): Add longer window time in days for the start or end of specified date to cater for edge cases\n        pivot (optional bool): Pivot the data on the timestamp column with True or do not pivot the data with False\n        limit (optional int): The number of rows to be returned\n        offset (optional int): The number of rows to skip before returning rows\n\n    Returns:\n        DataFrame: A dataframe of interpolation at time timeseries data\n    \"\"\"\n    interpolation_at_time_parameters = {\n        \"source\": self.data_source,\n        \"tag_names\": tagname_filter,\n        \"timestamps\": timestamp_filter,\n        \"include_bad_data\": include_bad_data,\n        \"window_length\": window_length,\n        \"pivot\": pivot,\n        \"limit\": limit,\n        \"offset\": offset,\n        \"tagname_column\": self.tagname_column,\n        \"timestamp_column\": self.timestamp_column,\n        \"status_column\": self.status_column,\n        \"value_column\": self.value_column,\n        \"supress_warning\": True,\n    }\n\n    return interpolation_at_time.get(\n        self.connection, interpolation_at_time_parameters\n    )\n</code></pre>"},{"location":"sdk/code-reference/query/functions/time_series/time_series_query_builder/#src.sdk.python.rtdip_sdk.queries.time_series.time_series_query_builder.TimeSeriesQueryBuilder.latest","title":"<code>latest(tagname_filter=None, limit=None, offset=None)</code>","text":"<p>A query to retrieve latest event_values.</p> <p>Parameters:</p> Name Type Description Default <code>tagname_filter</code> <code>list str</code> <p>List of tagnames to filter on the source</p> <code>None</code> <code>limit</code> <code>optional int</code> <p>The number of rows to be returned</p> <code>None</code> <code>offset</code> <code>optional int</code> <p>The number of rows to skip before returning rows</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe of events latest_values</p> Source code in <code>src/sdk/python/rtdip_sdk/queries/time_series/time_series_query_builder.py</code> <pre><code>def latest(\n    self,\n    tagname_filter: [str] = None,\n    limit: int = None,\n    offset: int = None,\n) -&gt; DataFrame:\n\"\"\"\n    A query to retrieve latest event_values.\n\n    Args:\n        tagname_filter (list str): List of tagnames to filter on the source\n        limit (optional int): The number of rows to be returned\n        offset (optional int): The number of rows to skip before returning rows\n\n    Returns:\n        DataFrame: A dataframe of events latest_values\n    \"\"\"\n    latest_parameters = {\n        \"source\": self.data_source,\n        \"tag_names\": [] if tagname_filter is None else tagname_filter,\n        \"tagname_column\": self.tagname_column,\n        \"limit\": limit,\n        \"offset\": offset,\n        \"supress_warning\": True,\n    }\n\n    return latest.get(self.connection, latest_parameters)\n</code></pre>"},{"location":"sdk/code-reference/query/functions/time_series/time_series_query_builder/#src.sdk.python.rtdip_sdk.queries.time_series.time_series_query_builder.TimeSeriesQueryBuilder.metadata","title":"<code>metadata(tagname_filter=None, limit=None, offset=None)</code>","text":"<p>A query to retrieve metadata.</p> <p>Parameters:</p> Name Type Description Default <code>tagname_filter</code> <code>list str</code> <p>List of tagnames to filter on the source</p> <code>None</code> <code>limit</code> <code>optional int</code> <p>The number of rows to be returned</p> <code>None</code> <code>offset</code> <code>optional int</code> <p>The number of rows to skip before returning rows</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe of metadata</p> Source code in <code>src/sdk/python/rtdip_sdk/queries/time_series/time_series_query_builder.py</code> <pre><code>def metadata(\n    self,\n    tagname_filter: [str] = None,\n    limit: int = None,\n    offset: int = None,\n) -&gt; DataFrame:\n\"\"\"\n    A query to retrieve metadata.\n\n    Args:\n        tagname_filter (list str): List of tagnames to filter on the source\n        limit (optional int): The number of rows to be returned\n        offset (optional int): The number of rows to skip before returning rows\n\n    Returns:\n        DataFrame: A dataframe of metadata\n    \"\"\"\n    metadata_parameters = {\n        \"source\": self.data_source,\n        \"tag_names\": [] if tagname_filter is None else tagname_filter,\n        \"tagname_column\": self.tagname_column,\n        \"limit\": limit,\n        \"offset\": offset,\n        \"supress_warning\": True,\n    }\n\n    return metadata.get(self.connection, metadata_parameters)\n</code></pre>"},{"location":"sdk/code-reference/query/functions/time_series/time_series_query_builder/#src.sdk.python.rtdip_sdk.queries.time_series.time_series_query_builder.TimeSeriesQueryBuilder.raw","title":"<code>raw(tagname_filter, start_date, end_date, include_bad_data=False, limit=None, offset=None)</code>","text":"<p>A function to return back raw data.</p> <p>Parameters:</p> Name Type Description Default <code>tagname_filter</code> <code>list str</code> <p>List of tagnames to filter on the source</p> required <code>start_date</code> <code>str</code> <p>Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> required <code>end_date</code> <code>str</code> <p>End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> required <code>include_bad_data</code> <code>optional bool</code> <p>Include \"Bad\" data points with True or remove \"Bad\" data points with False</p> <code>False</code> <code>limit</code> <code>optional int</code> <p>The number of rows to be returned</p> <code>None</code> <code>offset</code> <code>optional int</code> <p>The number of rows to skip before returning rows</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe of raw timeseries data.</p> Source code in <code>src/sdk/python/rtdip_sdk/queries/time_series/time_series_query_builder.py</code> <pre><code>def raw(\n    self,\n    tagname_filter: [str],\n    start_date: str,\n    end_date: str,\n    include_bad_data: bool = False,\n    limit: int = None,\n    offset: int = None,\n) -&gt; DataFrame:\n\"\"\"\n    A function to return back raw data.\n\n    Args:\n        tagname_filter (list str): List of tagnames to filter on the source\n        start_date (str): Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        end_date (str): End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        include_bad_data (optional bool): Include \"Bad\" data points with True or remove \"Bad\" data points with False\n        limit (optional int): The number of rows to be returned\n        offset (optional int): The number of rows to skip before returning rows\n\n    Returns:\n        DataFrame: A dataframe of raw timeseries data.\n    \"\"\"\n    raw_parameters = {\n        \"source\": self.data_source,\n        \"tag_names\": tagname_filter,\n        \"start_date\": start_date,\n        \"end_date\": end_date,\n        \"include_bad_data\": include_bad_data,\n        \"limit\": limit,\n        \"offset\": offset,\n        \"tagname_column\": self.tagname_column,\n        \"timestamp_column\": self.timestamp_column,\n        \"status_column\": self.status_column,\n        \"value_column\": self.value_column,\n        \"supress_warning\": True,\n    }\n    return raw.get(self.connection, raw_parameters)\n</code></pre>"},{"location":"sdk/code-reference/query/functions/time_series/time_series_query_builder/#src.sdk.python.rtdip_sdk.queries.time_series.time_series_query_builder.TimeSeriesQueryBuilder.resample","title":"<code>resample(tagname_filter, start_date, end_date, time_interval_rate, time_interval_unit, agg_method, include_bad_data=False, pivot=False, limit=None, offset=None)</code>","text":"<p>A query to resample the source data.</p> <p>Parameters:</p> Name Type Description Default <code>tagname_filter</code> <code>list str</code> <p>List of tagnames to filter on the source</p> required <code>start_date</code> <code>str</code> <p>Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> required <code>end_date</code> <code>str</code> <p>End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> required <code>time_interval_rate</code> <code>str</code> <p>The time interval rate (numeric input)</p> required <code>time_interval_unit</code> <code>str</code> <p>The time interval unit (second, minute, day, hour)</p> required <code>agg_method</code> <code>str</code> <p>Aggregation Method (first, last, avg, min, max)</p> required <code>include_bad_data</code> <code>optional bool</code> <p>Include \"Bad\" data points with True or remove \"Bad\" data points with False</p> <code>False</code> <code>pivot</code> <code>optional bool</code> <p>Pivot the data on the timestamp column with True or do not pivot the data with False</p> <code>False</code> <code>limit</code> <code>optional int</code> <p>The number of rows to be returned</p> <code>None</code> <code>offset</code> <code>optional int</code> <p>The number of rows to skip before returning rows</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe of resampled timeseries data.</p> Source code in <code>src/sdk/python/rtdip_sdk/queries/time_series/time_series_query_builder.py</code> <pre><code>def resample(\n    self,\n    tagname_filter: [str],\n    start_date: str,\n    end_date: str,\n    time_interval_rate: str,\n    time_interval_unit: str,\n    agg_method: str,\n    include_bad_data: bool = False,\n    pivot: bool = False,\n    limit: int = None,\n    offset: int = None,\n) -&gt; DataFrame:\n\"\"\"\n    A query to resample the source data.\n\n    Args:\n        tagname_filter (list str): List of tagnames to filter on the source\n        start_date (str): Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        end_date (str): End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        time_interval_rate (str): The time interval rate (numeric input)\n        time_interval_unit (str): The time interval unit (second, minute, day, hour)\n        agg_method (str): Aggregation Method (first, last, avg, min, max)\n        include_bad_data (optional bool): Include \"Bad\" data points with True or remove \"Bad\" data points with False\n        pivot (optional bool): Pivot the data on the timestamp column with True or do not pivot the data with False\n        limit (optional int): The number of rows to be returned\n        offset (optional int): The number of rows to skip before returning rows\n\n    Returns:\n        DataFrame: A dataframe of resampled timeseries data.\n    \"\"\"\n\n    resample_parameters = {\n        \"source\": self.data_source,\n        \"tag_names\": tagname_filter,\n        \"start_date\": start_date,\n        \"end_date\": end_date,\n        \"include_bad_data\": include_bad_data,\n        \"time_interval_rate\": time_interval_rate,\n        \"time_interval_unit\": time_interval_unit,\n        \"agg_method\": agg_method,\n        \"pivot\": pivot,\n        \"limit\": limit,\n        \"offset\": offset,\n        \"tagname_column\": self.tagname_column,\n        \"timestamp_column\": self.timestamp_column,\n        \"status_column\": self.status_column,\n        \"value_column\": self.value_column,\n        \"supress_warning\": True,\n    }\n\n    return resample.get(self.connection, resample_parameters)\n</code></pre>"},{"location":"sdk/code-reference/query/functions/time_series/time_series_query_builder/#src.sdk.python.rtdip_sdk.queries.time_series.time_series_query_builder.TimeSeriesQueryBuilder.source","title":"<code>source(source, tagname_column='TagName', timestamp_column='EventTime', status_column='Status', value_column='Value')</code>","text":"<p>Specifies the source of the query.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>Source of the query can be a Unity Catalog table, Hive metastore table or path</p> required <code>tagname_column</code> <code>optional str</code> <p>The column name in the source that contains the tagnames or series</p> <code>'TagName'</code> <code>timestamp_column</code> <code>optional str</code> <p>The timestamp column name in the source</p> <code>'EventTime'</code> <code>status_column</code> <code>optional str</code> <p>The status column name in the source indicating <code>Good</code> or <code>Bad</code>. If this is not available, specify <code>None</code></p> <code>'Status'</code> <code>value_column</code> <code>optional str</code> <p>The value column name in the source which is normally a float or string value for the time series event</p> <code>'Value'</code> Source code in <code>src/sdk/python/rtdip_sdk/queries/time_series/time_series_query_builder.py</code> <pre><code>def source(\n    self,\n    source: str,\n    tagname_column: str = \"TagName\",\n    timestamp_column: str = \"EventTime\",\n    status_column: Union[str, None] = \"Status\",\n    value_column: str = \"Value\",\n):\n\"\"\"\n    Specifies the source of the query.\n\n    Args:\n        source (str): Source of the query can be a Unity Catalog table, Hive metastore table or path\n        tagname_column (optional str): The column name in the source that contains the tagnames or series\n        timestamp_column (optional str): The timestamp column name in the source\n        status_column (optional str): The status column name in the source indicating `Good` or `Bad`. If this is not available, specify `None`\n        value_column (optional str): The value column name in the source which is normally a float or string value for the time series event\n    \"\"\"\n    self.data_source = \"`.`\".join(source.split(\".\"))\n    self.tagname_column = tagname_column\n    self.timestamp_column = timestamp_column\n    self.status_column = status_column\n    self.value_column = value_column\n    return self\n</code></pre>"},{"location":"sdk/code-reference/query/functions/time_series/time_series_query_builder/#src.sdk.python.rtdip_sdk.queries.time_series.time_series_query_builder.TimeSeriesQueryBuilder.summary","title":"<code>summary(tagname_filter, start_date, end_date, include_bad_data=False, limit=None, offset=None)</code>","text":"<p>A function to return back a summary of statistics.</p> <p>Parameters:</p> Name Type Description Default <code>tagname_filter</code> <code>list str</code> <p>List of tagnames to filter on the source</p> required <code>start_date</code> <code>str</code> <p>Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> required <code>end_date</code> <code>str</code> <p>End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> required <code>include_bad_data</code> <code>optional bool</code> <p>Include \"Bad\" data points with True or remove \"Bad\" data points with False</p> <code>False</code> <code>limit</code> <code>optional int</code> <p>The number of rows to be returned</p> <code>None</code> <code>offset</code> <code>optional int</code> <p>The number of rows to skip before returning rows</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe of raw timeseries data.</p> Source code in <code>src/sdk/python/rtdip_sdk/queries/time_series/time_series_query_builder.py</code> <pre><code>def summary(\n    self,\n    tagname_filter: [str],\n    start_date: str,\n    end_date: str,\n    include_bad_data: bool = False,\n    limit: int = None,\n    offset: int = None,\n) -&gt; DataFrame:\n\"\"\"\n    A function to return back a summary of statistics.\n\n    Args:\n        tagname_filter (list str): List of tagnames to filter on the source\n        start_date (str): Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        end_date (str): End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        include_bad_data (optional bool): Include \"Bad\" data points with True or remove \"Bad\" data points with False\n        limit (optional int): The number of rows to be returned\n        offset (optional int): The number of rows to skip before returning rows\n\n    Returns:\n        DataFrame: A dataframe of raw timeseries data.\n    \"\"\"\n    summary_parameters = {\n        \"source\": self.data_source,\n        \"tag_names\": tagname_filter,\n        \"start_date\": start_date,\n        \"end_date\": end_date,\n        \"include_bad_data\": include_bad_data,\n        \"limit\": limit,\n        \"offset\": offset,\n        \"tagname_column\": self.tagname_column,\n        \"timestamp_column\": self.timestamp_column,\n        \"status_column\": self.status_column,\n        \"value_column\": self.value_column,\n        \"supress_warning\": True,\n    }\n    return summary.get(self.connection, summary_parameters)\n</code></pre>"},{"location":"sdk/code-reference/query/functions/time_series/time_series_query_builder/#src.sdk.python.rtdip_sdk.queries.time_series.time_series_query_builder.TimeSeriesQueryBuilder.time_weighted_average","title":"<code>time_weighted_average(tagname_filter, start_date, end_date, time_interval_rate, time_interval_unit, step, source_metadata=None, include_bad_data=False, window_length=1, pivot=False, limit=None, offset=None)</code>","text":"<p>A function that receives a dataframe of raw tag data and performs a time weighted averages.</p> <p>Parameters:</p> Name Type Description Default <code>tagname_filter</code> <code>list str</code> <p>List of tagnames to filter on the source</p> required <code>start_date</code> <code>str</code> <p>Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> required <code>end_date</code> <code>str</code> <p>End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> required <code>time_interval_rate</code> <code>str</code> <p>The time interval rate (numeric input)</p> required <code>time_interval_unit</code> <code>str</code> <p>The time interval unit (second, minute, day, hour)</p> required <code>step</code> <code>str</code> <p>data points with step \"enabled\" or \"disabled\". The options for step are \"true\", \"false\" or \"metadata\". \"metadata\" will retrieve the step value from the metadata table</p> required <code>source_metadata</code> <code>optional str</code> <p>if step is set to \"metadata\", then this parameter must be populated with the source containing the tagname metadata with a column called \"Step\"</p> <code>None</code> <code>include_bad_data</code> <code>optional bool</code> <p>Include \"Bad\" data points with True or remove \"Bad\" data points with False</p> <code>False</code> <code>window_length</code> <code>optional int</code> <p>Add longer window time in days for the start or end of specified date to cater for edge cases</p> <code>1</code> <code>pivot</code> <code>optional bool</code> <p>Pivot the data on the timestamp column with True or do not pivot the data with False</p> <code>False</code> <code>limit</code> <code>optional int</code> <p>The number of rows to be returned</p> <code>None</code> <code>offset</code> <code>optional int</code> <p>The number of rows to skip before returning rows</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe of time weighted averages timeseries data</p> Source code in <code>src/sdk/python/rtdip_sdk/queries/time_series/time_series_query_builder.py</code> <pre><code>def time_weighted_average(\n    self,\n    tagname_filter: [str],\n    start_date: str,\n    end_date: str,\n    time_interval_rate: str,\n    time_interval_unit: str,\n    step: str,\n    source_metadata: str = None,\n    include_bad_data: bool = False,\n    window_length: int = 1,\n    pivot: bool = False,\n    limit: int = None,\n    offset: int = None,\n) -&gt; DataFrame:\n\"\"\"\n    A function that receives a dataframe of raw tag data and performs a time weighted averages.\n\n    Args:\n        tagname_filter (list str): List of tagnames to filter on the source\n        start_date (str): Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        end_date (str): End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        time_interval_rate (str): The time interval rate (numeric input)\n        time_interval_unit (str): The time interval unit (second, minute, day, hour)\n        step (str): data points with step \"enabled\" or \"disabled\". The options for step are \"true\", \"false\" or \"metadata\". \"metadata\" will retrieve the step value from the metadata table\n        source_metadata (optional str): if step is set to \"metadata\", then this parameter must be populated with the source containing the tagname metadata with a column called \"Step\"\n        include_bad_data (optional bool): Include \"Bad\" data points with True or remove \"Bad\" data points with False\n        window_length (optional int): Add longer window time in days for the start or end of specified date to cater for edge cases\n        pivot (optional bool): Pivot the data on the timestamp column with True or do not pivot the data with False\n        limit (optional int): The number of rows to be returned\n        offset (optional int): The number of rows to skip before returning rows\n\n    Returns:\n        DataFrame: A dataframe of time weighted averages timeseries data\n    \"\"\"\n    time_weighted_average_parameters = {\n        \"source\": self.data_source,\n        \"tag_names\": tagname_filter,\n        \"start_date\": start_date,\n        \"end_date\": end_date,\n        \"include_bad_data\": include_bad_data,\n        \"time_interval_rate\": time_interval_rate,\n        \"time_interval_unit\": time_interval_unit,\n        \"step\": step,\n        \"source_metadata\": None\n        if source_metadata is None\n        else \"`.`\".join(source_metadata.split(\".\")),\n        \"window_length\": window_length,\n        \"pivot\": pivot,\n        \"limit\": limit,\n        \"offset\": offset,\n        \"tagname_column\": self.tagname_column,\n        \"timestamp_column\": self.timestamp_column,\n        \"status_column\": self.status_column,\n        \"value_column\": self.value_column,\n        \"supress_warning\": True,\n    }\n\n    return time_weighted_average.get(\n        self.connection, time_weighted_average_parameters\n    )\n</code></pre>"},{"location":"sdk/code-reference/query/functions/weather/latest/","title":"Weather Latest Function","text":""},{"location":"sdk/code-reference/query/functions/weather/latest/#src.sdk.python.rtdip_sdk.queries.weather.latest.get_grid","title":"<code>get_grid(connection, parameters_dict)</code>","text":"<p>A function to return the latest event values by querying databricks SQL Warehouse using a connection specified by the user.</p> <p>This will return the raw weather forecast data for a grid.</p> <p>The available connectors by RTDIP are Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect.</p> <p>The available authentcation methods are Certificate Authentication, Client Secret Authentication or Default Authentication. See documentation.</p> <p>This function requires the user to input a dictionary of parameters. (See Attributes table below)</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>object</code> <p>Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)</p> required <code>parameters_dict</code> <code>dict</code> <p>A dictionary of parameters (see Attributes table below)</p> required <p>Attributes:</p> Name Type Description <code>forecast</code> <code>str</code> <p>Business unit</p> <code>region</code> <code>str</code> <p>Region</p> <code>data_security_level</code> <code>str</code> <p>Level of data security</p> <code>data_type</code> <code>str</code> <p>Type of the data (float, integer, double, string)</p> <code>max_lat</code> <code>float</code> <p>Maximum latitude</p> <code>max_lon</code> <code>float</code> <p>Maximum longitude</p> <code>min_lat</code> <code>float</code> <p>Minimum latitude</p> <code>min_lon</code> <code>float</code> <p>Minimum longitude</p> <code>source</code> <code>optional str</code> <p>Source of the data ie ECMWF</p> <code>limit</code> <code>optional int</code> <p>The number of rows to be returned</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe of event latest values.</p> Source code in <code>src/sdk/python/rtdip_sdk/queries/weather/latest.py</code> <pre><code>def get_grid(connection: object, parameters_dict: dict) -&gt; pd.DataFrame:\n\"\"\"\n    A function to return the latest event values by querying databricks SQL Warehouse using a connection specified by the user.\n\n    This will return the raw weather forecast data for a grid.\n\n    The available connectors by RTDIP are Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect.\n\n    The available authentcation methods are Certificate Authentication, Client Secret Authentication or Default Authentication. See documentation.\n\n    This function requires the user to input a dictionary of parameters. (See Attributes table below)\n\n    Args:\n        connection: Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)\n        parameters_dict: A dictionary of parameters (see Attributes table below)\n\n    Attributes:\n        forecast (str): Business unit\n        region (str): Region\n        data_security_level (str): Level of data security\n        data_type (str): Type of the data (float, integer, double, string)\n        max_lat (float): Maximum latitude\n        max_lon (float): Maximum longitude\n        min_lat (float): Minimum latitude\n        min_lon (float): Minimum longitude\n        source (optional str): Source of the data ie ECMWF\n        limit (optional int): The number of rows to be returned\n\n    Returns:\n        DataFrame: A dataframe of event latest values.\n    \"\"\"\n    try:\n        query = _query_builder(parameters_dict, \"latest_grid\")\n\n        try:\n            cursor = connection.cursor()\n            cursor.execute(query)\n            df = cursor.fetch_all()\n            cursor.close()\n            connection.close()\n            return df\n        except Exception as e:\n            logging.exception(\"error returning dataframe\")\n            raise e\n\n    except Exception as e:\n        logging.exception(\"error returning latest function\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/functions/weather/latest/#src.sdk.python.rtdip_sdk.queries.weather.latest.get_point","title":"<code>get_point(connection, parameters_dict)</code>","text":"<p>A function to return the latest event values by querying databricks SQL Warehouse using a connection specified by the user.</p> <p>This will return the raw weather forecast data for a single point.</p> <p>The available connectors by RTDIP are Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect.</p> <p>The available authentcation methods are Certificate Authentication, Client Secret Authentication or Default Authentication. See documentation.</p> <p>This function requires the user to input a dictionary of parameters. (See Attributes table below)</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>object</code> <p>Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)</p> required <code>parameters_dict</code> <code>dict</code> <p>A dictionary of parameters (see Attributes table below)</p> required <p>Attributes:</p> Name Type Description <code>forecast</code> <code>str</code> <p>Business unit</p> <code>region</code> <code>str</code> <p>Region</p> <code>data_security_level</code> <code>str</code> <p>Level of data security</p> <code>data_type</code> <code>str</code> <p>Type of the data (float, integer, double, string)</p> <code>lat</code> <code>float</code> <p>latitude</p> <code>lon</code> <code>float</code> <p>longitude</p> <code>source</code> <code>optional str</code> <p>Source of the data ie ECMWF</p> <code>limit</code> <code>optional int</code> <p>The number of rows to be returned</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe of event latest values.</p> Source code in <code>src/sdk/python/rtdip_sdk/queries/weather/latest.py</code> <pre><code>def get_point(connection: object, parameters_dict: dict) -&gt; pd.DataFrame:\n\"\"\"\n    A function to return the latest event values by querying databricks SQL Warehouse using a connection specified by the user.\n\n    This will return the raw weather forecast data for a single point.\n\n    The available connectors by RTDIP are Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect.\n\n    The available authentcation methods are Certificate Authentication, Client Secret Authentication or Default Authentication. See documentation.\n\n    This function requires the user to input a dictionary of parameters. (See Attributes table below)\n\n    Args:\n        connection: Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)\n        parameters_dict: A dictionary of parameters (see Attributes table below)\n\n    Attributes:\n        forecast (str): Business unit\n        region (str): Region\n        data_security_level (str): Level of data security\n        data_type (str): Type of the data (float, integer, double, string)\n        lat (float): latitude\n        lon (float): longitude\n        source (optional str): Source of the data ie ECMWF\n        limit (optional int): The number of rows to be returned\n\n    Returns:\n        DataFrame: A dataframe of event latest values.\n    \"\"\"\n    try:\n        query = _query_builder(parameters_dict, \"latest_point\")\n\n        try:\n            cursor = connection.cursor()\n            cursor.execute(query)\n            df = cursor.fetch_all()\n            cursor.close()\n            connection.close()\n            return df\n        except Exception as e:\n            logging.exception(\"error returning dataframe\")\n            raise e\n\n    except Exception as e:\n        logging.exception(\"error returning latest function\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/functions/weather/raw/","title":"Weather Raw Function","text":""},{"location":"sdk/code-reference/query/functions/weather/raw/#src.sdk.python.rtdip_sdk.queries.weather.raw.get_grid","title":"<code>get_grid(connection, parameters_dict)</code>","text":"<p>A function to return back raw data by querying databricks SQL Warehouse using a connection specified by the user.</p> <p>This will return the raw weather forecast data for a grid.</p> <p>The available connectors by RTDIP are Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect.</p> <p>The available authentication methods are Certificate Authentication, Client Secret Authentication or Default Authentication. See documentation.</p> <p>This function requires the user to input a dictionary of parameters which are focused on Weather Data. (See Attributes table below)</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>object</code> <p>Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)</p> required <code>parameters_dict</code> <code>dict</code> <p>A dictionary of parameters (see Attributes table below)</p> required <p>Attributes:</p> Name Type Description <code>forecast</code> <code>str</code> <p>Business unit</p> <code>region</code> <code>str</code> <p>Region</p> <code>data_security_level</code> <code>str</code> <p>Level of data security</p> <code>data_type</code> <code>str</code> <p>Type of the data (float, integer, double, string)</p> <code>start_date</code> <code>str</code> <p>Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> <code>end_date</code> <code>str</code> <p>End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> <code>max_lat</code> <code>float</code> <p>Maximum latitude</p> <code>max_lon</code> <code>float</code> <p>Maximum longitude</p> <code>min_lat</code> <code>float</code> <p>Minimum latitude</p> <code>min_lon</code> <code>float</code> <p>Minimum longitude</p> <code>source</code> <code>optional str</code> <p>Source of the data ie ECMWF</p> <code>limit</code> <code>optional int</code> <p>The number of rows to be returned</p> <p>}</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe of raw weather forecast data.</p> Source code in <code>src/sdk/python/rtdip_sdk/queries/weather/raw.py</code> <pre><code>def get_grid(connection: object, parameters_dict: dict) -&gt; pd.DataFrame:\n\"\"\"\n    A function to return back raw data by querying databricks SQL Warehouse using a connection specified by the user.\n\n    This will return the raw weather forecast data for a grid.\n\n    The available connectors by RTDIP are Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect.\n\n    The available authentication methods are Certificate Authentication, Client Secret Authentication or Default Authentication. See documentation.\n\n    This function requires the user to input a dictionary of parameters which are focused on Weather Data. (See Attributes table below)\n\n    Args:\n        connection: Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)\n        parameters_dict: A dictionary of parameters (see Attributes table below)\n\n    Attributes:\n        forecast (str): Business unit\n        region (str): Region\n        data_security_level (str): Level of data security\n        data_type (str): Type of the data (float, integer, double, string)\n        start_date (str): Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        end_date (str): End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        max_lat (float): Maximum latitude\n        max_lon (float): Maximum longitude\n        min_lat (float): Minimum latitude\n        min_lon (float): Minimum longitude\n        source (optional str): Source of the data ie ECMWF\n        limit (optional int): The number of rows to be returned\n    }\n\n    Returns:\n        DataFrame: A dataframe of raw weather forecast data.\n    \"\"\"\n    try:\n        query = _query_builder(parameters_dict, \"raw_grid\")\n\n        print(query)\n\n        try:\n            cursor = connection.cursor()\n            cursor.execute(query)\n            df = cursor.fetch_all()\n            cursor.close()\n            connection.close()\n            return df\n        except Exception as e:\n            logging.exception(\"error returning dataframe\")\n            raise e\n\n    except Exception as e:\n        logging.exception(\"error with raw function\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/functions/weather/raw/#src.sdk.python.rtdip_sdk.queries.weather.raw.get_point","title":"<code>get_point(connection, parameters_dict)</code>","text":"<p>A function to return back raw data by querying databricks SQL Warehouse using a connection specified by the user.</p> <p>This will return the raw weather forecast data for a single point.</p> <p>The available connectors by RTDIP are Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect.</p> <p>The available authentication methods are Certificate Authentication, Client Secret Authentication or Default Authentication. See documentation.</p> <p>This function requires the user to input a dictionary of parameters which are focused on Weather Data. (See Attributes table below)</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>object</code> <p>Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)</p> required <code>parameters_dict</code> <code>dict</code> <p>A dictionary of parameters (see Attributes table below)</p> required <p>Attributes:</p> Name Type Description <code>forecast</code> <code>str</code> <p>Business unit</p> <code>region</code> <code>str</code> <p>Region</p> <code>data_security_level</code> <code>str</code> <p>Level of data security</p> <code>data_type</code> <code>str</code> <p>Type of the data (float, integer, double, string)</p> <code>start_date</code> <code>str</code> <p>Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> <code>end_date</code> <code>str</code> <p>End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> <code>lat</code> <code>float</code> <p>latitude</p> <code>lon</code> <code>float</code> <p>longitude</p> <code>source</code> <code>optional str</code> <p>Source of the data ie ECMWF</p> <code>limit</code> <code>optional int</code> <p>The number of rows to be returned</p> <p>}</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe of raw weather forecast data.</p> Source code in <code>src/sdk/python/rtdip_sdk/queries/weather/raw.py</code> <pre><code>def get_point(connection: object, parameters_dict: dict) -&gt; pd.DataFrame:\n\"\"\"\n    A function to return back raw data by querying databricks SQL Warehouse using a connection specified by the user.\n\n    This will return the raw weather forecast data for a single point.\n\n    The available connectors by RTDIP are Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect.\n\n    The available authentication methods are Certificate Authentication, Client Secret Authentication or Default Authentication. See documentation.\n\n    This function requires the user to input a dictionary of parameters which are focused on Weather Data. (See Attributes table below)\n\n    Args:\n        connection: Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)\n        parameters_dict: A dictionary of parameters (see Attributes table below)\n\n    Attributes:\n        forecast (str): Business unit\n        region (str): Region\n        data_security_level (str): Level of data security\n        data_type (str): Type of the data (float, integer, double, string)\n        start_date (str): Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        end_date (str): End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        lat (float): latitude\n        lon (float): longitude\n        source (optional str): Source of the data ie ECMWF\n        limit (optional int): The number of rows to be returned\n    }\n\n    Returns:\n        DataFrame: A dataframe of raw weather forecast data.\n    \"\"\"\n    try:\n        query = _query_builder(parameters_dict, \"raw_point\")\n\n        print(query)\n\n        try:\n            cursor = connection.cursor()\n            cursor.execute(query)\n            df = cursor.fetch_all()\n            cursor.close()\n            connection.close()\n            return df\n        except Exception as e:\n            logging.exception(\"error returning dataframe\")\n            raise e\n\n    except Exception as e:\n        logging.exception(\"error with raw function\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/functions/weather/weather_query_builder/","title":"Weather Query Builder","text":""},{"location":"sdk/code-reference/query/functions/weather/weather_query_builder/#src.sdk.python.rtdip_sdk.queries.weather.weather_query_builder.WeatherQueryBuilder","title":"<code>WeatherQueryBuilder</code>","text":"<p>A builder for developing RTDIP queries using any delta table</p> Source code in <code>src/sdk/python/rtdip_sdk/queries/weather/weather_query_builder.py</code> <pre><code>class WeatherQueryBuilder:\n\"\"\"\n    A builder for developing RTDIP queries using any delta table\n    \"\"\"\n\n    parameters: dict\n    connection: ConnectionInterface\n\n    def connect(self, connection: ConnectionInterface):\n\"\"\"\n        Specifies the connection to be used for the query\n\n        Args:\n            connection: Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)\n        \"\"\"\n        self.connection = connection\n        return self\n\n    def source(\n        self,\n        source: str,\n        tagname_column: str = \"TagName\",\n        timestamp_column: str = \"EventTime\",\n        status_column: Union[str, None] = \"Status\",\n        value_column: str = \"Value\",\n    ):\n\"\"\"\n        Specifies the source of the query\n\n        Args:\n            source (str): Source of the query can be a Unity Catalog table, Hive metastore table or path\n            tagname_column (optional str): The column name in the source that contains the tagnames or series\n            timestamp_column (optional str): The timestamp column name in the source\n            status_column (optional str): The status column name in the source indicating `Good` or `Bad`. If this is not available, specify `None`\n            value_column (optional str): The value column name in the source which is normally a float or string value for the time series event\n        \"\"\"\n        self.data_source = \"`.`\".join(source.split(\".\"))\n        self.tagname_column = tagname_column\n        self.timestamp_column = timestamp_column\n        self.status_column = status_column\n        self.value_column = value_column\n        return self\n\n    def raw_point(\n        self,\n        forecast: str,\n        region: str,\n        data_security_level: str,\n        data_type: str,\n        start_date: str,\n        end_date: str,\n        lat: float,\n        lon: float,\n        source: str = None,\n        time_zone: str = None,\n        include_bad_data: bool = False,\n        limit: int = None,\n    ) -&gt; DataFrame:\n\"\"\"\n        A function to return back raw data for a point.\n\n        Args:\n            forecast (str): Business unit\n            region (str): Region\n            data_security_level (str): Level of data security\n            data_type (str): Type of the data (float, integer, double, string)\n            start_date (str): Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n            end_date (str): End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n            lat (float): latitude\n            lon (float): longitude\n            source (optional str): Source of the data ie ECMWF\n            time_zone (str): Timezone of the data\n            include_bad_data (bool): Include \"Bad\" data points with True or remove \"Bad\" data points with False\n            limit (optional int): The number of rows to be returned\n\n        Returns:\n            DataFrame: A dataframe of raw timeseries data.\n        \"\"\"\n        raw_parameters = {\n            \"forecast\": forecast,\n            \"region\": region,\n            \"data_security_level\": data_security_level,\n            \"data_type\": data_type,\n            \"start_date\": start_date,\n            \"end_date\": end_date,\n            \"lat\": lat,\n            \"lon\": lon,\n            \"source\": source,\n            \"time_zone\": time_zone,\n            \"include_bad_data\": include_bad_data,\n            \"limit\": limit,\n            \"supress_warning\": True,\n        }\n\n        return raw.get_point(self.connection, raw_parameters)\n\n    def latest_point(\n        self,\n        forecast: str,\n        region: str,\n        data_security_level: str,\n        data_type: str,\n        lat: float,\n        lon: float,\n        source: str = None,\n        limit: int = None,\n    ) -&gt; DataFrame:\n\"\"\"\n        A function to return back the latest data for a point.\n\n        Args:\n            forecast (str): Business unit\n            region (str): Region\n            data_security_level (str): Level of data security\n            data_type (str): Type of the data (float, integer, double, string)\n            lat (float): latitude\n            lon (float): longitude\n            source (optional str): Source of the data ie ECMWF\n            limit (optional int): The number of rows to be returned\n\n        Returns:\n            DataFrame: A dataframe of raw timeseries data.\n        \"\"\"\n        raw_parameters = {\n            \"forecast\": forecast,\n            \"region\": region,\n            \"data_security_level\": data_security_level,\n            \"data_type\": data_type,\n            \"lat\": lat,\n            \"lon\": lon,\n            \"source\": source,\n            \"limit\": limit,\n            \"supress_warning\": True,\n        }\n\n        return latest.get_point(self.connection, raw_parameters)\n\n    def raw_grid(  # NOSONAR\n        self,  # NOSONAR\n        forecast: str,\n        region: str,\n        data_security_level: str,\n        data_type: str,\n        start_date: str,\n        end_date: str,\n        min_lat: float,\n        min_lon: float,\n        max_lat: float,\n        max_lon: float,\n        source: str = None,\n        time_zone: str = None,\n        include_bad_data: bool = False,\n        limit: int = None,  # NOSONAR\n    ) -&gt; DataFrame:\n\"\"\"\n        A function to return back raw data for a point.\n\n        Args:\n            forecast (str): Business unit\n            region (str): Region\n            data_security_level (str): Level of data security\n            data_type (str): Type of the data (float, integer, double, string)\n            start_date (str): Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n            end_date (str): End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n            min_lat (float): Min latitude\n            min_lon (float): Min longitude\n            max_lat (float): Max latitude\n            max_lon (float): Max longitude\n            source (optional str): Source of the data ie ECMWF\n            time_zone (str): Timezone of the data\n            include_bad_data (bool): Include \"Bad\" data points with True or remove \"Bad\" data points with False\n            limit (optional int): The number of rows to be returned\n\n        Returns:\n            DataFrame: A dataframe of raw timeseries data.\n        \"\"\"\n        raw_parameters = {\n            \"forecast\": forecast,\n            \"region\": region,\n            \"data_security_level\": data_security_level,\n            \"data_type\": data_type,\n            \"start_date\": start_date,\n            \"end_date\": end_date,\n            \"min_lat\": min_lat,\n            \"min_lon\": min_lon,\n            \"max_lat\": max_lat,\n            \"max_lon\": max_lon,\n            \"source\": source,\n            \"time_zone\": time_zone,\n            \"include_bad_data\": include_bad_data,\n            \"limit\": limit,\n            \"supress_warning\": True,\n        }\n\n        return raw.get_grid(self.connection, raw_parameters)\n\n    def latest_grid(\n        self,\n        forecast: str,\n        region: str,\n        data_security_level: str,\n        data_type: str,\n        min_lat: float,\n        min_lon: float,\n        max_lat: float,\n        max_lon: float,\n        source: str = None,\n        limit: int = None,\n    ) -&gt; DataFrame:\n\"\"\"\n        A function to return back the latest data for a point.\n\n        Args:\n            forecast (str): Business unit\n            region (str): Region\n            data_security_level (str): Level of data security\n            data_type (str): Type of the data (float, integer, double, string)\n            min_lat (float): Min latitude\n            min_lon (float): Min longitude\n            max_lat (float): Max latitude\n            max_lon (float): Max longitude\n            source (optional str): Source of the data ie ECMWF\n            limit (optional int): The number of rows to be returned\n\n        Returns:\n            DataFrame: A dataframe of raw timeseries data.\n        \"\"\"\n        raw_parameters = {\n            \"forecast\": forecast,\n            \"region\": region,\n            \"data_security_level\": data_security_level,\n            \"data_type\": data_type,\n            \"min_lat\": min_lat,\n            \"min_lon\": min_lon,\n            \"max_lat\": max_lat,\n            \"max_lon\": max_lon,\n            \"source\": source,\n            \"limit\": limit,\n            \"supress_warning\": True,\n        }\n\n        return latest.get_grid(self.connection, raw_parameters)\n</code></pre>"},{"location":"sdk/code-reference/query/functions/weather/weather_query_builder/#src.sdk.python.rtdip_sdk.queries.weather.weather_query_builder.WeatherQueryBuilder.connect","title":"<code>connect(connection)</code>","text":"<p>Specifies the connection to be used for the query</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>ConnectionInterface</code> <p>Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)</p> required Source code in <code>src/sdk/python/rtdip_sdk/queries/weather/weather_query_builder.py</code> <pre><code>def connect(self, connection: ConnectionInterface):\n\"\"\"\n    Specifies the connection to be used for the query\n\n    Args:\n        connection: Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)\n    \"\"\"\n    self.connection = connection\n    return self\n</code></pre>"},{"location":"sdk/code-reference/query/functions/weather/weather_query_builder/#src.sdk.python.rtdip_sdk.queries.weather.weather_query_builder.WeatherQueryBuilder.latest_grid","title":"<code>latest_grid(forecast, region, data_security_level, data_type, min_lat, min_lon, max_lat, max_lon, source=None, limit=None)</code>","text":"<p>A function to return back the latest data for a point.</p> <p>Parameters:</p> Name Type Description Default <code>forecast</code> <code>str</code> <p>Business unit</p> required <code>region</code> <code>str</code> <p>Region</p> required <code>data_security_level</code> <code>str</code> <p>Level of data security</p> required <code>data_type</code> <code>str</code> <p>Type of the data (float, integer, double, string)</p> required <code>min_lat</code> <code>float</code> <p>Min latitude</p> required <code>min_lon</code> <code>float</code> <p>Min longitude</p> required <code>max_lat</code> <code>float</code> <p>Max latitude</p> required <code>max_lon</code> <code>float</code> <p>Max longitude</p> required <code>source</code> <code>optional str</code> <p>Source of the data ie ECMWF</p> <code>None</code> <code>limit</code> <code>optional int</code> <p>The number of rows to be returned</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe of raw timeseries data.</p> Source code in <code>src/sdk/python/rtdip_sdk/queries/weather/weather_query_builder.py</code> <pre><code>def latest_grid(\n    self,\n    forecast: str,\n    region: str,\n    data_security_level: str,\n    data_type: str,\n    min_lat: float,\n    min_lon: float,\n    max_lat: float,\n    max_lon: float,\n    source: str = None,\n    limit: int = None,\n) -&gt; DataFrame:\n\"\"\"\n    A function to return back the latest data for a point.\n\n    Args:\n        forecast (str): Business unit\n        region (str): Region\n        data_security_level (str): Level of data security\n        data_type (str): Type of the data (float, integer, double, string)\n        min_lat (float): Min latitude\n        min_lon (float): Min longitude\n        max_lat (float): Max latitude\n        max_lon (float): Max longitude\n        source (optional str): Source of the data ie ECMWF\n        limit (optional int): The number of rows to be returned\n\n    Returns:\n        DataFrame: A dataframe of raw timeseries data.\n    \"\"\"\n    raw_parameters = {\n        \"forecast\": forecast,\n        \"region\": region,\n        \"data_security_level\": data_security_level,\n        \"data_type\": data_type,\n        \"min_lat\": min_lat,\n        \"min_lon\": min_lon,\n        \"max_lat\": max_lat,\n        \"max_lon\": max_lon,\n        \"source\": source,\n        \"limit\": limit,\n        \"supress_warning\": True,\n    }\n\n    return latest.get_grid(self.connection, raw_parameters)\n</code></pre>"},{"location":"sdk/code-reference/query/functions/weather/weather_query_builder/#src.sdk.python.rtdip_sdk.queries.weather.weather_query_builder.WeatherQueryBuilder.latest_point","title":"<code>latest_point(forecast, region, data_security_level, data_type, lat, lon, source=None, limit=None)</code>","text":"<p>A function to return back the latest data for a point.</p> <p>Parameters:</p> Name Type Description Default <code>forecast</code> <code>str</code> <p>Business unit</p> required <code>region</code> <code>str</code> <p>Region</p> required <code>data_security_level</code> <code>str</code> <p>Level of data security</p> required <code>data_type</code> <code>str</code> <p>Type of the data (float, integer, double, string)</p> required <code>lat</code> <code>float</code> <p>latitude</p> required <code>lon</code> <code>float</code> <p>longitude</p> required <code>source</code> <code>optional str</code> <p>Source of the data ie ECMWF</p> <code>None</code> <code>limit</code> <code>optional int</code> <p>The number of rows to be returned</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe of raw timeseries data.</p> Source code in <code>src/sdk/python/rtdip_sdk/queries/weather/weather_query_builder.py</code> <pre><code>def latest_point(\n    self,\n    forecast: str,\n    region: str,\n    data_security_level: str,\n    data_type: str,\n    lat: float,\n    lon: float,\n    source: str = None,\n    limit: int = None,\n) -&gt; DataFrame:\n\"\"\"\n    A function to return back the latest data for a point.\n\n    Args:\n        forecast (str): Business unit\n        region (str): Region\n        data_security_level (str): Level of data security\n        data_type (str): Type of the data (float, integer, double, string)\n        lat (float): latitude\n        lon (float): longitude\n        source (optional str): Source of the data ie ECMWF\n        limit (optional int): The number of rows to be returned\n\n    Returns:\n        DataFrame: A dataframe of raw timeseries data.\n    \"\"\"\n    raw_parameters = {\n        \"forecast\": forecast,\n        \"region\": region,\n        \"data_security_level\": data_security_level,\n        \"data_type\": data_type,\n        \"lat\": lat,\n        \"lon\": lon,\n        \"source\": source,\n        \"limit\": limit,\n        \"supress_warning\": True,\n    }\n\n    return latest.get_point(self.connection, raw_parameters)\n</code></pre>"},{"location":"sdk/code-reference/query/functions/weather/weather_query_builder/#src.sdk.python.rtdip_sdk.queries.weather.weather_query_builder.WeatherQueryBuilder.raw_grid","title":"<code>raw_grid(forecast, region, data_security_level, data_type, start_date, end_date, min_lat, min_lon, max_lat, max_lon, source=None, time_zone=None, include_bad_data=False, limit=None)</code>","text":"<p>A function to return back raw data for a point.</p> <p>Parameters:</p> Name Type Description Default <code>forecast</code> <code>str</code> <p>Business unit</p> required <code>region</code> <code>str</code> <p>Region</p> required <code>data_security_level</code> <code>str</code> <p>Level of data security</p> required <code>data_type</code> <code>str</code> <p>Type of the data (float, integer, double, string)</p> required <code>start_date</code> <code>str</code> <p>Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> required <code>end_date</code> <code>str</code> <p>End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> required <code>min_lat</code> <code>float</code> <p>Min latitude</p> required <code>min_lon</code> <code>float</code> <p>Min longitude</p> required <code>max_lat</code> <code>float</code> <p>Max latitude</p> required <code>max_lon</code> <code>float</code> <p>Max longitude</p> required <code>source</code> <code>optional str</code> <p>Source of the data ie ECMWF</p> <code>None</code> <code>time_zone</code> <code>str</code> <p>Timezone of the data</p> <code>None</code> <code>include_bad_data</code> <code>bool</code> <p>Include \"Bad\" data points with True or remove \"Bad\" data points with False</p> <code>False</code> <code>limit</code> <code>optional int</code> <p>The number of rows to be returned</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe of raw timeseries data.</p> Source code in <code>src/sdk/python/rtdip_sdk/queries/weather/weather_query_builder.py</code> <pre><code>def raw_grid(  # NOSONAR\n    self,  # NOSONAR\n    forecast: str,\n    region: str,\n    data_security_level: str,\n    data_type: str,\n    start_date: str,\n    end_date: str,\n    min_lat: float,\n    min_lon: float,\n    max_lat: float,\n    max_lon: float,\n    source: str = None,\n    time_zone: str = None,\n    include_bad_data: bool = False,\n    limit: int = None,  # NOSONAR\n) -&gt; DataFrame:\n\"\"\"\n    A function to return back raw data for a point.\n\n    Args:\n        forecast (str): Business unit\n        region (str): Region\n        data_security_level (str): Level of data security\n        data_type (str): Type of the data (float, integer, double, string)\n        start_date (str): Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        end_date (str): End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        min_lat (float): Min latitude\n        min_lon (float): Min longitude\n        max_lat (float): Max latitude\n        max_lon (float): Max longitude\n        source (optional str): Source of the data ie ECMWF\n        time_zone (str): Timezone of the data\n        include_bad_data (bool): Include \"Bad\" data points with True or remove \"Bad\" data points with False\n        limit (optional int): The number of rows to be returned\n\n    Returns:\n        DataFrame: A dataframe of raw timeseries data.\n    \"\"\"\n    raw_parameters = {\n        \"forecast\": forecast,\n        \"region\": region,\n        \"data_security_level\": data_security_level,\n        \"data_type\": data_type,\n        \"start_date\": start_date,\n        \"end_date\": end_date,\n        \"min_lat\": min_lat,\n        \"min_lon\": min_lon,\n        \"max_lat\": max_lat,\n        \"max_lon\": max_lon,\n        \"source\": source,\n        \"time_zone\": time_zone,\n        \"include_bad_data\": include_bad_data,\n        \"limit\": limit,\n        \"supress_warning\": True,\n    }\n\n    return raw.get_grid(self.connection, raw_parameters)\n</code></pre>"},{"location":"sdk/code-reference/query/functions/weather/weather_query_builder/#src.sdk.python.rtdip_sdk.queries.weather.weather_query_builder.WeatherQueryBuilder.raw_point","title":"<code>raw_point(forecast, region, data_security_level, data_type, start_date, end_date, lat, lon, source=None, time_zone=None, include_bad_data=False, limit=None)</code>","text":"<p>A function to return back raw data for a point.</p> <p>Parameters:</p> Name Type Description Default <code>forecast</code> <code>str</code> <p>Business unit</p> required <code>region</code> <code>str</code> <p>Region</p> required <code>data_security_level</code> <code>str</code> <p>Level of data security</p> required <code>data_type</code> <code>str</code> <p>Type of the data (float, integer, double, string)</p> required <code>start_date</code> <code>str</code> <p>Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> required <code>end_date</code> <code>str</code> <p>End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> required <code>lat</code> <code>float</code> <p>latitude</p> required <code>lon</code> <code>float</code> <p>longitude</p> required <code>source</code> <code>optional str</code> <p>Source of the data ie ECMWF</p> <code>None</code> <code>time_zone</code> <code>str</code> <p>Timezone of the data</p> <code>None</code> <code>include_bad_data</code> <code>bool</code> <p>Include \"Bad\" data points with True or remove \"Bad\" data points with False</p> <code>False</code> <code>limit</code> <code>optional int</code> <p>The number of rows to be returned</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe of raw timeseries data.</p> Source code in <code>src/sdk/python/rtdip_sdk/queries/weather/weather_query_builder.py</code> <pre><code>def raw_point(\n    self,\n    forecast: str,\n    region: str,\n    data_security_level: str,\n    data_type: str,\n    start_date: str,\n    end_date: str,\n    lat: float,\n    lon: float,\n    source: str = None,\n    time_zone: str = None,\n    include_bad_data: bool = False,\n    limit: int = None,\n) -&gt; DataFrame:\n\"\"\"\n    A function to return back raw data for a point.\n\n    Args:\n        forecast (str): Business unit\n        region (str): Region\n        data_security_level (str): Level of data security\n        data_type (str): Type of the data (float, integer, double, string)\n        start_date (str): Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        end_date (str): End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        lat (float): latitude\n        lon (float): longitude\n        source (optional str): Source of the data ie ECMWF\n        time_zone (str): Timezone of the data\n        include_bad_data (bool): Include \"Bad\" data points with True or remove \"Bad\" data points with False\n        limit (optional int): The number of rows to be returned\n\n    Returns:\n        DataFrame: A dataframe of raw timeseries data.\n    \"\"\"\n    raw_parameters = {\n        \"forecast\": forecast,\n        \"region\": region,\n        \"data_security_level\": data_security_level,\n        \"data_type\": data_type,\n        \"start_date\": start_date,\n        \"end_date\": end_date,\n        \"lat\": lat,\n        \"lon\": lon,\n        \"source\": source,\n        \"time_zone\": time_zone,\n        \"include_bad_data\": include_bad_data,\n        \"limit\": limit,\n        \"supress_warning\": True,\n    }\n\n    return raw.get_point(self.connection, raw_parameters)\n</code></pre>"},{"location":"sdk/code-reference/query/functions/weather/weather_query_builder/#src.sdk.python.rtdip_sdk.queries.weather.weather_query_builder.WeatherQueryBuilder.source","title":"<code>source(source, tagname_column='TagName', timestamp_column='EventTime', status_column='Status', value_column='Value')</code>","text":"<p>Specifies the source of the query</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>Source of the query can be a Unity Catalog table, Hive metastore table or path</p> required <code>tagname_column</code> <code>optional str</code> <p>The column name in the source that contains the tagnames or series</p> <code>'TagName'</code> <code>timestamp_column</code> <code>optional str</code> <p>The timestamp column name in the source</p> <code>'EventTime'</code> <code>status_column</code> <code>optional str</code> <p>The status column name in the source indicating <code>Good</code> or <code>Bad</code>. If this is not available, specify <code>None</code></p> <code>'Status'</code> <code>value_column</code> <code>optional str</code> <p>The value column name in the source which is normally a float or string value for the time series event</p> <code>'Value'</code> Source code in <code>src/sdk/python/rtdip_sdk/queries/weather/weather_query_builder.py</code> <pre><code>def source(\n    self,\n    source: str,\n    tagname_column: str = \"TagName\",\n    timestamp_column: str = \"EventTime\",\n    status_column: Union[str, None] = \"Status\",\n    value_column: str = \"Value\",\n):\n\"\"\"\n    Specifies the source of the query\n\n    Args:\n        source (str): Source of the query can be a Unity Catalog table, Hive metastore table or path\n        tagname_column (optional str): The column name in the source that contains the tagnames or series\n        timestamp_column (optional str): The timestamp column name in the source\n        status_column (optional str): The status column name in the source indicating `Good` or `Bad`. If this is not available, specify `None`\n        value_column (optional str): The value column name in the source which is normally a float or string value for the time series event\n    \"\"\"\n    self.data_source = \"`.`\".join(source.split(\".\"))\n    self.tagname_column = tagname_column\n    self.timestamp_column = timestamp_column\n    self.status_column = status_column\n    self.value_column = value_column\n    return self\n</code></pre>"},{"location":"sdk/code-reference/query/sql/sql_query/","title":"SQL Query","text":""},{"location":"sdk/code-reference/query/sql/sql_query/#src.sdk.python.rtdip_sdk.queries.sql.sql_query.SQLQueryBuilder","title":"<code>SQLQueryBuilder</code>","text":"<p>A builder for developing RTDIP queries using any delta table</p> Source code in <code>src/sdk/python/rtdip_sdk/queries/sql/sql_query.py</code> <pre><code>class SQLQueryBuilder:\n\"\"\"\n    A builder for developing RTDIP queries using any delta table\n    \"\"\"\n\n    sql_query: dict\n    connection: ConnectionInterface\n\n    def get(self, connection=object, sql_query=str) -&gt; pd.DataFrame:\n\"\"\"\n        A function to return back raw data by querying databricks SQL Warehouse using a connection specified by the user.\n\n        The available connectors by RTDIP are Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect.\n\n        The available authentication methods are Certificate Authentication, Client Secret Authentication or Default Authentication. See documentation.\n\n        This function requires the user to input a dictionary of parameters. (See Attributes table below)\n\n        Args:\n            connection (obj): Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)\n            sql_query (str): A string of the SQL query to be executed.\n\n        Returns:\n            DataFrame: A dataframe of raw timeseries data.\n        \"\"\"\n        try:\n            try:\n                cursor = connection.cursor()\n                cursor.execute(sql_query)\n                df = cursor.fetch_all()\n                cursor.close()\n                connection.close()\n                return df\n            except Exception as e:\n                logging.exception(\"Error returning dataframe\")\n                raise e\n\n        except Exception as e:\n            logging.exception(\"error with sql query\")\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/query/sql/sql_query/#src.sdk.python.rtdip_sdk.queries.sql.sql_query.SQLQueryBuilder.get","title":"<code>get(connection=object, sql_query=str)</code>","text":"<p>A function to return back raw data by querying databricks SQL Warehouse using a connection specified by the user.</p> <p>The available connectors by RTDIP are Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect.</p> <p>The available authentication methods are Certificate Authentication, Client Secret Authentication or Default Authentication. See documentation.</p> <p>This function requires the user to input a dictionary of parameters. (See Attributes table below)</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>obj</code> <p>Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)</p> <code>object</code> <code>sql_query</code> <code>str</code> <p>A string of the SQL query to be executed.</p> <code>str</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe of raw timeseries data.</p> Source code in <code>src/sdk/python/rtdip_sdk/queries/sql/sql_query.py</code> <pre><code>def get(self, connection=object, sql_query=str) -&gt; pd.DataFrame:\n\"\"\"\n    A function to return back raw data by querying databricks SQL Warehouse using a connection specified by the user.\n\n    The available connectors by RTDIP are Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect.\n\n    The available authentication methods are Certificate Authentication, Client Secret Authentication or Default Authentication. See documentation.\n\n    This function requires the user to input a dictionary of parameters. (See Attributes table below)\n\n    Args:\n        connection (obj): Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)\n        sql_query (str): A string of the SQL query to be executed.\n\n    Returns:\n        DataFrame: A dataframe of raw timeseries data.\n    \"\"\"\n    try:\n        try:\n            cursor = connection.cursor()\n            cursor.execute(sql_query)\n            df = cursor.fetch_all()\n            cursor.close()\n            connection.close()\n            return df\n        except Exception as e:\n            logging.exception(\"Error returning dataframe\")\n            raise e\n\n    except Exception as e:\n        logging.exception(\"error with sql query\")\n        raise e\n</code></pre>"},{"location":"sdk/examples/pipelines/deploy/ECMWF-to-Delta/","title":"ECMWF to Delta Pipeline","text":"<p>This article provides a guide on how to execute a pipeline that makes an API request to pull the ECMWF MARS Data as a .nc file, transform the .nc file to a dataframe from a grid range and writes to a Delta Table locally using the RTDIP SDK. </p> <p>This pipeline was tested on an M2 Macbook Pro using VS Code in a Python (3.11) environment.</p>"},{"location":"sdk/examples/pipelines/deploy/ECMWF-to-Delta/#prerequisites","title":"Prerequisites","text":"<p>This pipeline job requires the packages:</p> <ul> <li>rtdip-sdk</li> </ul>"},{"location":"sdk/examples/pipelines/deploy/ECMWF-to-Delta/#components","title":"Components","text":"Name Description SparkECMWFWeatherForecastSource Pulls data from ECMWF MARS API and stores as a .nc file. ECMWFExtractGridToWeatherDataModel Transforms ECMWF .nc file to a dataframe ingesting Grid Data. SparkDeltaDestination Writes to Delta."},{"location":"sdk/examples/pipelines/deploy/ECMWF-to-Delta/#common-errors","title":"Common Errors","text":"Error Solution [com.google.common.util.concurrent.ExecutionError: java.lang.NoClassDefFoundError: org/apache/spark/ErrorClassesJsonReader] The Delta version in the Spark Session must be compatible with your local Pyspark version. See here for version compatibility"},{"location":"sdk/examples/pipelines/deploy/ECMWF-to-Delta/#example","title":"Example","text":"<p>Below is an example of how to read from and write to Delta Tables locally without the need for Spark</p> <pre><code>from rtdip_sdk.pipelines.sources.spark.ecmwf.weather_forecast import SparkECMWFWeatherForecastSource\nfrom rtdip_sdk.pipelines.transformers.spark.ecmwf.nc_extractgrid_to_weather_data_model import ECMWFExtractGridToWeatherDataModel\nfrom rtdip_sdk.pipelines.destinations.spark.delta import SparkDeltaDestination\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n\necmwf_api_key = \"xxxxx\"\necmwf_api_email = \"xxxxx\"\n\ndate_start = \"2020-10-01 00:00:00\"\ndate_end = \"2020-10-02 00:00:00\"\n\necmwf_class = \"od\"\nstream = \"oper\"\nexpver = \"1\"\nleveltype = \"sfc\"\nrun_interval = \"12\"\nrun_frequency = \"H\"\ngrid_step = 0.1\nec_vars = [\n    \"cbh\", \"dsrp\", \"sp\", \"tcwv\", \"tcc\"\n]\n\ntag_prefix = \"US:[55, -130, 20, -60]:\"\nmethod = \"nearest\"\npath = '/dbfs/forecast/nc/US/' # Path to save the data can be changed\nforecast_area = [55, -130, 20, -60]  # N/W/S/E\nlat_max = 50\nlat_min = 25\nlon_max = -65\nlon_min = -75\n\n\ndef pipeline():\n\n    spark = SparkSessionUtility(config={}).execute()\n\n\n    weather_source = SparkECMWFWeatherForecastSource(\n        spark=spark,\n        date_start=date_start,\n        date_end=date_end,\n        save_path=path,\n        ecmwf_class=ecmwf_class,\n        stream=stream,\n        expver=expver,\n        leveltype=leveltype,\n        ec_vars=ec_vars,\n        forecast_area=forecast_area,\n        ecmwf_api_key=ecmwf_api_key,\n        ecmwf_api_email=ecmwf_api_email,\n    )\n\n    weather_source.read_batch()\n\n    extract = ECMWFExtractGridToWeatherDataModel(\n            lat_min=lat_min,\n            lat_max=lat_max,\n            lon_min=lon_min,\n            lon_max=lon_max,\n            grid_step=grid_step,\n            load_path=path,\n            date_start=date_start,\n            date_end=date_end,\n            run_interval=run_interval,\n            run_frequency=run_frequency\n    )\n\n    df = extract.transform(tag_prefix, ec_vars, method)\n\n    sparkdf = spark.createDataFrame(df) \n\n    SparkDeltaDestination(\n        data=sparkdf, options={}, destination=\"{path/to/table}\"\n    ).write_batch()\n\n\nif __name__ == \"__main__\":\n    pipeline()\n</code></pre>"},{"location":"sdk/examples/pipelines/deploy/EdgeX-Eventhub-to-Delta/","title":"EdgeX Eventhub to Delta Pipeline","text":"<p>This article provides a guide on how to execute a pipeline that batch reads EdgeX data from an Eventhub and writes to a Delta Table locally using the RTDIP SDK. This pipeline was tested on an M2 Macbook Pro using VS Code in a Python (3.10) environment.</p>"},{"location":"sdk/examples/pipelines/deploy/EdgeX-Eventhub-to-Delta/#prerequisites","title":"Prerequisites","text":"<p>This pipeline job requires the packages:</p> <ul> <li>rtdip-sdk</li> </ul>"},{"location":"sdk/examples/pipelines/deploy/EdgeX-Eventhub-to-Delta/#components","title":"Components","text":"Name Description SparkEventhubSource Reads data from an Eventhub. BinaryToStringTransformer Transforms Spark DataFrame column to string. EdgeXOPCUAJsonToPCDMTransformer Transforms EdgeX to PCDM. SparkDeltaDestination Writes to Delta."},{"location":"sdk/examples/pipelines/deploy/EdgeX-Eventhub-to-Delta/#common-errors","title":"Common Errors","text":"Error Solution [com.google.common.util.concurrent.ExecutionError: java.lang.NoClassDefFoundError: org/apache/spark/ErrorClassesJsonReader] The Delta version in the Spark Session must be compatible with your local Pyspark version. See here for version compatibility"},{"location":"sdk/examples/pipelines/deploy/EdgeX-Eventhub-to-Delta/#example","title":"Example","text":"<p>Below is an example of how to read from and write to Delta Tables locally without the need for Spark</p> <pre><code>from rtdip_sdk.pipelines.sources.spark.eventhub import SparkEventhubSource\nfrom rtdip_sdk.pipelines.transformers.spark.binary_to_string import (\n    BinaryToStringTransformer,\n)\nfrom rtdip_sdk.pipelines.destinations.spark.delta import SparkDeltaDestination\nfrom rtdip_sdk.pipelines.transformers.spark.edgex_opcua_json_to_pcdm import (\n    EdgeXOPCUAJsonToPCDMTransformer,\n)\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\nimport json\n\n\ndef pipeline():\n\n    spark = SparkSessionUtility(config={}).execute()\n\n    ehConf = {\n        \"eventhubs.connectionString\": \"{EventhubConnectionString}\",\n        \"eventhubs.consumerGroup\": \"{EventhubConsumerGroup}\",\n        \"eventhubs.startingPosition\": json.dumps(\n            {\"offset\": \"0\", \"seqNo\": -1, \"enqueuedTime\": None, \"isInclusive\": True}\n        ),\n    }\n\n    source = SparkEventhubSource(spark, ehConf).read_batch()\n    string_data = BinaryToStringTransformer(source, \"body\", \"body\").transform()\n    PCDM_data = EdgeXOPCUAJsonToPCDMTransformer(string_data, \"body\").transform()\n    SparkDeltaDestination(\n        data=PCDM_data, options={}, destination=\"{path/to/table}\"\n    ).write_batch()\n\n\nif __name__ == \"__main__\":\n    pipeline()\n</code></pre>"},{"location":"sdk/examples/pipelines/deploy/MISODailyLoad-Batch-Pipeline-Local/","title":"MISO Pipeline using RTDIP","text":"<p>This article provides a guide on how to execute a MISO pipeline using RTDIP. This pipeline was tested on an M2 Macbook Pro using VS Code in a Conda (3.11) environment.</p>"},{"location":"sdk/examples/pipelines/deploy/MISODailyLoad-Batch-Pipeline-Local/#prerequisites","title":"Prerequisites","text":"<p>This pipeline assumes you have followed the installation instructions as specified in the Getting Started section. In particular ensure you have installed the following:</p> <ul> <li> <p>RTDIP SDK</p> </li> <li> <p>Java</p> </li> </ul> <p>RTDIP SDK Installation</p> <p>Ensure you have installed the RTDIP SDK as follows: <pre><code>pip install \"rtdip-sdk[pipelines,pyspark]\"\n</code></pre></p>"},{"location":"sdk/examples/pipelines/deploy/MISODailyLoad-Batch-Pipeline-Local/#components","title":"Components","text":"Name Description MISODailyLoadISOSource Read daily load data from MISO API. MISOToMDMTransformer Converts MISO Raw data into Meters Data Model. SparkDeltaDestination Writes to a Delta table."},{"location":"sdk/examples/pipelines/deploy/MISODailyLoad-Batch-Pipeline-Local/#example","title":"Example","text":"<p>Below is an example of how to set up a pipeline to read daily load data from the MISO API, transform it into the Meters Data Model and write it to a Delta table. <pre><code>from rtdip_sdk.pipelines.sources import MISODailyLoadISOSource\nfrom rtdip_sdk.pipelines.transformers import MISOToMDMTransformer\nfrom rtdip_sdk.pipelines.destinations import SparkDeltaDestination\nfrom pyspark.sql import SparkSession\n\ndef pipeline():\n    spark = SparkSession.builder.config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.4.0\")\\\n                                .config(\"spark.sql.extensions\",\"io.delta.sql.DeltaSparkSessionExtension\")\\\n                                .config(\"spark.sql.catalog.spark_catalog\",\"org.apache.spark.sql.delta.catalog.DeltaCatalog\").getOrCreate()\n\n    source_df = MISODailyLoadISOSource(\n        spark = spark,\n        options = {\n        \"load_type\": \"actual\",\n        \"date\": \"20230520\",\n        }\n    ).read_batch()\n\n    transform_value_df = MISOToMDMTransformer(\n        spark=spark,\n        data=source_df,\n        output_type= \"usage\"\n    ).transform()\n\n    transform_meta_df = MISOToMDMTransformer(\n        spark=spark,\n        data=source_df,\n        output_type= \"meta\"\n    ).transform()\n\n    SparkDeltaDestination(\n        data=transform_value_df,\n        options={\n            \"partitionBy\":\"timestamp\"\n        },   \n        destination=\"miso_usage_data\"\n    ).write_batch()    \n\n    SparkDeltaDestination(\n        data=transform_meta_df,\n        options={\n            \"partitionBy\":\"timestamp\"\n        },   \n        destination=\"miso_meta_data\"\n    ).write_batch() \n\nif __name__ == \"__main__\":\n    pipeline()\n</code></pre></p> <p>Using environments</p> <p>If using an environment, include the following lines at the top of your script to prevent a difference in Python versions in worker and driver: <pre><code>import sys, os\n\nos.environ['PYSPARK_PYTHON'] = sys.executable\nos.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n</code></pre></p>"},{"location":"sdk/examples/pipelines/deploy/PJMDailyLoad-Batch-Pipeline-Local/","title":"MISO Pipeline using RTDIP","text":"<p>This article provides a guide on how to execute a MISO pipeline using RTDIP. This pipeline was tested on an M2 Macbook Pro using VS Code in a Conda (3.11) environment.</p>"},{"location":"sdk/examples/pipelines/deploy/PJMDailyLoad-Batch-Pipeline-Local/#prerequisites","title":"Prerequisites","text":"<p>This pipeline assumes you have a valid API key from PJM and have followed the installation instructions as specified in the Getting Started section. In particular ensure you have installed the following:</p> <ul> <li> <p>RTDIP SDK</p> </li> <li> <p>Java</p> </li> </ul> <p>RTDIP SDK Installation</p> <p>Ensure you have installed the RTDIP SDK as follows: <pre><code>pip install \"rtdip-sdk[pipelines,pyspark]\"\n</code></pre></p>"},{"location":"sdk/examples/pipelines/deploy/PJMDailyLoad-Batch-Pipeline-Local/#components","title":"Components","text":"Name Description PJMDailyLoadISOSource Read daily load data from MISO API. PJMToMDMTransformer Converts PJM Raw data into Meters Data Model. SparkDeltaDestination Writes to a Delta table."},{"location":"sdk/examples/pipelines/deploy/PJMDailyLoad-Batch-Pipeline-Local/#example","title":"Example","text":"<p>Below is an example of how to set up a pipeline to read daily load data from the PJM API, transform it into the Meters Data Model and write it to a Delta table. <pre><code>from rtdip_sdk.pipelines.sources import PJMDailyLoadISOSource\nfrom rtdip_sdk.pipelines.transformers import PJMToMDMTransformer\nfrom rtdip_sdk.pipelines.destinations import SparkDeltaDestination\nfrom pyspark.sql import SparkSession\n\ndef pipeline():\n    spark = SparkSession.builder.config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.4.0\")\\\n                                .config(\"spark.sql.extensions\",\"io.delta.sql.DeltaSparkSessionExtension\")\\\n                                .config(\"spark.sql.catalog.spark_catalog\",\"org.apache.spark.sql.delta.catalog.DeltaCatalog\").getOrCreate()\n\n    source_df = PJMDailyLoadISOSource(\n        spark = spark,\n        options = {\n            \"api_key\": \"{api_key}\", \n            \"load_type\": \"actual\"\n        }\n    ).read_batch()\n\n    transform_value_df = PJMToMDMTransformer(\n        spark=spark,\n        data=source_df,\n        output_type= \"usage\"\n    ).transform()\n\n    transform_meta_df = PJMToMDMTransformer(\n        spark=spark,\n        data=source_df,\n        output_type= \"meta\"\n    ).transform()\n\n    SparkDeltaDestination(\n        data=transform_value_df,\n        options={\n            \"partitionBy\":\"timestamp\"\n        },   \n        destination=\"pjm_usage_data\"\n    ).write_batch()    \n\n    SparkDeltaDestination(\n        data=transform_meta_df,\n        options={\n            \"partitionBy\":\"timestamp\"\n        },   \n        destination=\"pjm_meta_data\"\n    ).write_batch() \n\nif __name__ == \"__main__\":\n    pipeline()\n</code></pre></p> <p>Using environments</p> <p>If using an environment, include the following lines at the top of your script to prevent a difference in Python versions in worker and driver: <pre><code>import sys, os\n\nos.environ['PYSPARK_PYTHON'] = sys.executable\nos.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n</code></pre></p>"},{"location":"sdk/examples/pipelines/deploy/Python-Delta-to-Delta/","title":"Python Delta Local Pipeline","text":"<p>This article provides a guide on how to execute a simple Delta Table copy locally without Spark using the RTDIP SDK. This pipeline was tested on an M2 Macbook Pro using VS Code in a Python (3.10) environment.</p>"},{"location":"sdk/examples/pipelines/deploy/Python-Delta-to-Delta/#prerequisites","title":"Prerequisites","text":"<p>This pipeline job requires the packages:</p> <ul> <li>rtdip-sdk</li> </ul>"},{"location":"sdk/examples/pipelines/deploy/Python-Delta-to-Delta/#components","title":"Components","text":"Name Description PythonDeltaSource Reads data from a Delta Table. PythonDeltaDestination Writes to a Delta table."},{"location":"sdk/examples/pipelines/deploy/Python-Delta-to-Delta/#example","title":"Example","text":"<p>Below is an example of how to read from and write to Delta Tables locally without the need for Spark</p> <pre><code>from rtdip_sdk.pipelines.sources.python.delta import PythonDeltaSource\nfrom rtdip_sdk.pipelines.destinations.python.delta import PythonDeltaDestination\n\nsource = PythonDeltaSource(\"{/path/to/source/table}\").read_batch()\n\ndestination = PythonDeltaDestination(source, \"{/path/to/destination/table}\", mode=\"append\").write_batch()\n</code></pre>"},{"location":"sdk/examples/pipelines/deploy/dagster/Fledge-Dagster-Pipeline-Databricks/","title":"Fledge Pipeline using Dagster and Databricks Connect","text":"<p>This article provides a guide on how to deploy a pipeline in dagster using the RTDIP SDK and Databricks Connect. This pipeline was tested on an M2 Macbook Pro using VS Code in a Python (3.10) environment.</p> <p>Note</p> <p>Reading from Eventhubs is currently not supported on Databricks Connect.</p>"},{"location":"sdk/examples/pipelines/deploy/dagster/Fledge-Dagster-Pipeline-Databricks/#prerequisites","title":"Prerequisites","text":"<p>Deployment using Databricks Connect requires:</p> <ul> <li> <p>a Databricks workspace</p> </li> <li> <p>a cluster in the same workspace</p> </li> <li> <p>a personal access token</p> </li> </ul> <p>Further information on Databricks requirements can be found here.</p> <p>This pipeline job requires the packages:</p> <ul> <li> <p>rtdip-sdk</p> </li> <li> <p>databricks-connect</p> </li> <li> <p>dagster</p> </li> </ul> <p>Dagster Installation</p> <p>For Mac users with an M1 or M2 chip, installation of dagster should be done as follows: <pre><code>pip install dagster dagster-webserver --find-links=https://github.com/dagster-io/build-grpcio/wiki/Wheels\n</code></pre></p>"},{"location":"sdk/examples/pipelines/deploy/dagster/Fledge-Dagster-Pipeline-Databricks/#components","title":"Components","text":"Name Description SparkDeltaSource Read data from a Delta table. BinaryToStringTransformer Converts a Spark DataFrame column from binary to string. FledgeOPCUAJsonToPCDMTransformer Converts a Spark DataFrame column containing a json string to the Process Control Data Model. SparkDeltaDestination Writes to a Delta table."},{"location":"sdk/examples/pipelines/deploy/dagster/Fledge-Dagster-Pipeline-Databricks/#authentication","title":"Authentication","text":"<p>For Databricks authentication, the following fields should be added to a configuration profile in your <code>.databrickscfg</code> file:</p> <pre><code>[PROFILE]\nhost = https://{workspace_instance}\ntoken = dapi...\ncluster_id = {cluster_id}\n</code></pre> <p>This profile should match the configurations in your <code>DatabricksSession</code> in the example below as it will be used by the Databricks extension in VS Code for authenticating your Databricks cluster.</p>"},{"location":"sdk/examples/pipelines/deploy/dagster/Fledge-Dagster-Pipeline-Databricks/#example","title":"Example","text":"<p>Below is an example of how to set up a pipeline to read Fledge data from a Delta table, transform it to RTDIP's PCDM model and write it to a Delta table.</p> <pre><code>from dagster import Definitions, ResourceDefinition, graph, op\nfrom databricks.connect import DatabricksSession\nfrom rtdip_sdk.pipelines.sources.spark.delta import SparkDeltaSource\nfrom rtdip_sdk.pipelines.transformers.spark.binary_to_string import BinaryToStringTransformer\nfrom rtdip_sdk.pipelines.transformers.spark.fledge_opcua_json_to_pcdm import FledgeOPCUAJsonToPCDMTransformer\nfrom rtdip_sdk.pipelines.destinations.spark.delta import SparkDeltaDestination\n\n# Databricks cluster configuration\ndatabricks_resource = ResourceDefinition.hardcoded_resource(\n                    DatabricksSession.builder.remote(\n                        host       = \"https://{workspace_instance_name}\",\n                        token      = \"{token}\",\n                        cluster_id = \"{cluster_id}\"\n                        ).getOrCreate()\n)\n\n# Pipeline\n@op(required_resource_keys={\"databricks\"})\ndef pipeline(context):\n    spark = context.resources.databricks\n    source = SparkDeltaSource(spark, {}, \"{path_to_table}\").read_batch()\n    transformer = BinaryToStringTransformer(source, \"{source_column_name}\", \"{target_column_name}\").transform()\n    transformer = FledgeOPCUAJsonToPCDMTransformer(transformer, \"{source_column_name}\").transform()\n    SparkDeltaDestination(transformer, {}, \"{path_to_table}\").write_batch()\n\n@graph\ndef fledge_pipeline():\n    pipeline()\n\nfledge_pipeline_job = fledge_pipeline.to_job(\n    resource_defs={ \n                    \"databricks\": databricks_resource\n                   }\n)\n\ndefs = Definitions(jobs=[fledge_pipeline_job])\n</code></pre>"},{"location":"sdk/examples/pipelines/deploy/dagster/Fledge-Dagster-Pipeline-Databricks/#deploy","title":"Deploy","text":"<p>The following command deploys the pipeline to dagster: <code>dagster dev -f &lt;path/to/file.py&gt;</code></p> <p>Using the link provided from the command above, click on Launchpad and hit run to run the pipeline.</p>"},{"location":"sdk/examples/pipelines/deploy/dagster/Fledge-Dagster-Pipeline-Local/","title":"Fledge Pipeline using Dagster","text":"<p>This article provides a guide on how to deploy a pipeline in dagster using the RTDIP SDK. This pipeline was tested on an M2 Macbook Pro using VS Code in a Python (3.10) environment.</p>"},{"location":"sdk/examples/pipelines/deploy/dagster/Fledge-Dagster-Pipeline-Local/#prerequisites","title":"Prerequisites","text":"<p>This pipeline job requires the packages:</p> <ul> <li> <p>rtdip-sdk</p> </li> <li> <p>dagster</p> </li> </ul> <p>Dagster Installation</p> <p>For Mac users with an M1 or M2 chip, installation of dagster should be done as follows: <pre><code>pip install dagster dagster-webserver --find-links=https://github.com/dagster-io/build-grpcio/wiki/Wheels\n</code></pre></p>"},{"location":"sdk/examples/pipelines/deploy/dagster/Fledge-Dagster-Pipeline-Local/#components","title":"Components","text":"Name Description SparkEventhubSource Read data from an Eventhub. BinaryToStringTransformer Converts a Spark DataFrame column from binary to string. FledgeOPCUAJsonToPCDMTransformer Converts a Spark DataFrame column containing a json string to the Process Control Data Model. SparkDeltaDestination Writes to a Delta table."},{"location":"sdk/examples/pipelines/deploy/dagster/Fledge-Dagster-Pipeline-Local/#example","title":"Example","text":"<p>Below is an example of how to set up a pipeline to read Fledge data from an Eventhub, transform it to RTDIP's PCDM model and write it to a Delta table on your machine.</p> <pre><code>import json\nfrom datetime import datetime as dt\nfrom dagster import Definitions, graph, op\nfrom dagster_pyspark.resources import pyspark_resource\nfrom rtdip_sdk.pipelines.sources.spark.eventhub import SparkEventhubSource\nfrom rtdip_sdk.pipelines.transformers.spark.binary_to_string import BinaryToStringTransformer\nfrom rtdip_sdk.pipelines.transformers.spark.fledge_opcua_json_to_pcdm import FledgeOPCUAJsonToPCDMTransformer\nfrom rtdip_sdk.pipelines.destinations.spark.delta import SparkDeltaDestination\n\n# PySpark cluster configuration\npackages = \"com.microsoft.azure:azure-eventhubs-spark_2.12:2.3.22,io.delta:delta-core_2.12:2.4.0\"\nmy_pyspark_resource = pyspark_resource.configured(\n    {\"spark_conf\": {\"spark.default.parallelism\": 1,\n                    \"spark.jars.packages\": packages,\n                    \"spark.sql.extensions\": \"io.delta.sql.DeltaSparkSessionExtension\", \n                    \"spark.sql.catalog.spark_catalog\": \"org.apache.spark.sql.delta.catalog.DeltaCatalog\"\n                    }\n    }\n)\n\n# EventHub configuration\neventhub_connection_string = \"{eventhub_connection_string}\"\neventhub_consumer_group = \"{eventhub_consumer_group}\"\n\nstartOffset = \"-1\"\nendTime = dt.now().strftime(\"%Y-%m-%dT%H:%M:%S.%fZ\")\n\nstartingEventPosition = {\n  \"offset\": startOffset,  \n  \"seqNo\": -1,            \n  \"enqueuedTime\": None,   \n  \"isInclusive\": True\n}\n\nendingEventPosition = {\n  \"offset\": None,           \n  \"seqNo\": -1,              \n  \"enqueuedTime\": endTime,\n  \"isInclusive\": True\n}\n\nehConf = {\n'eventhubs.connectionString' : eventhub_connection_string,\n'eventhubs.consumerGroup': eventhub_consumer_group,\n'eventhubs.startingPosition' : json.dumps(startingEventPosition),\n'eventhubs.endingPosition' : json.dumps(endingEventPosition),\n'maxEventsPerTrigger': 1000\n}\n\n# Pipeline\n@op(required_resource_keys={\"spark\"})\ndef pipeline(context):\n    spark = context.resources.pyspark.spark_session\n    source = SparkEventhubSource(spark, ehConf).read_batch()\n    transformer = BinaryToStringTransformer(source, \"{source_column_name}\", \"{target_column_name}\").transform()\n    transformer = FledgeOPCUAJsonToPCDMTransformer(transformer, \"{source_column_name}\").transform()\n    SparkDeltaDestination(transformer, {}, \"{path_to_table}\").write_batch()\n\n@graph\ndef fledge_pipeline():\n    pipeline()\n\nfledge_pipeline_job = fledge_pipeline.to_job(\n    resource_defs={\n                   \"spark\": my_pyspark_resource\n                   }\n)\n\ndefs = Definitions(jobs=[fledge_pipeline_job])\n</code></pre>"},{"location":"sdk/examples/pipelines/deploy/dagster/Fledge-Dagster-Pipeline-Local/#deploy","title":"Deploy","text":"<p>The following command deploys the pipeline to dagster: <code>dagster dev -f &lt;path/to/file.py&gt;</code></p> <p>Using the link provided from the command above, click on Launchpad and hit run to run the pipeline.</p>"},{"location":"sdk/examples/pipelines/deploy/databricks/MISODailyLoad-Batch-Pipeline-Databricks/","title":"MISO Pipeline using RTDIP and Databricks","text":"<p>This article provides a guide on how to deploy a MISO pipeline from a local file to a Databricks workflow using the RTDIP SDK and was tested on an M2 Macbook Pro using VS Code in a Conda (3.11) environment. RTDIP Pipeline Components provide Databricks with all the required Python packages and JARs to execute each component, this will automatically be set up during workflow creation.</p>"},{"location":"sdk/examples/pipelines/deploy/databricks/MISODailyLoad-Batch-Pipeline-Databricks/#prerequisites","title":"Prerequisites","text":"<p>This pipeline assumes you have a Databricks workspace and have followed the installation instructions as specified in the Getting Started section. In particular ensure you have installed the following:</p> <ul> <li> <p>RTDIP SDK</p> </li> <li> <p>Java</p> </li> </ul> <p>RTDIP SDK Installation</p> <p>Ensure you have installed the RTDIP SDK as follows: <pre><code>pip install \"rtdip-sdk[pipelines]\"\n</code></pre></p>"},{"location":"sdk/examples/pipelines/deploy/databricks/MISODailyLoad-Batch-Pipeline-Databricks/#components","title":"Components","text":"Name Description MISODailyLoadISOSource Read daily load data from MISO API. MISOToMDMTransformer Converts MISO Raw data into Meters Data Model. SparkDeltaDestination Writes to a Delta table. DatabricksSDKDeploy Deploys an RTDIP Pipeline to Databricks Workflows leveraging the Databricks SDK. DeltaTableOptimizeUtility Optimizes a Delta Table DeltaTableVacuumUtility Vacuums a Delta Table"},{"location":"sdk/examples/pipelines/deploy/databricks/MISODailyLoad-Batch-Pipeline-Databricks/#example","title":"Example","text":"<p>Below is an example of how to set up a pipeline job to read daily load data from the MISO API, transform it into the Meters Data Model and write it to a Delta table. <pre><code>from rtdip_sdk.pipelines.sources import MISODailyLoadISOSource\nfrom rtdip_sdk.pipelines.transformers import MISOToMDMTransformer\nfrom rtdip_sdk.pipelines.destinations import SparkDeltaDestination\n\ndef pipeline():   \n    source_df = MISODailyLoadISOSource(\n        spark = spark,\n        options = {\n        \"load_type\": \"actual\",\n        \"date\": \"20230520\",\n        }\n    ).read_batch()\n\n    transform_value_df = MISOToMDMTransformer(\n        spark=spark,\n        data=source_df,\n        output_type= \"usage\"\n    ).transform()\n\n    transform_meta_df = MISOToMDMTransformer(\n        spark=spark,\n        data=source_df,\n        output_type= \"meta\"\n    ).transform()\n\n    SparkDeltaDestination(\n        data=transform_value_df,\n        options={\n            \"partitionBy\":\"timestamp\"\n        },   \n        destination=\"miso_usage_data\" \n    ).write_batch()    \n\n    SparkDeltaDestination(\n        data=transform_meta_df,\n        options={\n            \"partitionBy\":\"timestamp\"\n        },   \n        destination=\"miso_meta_data\",\n        mode=\"overwrite\"\n    ).write_batch() \n\nif __name__ == \"__main__\":\n    pipeline()\n</code></pre></p>"},{"location":"sdk/examples/pipelines/deploy/databricks/MISODailyLoad-Batch-Pipeline-Databricks/#maintenance","title":"Maintenance","text":"<p>The RTDIP SDK can be used to maintain Delta tables in Databricks, an example of how to set up a maintenance job to optimize and vacuum the MISO tables written from the previous example is provided below. <pre><code>from rtdip_sdk.pipelines.utilities import DeltaTableOptimizeUtility, DeltaTableVacuumUtility\n\ndef maintenance():\n    TABLE_NAMES = [\n        \"{path.to.table.miso_usage_data}\",\n        \"{path.to.table.miso_meta_data}\"\n    ]\n\n    for table in TABLE_NAMES:\n\n        DeltaTableOptimizeUtility(\n            spark=spark, \n            table_name=table\n        ).execute()\n\n        DeltaTableVacuumUtility(\n            spark=spark,\n            table_name=table\n        ).execute()\n\nif __name__ == \"__main__\":\n    maintenance()\n</code></pre></p>"},{"location":"sdk/examples/pipelines/deploy/databricks/MISODailyLoad-Batch-Pipeline-Databricks/#deploy","title":"Deploy","text":"<p>Deployment to Databricks uses the Databricks SDK. Users have the option to control the job's configurations including the cluster and schedule. <pre><code>from rtdip_sdk.pipelines.deploy import DatabricksSDKDeploy, CreateJob, JobCluster, ClusterSpec, Task, NotebookTask, AutoScale, RuntimeEngine, DataSecurityMode, CronSchedule, Continuous, PauseStatus\nfrom rtdip_sdk.authentication.azure import DefaultAuth\n\ndef deploy():\n    credential = DefaultAuth().authenticate()\n    access_token = credential.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\n\n    DATABRICKS_WORKSPACE = \"{databricks-workspace-url}\"\n\n    # Create clusters\n    cluster_list = []\n    cluster_list.append(JobCluster(\n        job_cluster_key=\"pipeline-cluster\",\n        new_cluster=ClusterSpec(\n            node_type_id=\"Standard_E4ds_v5\",\n            autoscale=AutoScale(min_workers=1, max_workers=8),\n            spark_version=\"13.3.x-scala2.12\",\n            data_security_mode=DataSecurityMode.SINGLE_USER,\n            runtime_engine=RuntimeEngine.STANDARD\n        )\n    ))\n\n    # Create tasks\n    task_list = []\n    task_list.append(Task(\n        task_key=\"pipeline\",\n        job_cluster_key=\"pipeline-cluster\",\n        notebook_task=NotebookTask(\n            notebook_path=\"{path/to/pipeline.py}\"\n        )\n    ))\n\n    # Create a Databricks Job for the Task\n    job = CreateJob(\n        name=\"rtdip-miso-batch-pipeline-job\",\n        job_clusters=cluster_list,\n        tasks=task_list,\n        continuous=Continuous(pause_status=PauseStatus.UNPAUSED)\n    )\n\n    # Deploy to Databricks\n    databricks_pipeline_job = DatabricksSDKDeploy(databricks_job=job, host=DATABRICKS_WORKSPACE, token=access_token, workspace_directory=\"{path/to/databricks/workspace/directory}\")\n    databricks_pipeline_job.deploy()\n\n    cluster_list = []\n    cluster_list.append(JobCluster(\n        job_cluster_key=\"maintenance-cluster\",\n        new_cluster=ClusterSpec(\n            node_type_id=\"Standard_E4ds_v5\",\n            autoscale=AutoScale(min_workers=1, max_workers=3),\n            spark_version=\"13.3.x-scala2.12\",\n            data_security_mode=DataSecurityMode.SINGLE_USER,\n            runtime_engine=RuntimeEngine.PHOTON\n        )\n    ))\n\n    task_list = []\n    task_list.append(Task(\n        task_key=\"rtdip-miso-maintenance-task\",\n        job_cluster_key=\"maintenance-cluster\",\n        notebook_task=NotebookTask(\n            notebook_path=\"{path/to/maintenance.py}\"\n        )\n    ))\n\n    # Create a Databricks Job for the Task\n    job = CreateJob(\n        name=\"rtdip-miso-maintenance-job\",\n        job_clusters=cluster_list,\n        tasks=task_list,\n        schedule=CronSchedule(\n            quartz_cron_expression=\"4 * * * * ?\",\n            timezone_id=\"UTC\",\n            pause_status=PauseStatus.UNPAUSED\n        )\n    )\n\n    # Deploy to Databricks\n    databricks_pipeline_job = DatabricksSDKDeploy(databricks_job=job, host=DATABRICKS_WORKSPACE, token=access_token, workspace_directory=\"{path/to/databricks/workspace/directory}\")\n    databricks_pipeline_job.deploy()\n\nif __name__ == \"__main__\":\n    deploy()\n</code></pre></p>"},{"location":"sdk/examples/query/Circular-Average/","title":"Circular Average","text":"<p>Circular Average - A function that receives a dataframe of raw tag data and computes the circular mean for samples in a range, returning the results.</p>"},{"location":"sdk/examples/query/Circular-Average/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have installed the RTDIP SDK as specified in the Getting Started section.</p> <p>This example is using DefaultAuth() and DatabricksSQLConnection() to authenticate and connect. You can find other ways to authenticate here. The alternative built in connection methods are either by PYODBCSQLConnection(), TURBODBCSQLConnection() or SparkConnection().</p>"},{"location":"sdk/examples/query/Circular-Average/#parameters","title":"Parameters","text":"Name Type Description business_unit str Business unit of the data region str Region asset str Asset data_security_level str Level of data security data_type str Type of the data (float, integer, double, string) tag_names list List of tagname or tagnames [\"tag_1\", \"tag_2\"] start_date str Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz) end_date str End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz) time_interval_rate str The time interval rate (numeric input) time_interval_unit str The time interval unit (second, minute, day, hour) lower_bound int Lower boundary for the sample range upper_bound int Upper boundary for the sample range include_bad_data bool Include \"Bad\" data points with True or remove \"Bad\" data points with False"},{"location":"sdk/examples/query/Circular-Average/#example","title":"Example","text":"<pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import circular_average\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\nparameters = {\n    \"business_unit\": \"{business_unit}\",\n    \"region\": \"{region}\", \n    \"asset\": \"{asset_name}\", \n    \"data_security_level\": \"{security_level}\", \n    \"data_type\": \"float\",\n    \"tag_names\": [\"{tag_name_1}\", \"{tag_name_2}\"],\n    \"start_date\": \"2023-01-01\",\n    \"end_date\": \"2023-01-31\",\n    \"time_interval_rate\": \"15\",\n    \"time_interval_unit\": \"minute\",\n    \"lower_bound\": 0,\n    \"upper_bound\": 360,\n    \"include_bad_data\": True,\n}\nx = circular_average.get(connection, parameters)\nprint(x)\n</code></pre>"},{"location":"sdk/examples/query/Circular-Standard-Deviation/","title":"Circular Standard Deviation","text":"<p>Circular Standard Deviation - A function that receives a dataframe of raw tag data and computes the circular standard deviation for samples assumed to be in the range, returning the results.</p>"},{"location":"sdk/examples/query/Circular-Standard-Deviation/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have installed the RTDIP SDK as specified in the Getting Started section.</p> <p>This example is using DefaultAuth() and DatabricksSQLConnection() to authenticate and connect. You can find other ways to authenticate here. The alternative built in connection methods are either by PYODBCSQLConnection(), TURBODBCSQLConnection() or SparkConnection().</p>"},{"location":"sdk/examples/query/Circular-Standard-Deviation/#parameters","title":"Parameters","text":"Name Type Description business_unit str Business unit of the data region str Region asset str Asset data_security_level str Level of data security data_type str Type of the data (float, integer, double, string) tag_names list List of tagname or tagnames [\"tag_1\", \"tag_2\"] start_date str Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz) end_date str End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz) time_interval_rate str The time interval rate (numeric input) time_interval_unit str The time interval unit (second, minute, day, hour) lower_bound int Lower boundary for the sample range upper_bound int Upper boundary for the sample range include_bad_data bool Include \"Bad\" data points with True or remove \"Bad\" data points with False"},{"location":"sdk/examples/query/Circular-Standard-Deviation/#example","title":"Example","text":"<pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import circular_standard_deviation\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\nparameters = {\n    \"business_unit\": \"{business_unit}\",\n    \"region\": \"{region}\", \n    \"asset\": \"{asset_name}\", \n    \"data_security_level\": \"{security_level}\", \n    \"data_type\": \"float\",\n    \"tag_names\": [\"{tag_name_1}\", \"{tag_name_2}\"],\n    \"start_date\": \"2023-01-01\",\n    \"end_date\": \"2023-01-31\",\n    \"time_interval_rate\": \"15\",\n    \"time_interval_unit\": \"minute\",\n    \"lower_bound\": 0,\n    \"upper_bound\": 360,\n    \"include_bad_data\": True,\n}\nx = circular_standard_deviation.get(connection, parameters)\nprint(x)\n</code></pre>"},{"location":"sdk/examples/query/Interpolate/","title":"Interpolate","text":"<p>Interpolate - takes resampling one step further to estimate the values of unknown data points that fall between existing, known data points. In addition to the resampling parameters, interpolation also requires:</p> <p>Interpolation Method - Forward Fill, Backward Fill or Linear</p>"},{"location":"sdk/examples/query/Interpolate/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have installed the RTDIP SDK as specified in the Getting Started section.</p> <p>This example is using DefaultAuth() and DatabricksSQLConnection() to authenticate and connect. You can find other ways to authenticate here. The alternative built in connection methods are either by PYODBCSQLConnection(), TURBODBCSQLConnection() or SparkConnection().</p>"},{"location":"sdk/examples/query/Interpolate/#parameters","title":"Parameters","text":"Name Type Description business_unit str Business unit of the data region str Region asset str Asset data_security_level str Level of data security data_type str Type of the data (float, integer, double, string) tag_names list List of tagname or tagnames [\"tag_1\", \"tag_2\"] start_date str Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz) end_date str End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz) sample_rate int (deprecated) Please use time_interval_rate instead. See below. sample_unit str (deprecated) Please use time_interval_unit instead. See below. time_interval_rate str The time interval rate (numeric input) time_interval_unit str The time interval unit (second, minute, day, hour) agg_method str Aggregation Method (first, last, avg, min, max) interpolation_method str Interpolation method (forward_fill, backward_fill, linear) include_bad_data bool Include \"Bad\" data points with True or remove \"Bad\" data points with False"},{"location":"sdk/examples/query/Interpolate/#example","title":"Example","text":"<pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import interpolate\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\nparameters = {\n    \"business_unit\": \"{business_unit}\",\n    \"region\": \"{region}\", \n    \"asset\": \"{asset_name}\", \n    \"data_security_level\": \"{security_level}\", \n    \"data_type\": \"float\",\n    \"tag_names\": [\"{tag_name_1}\", \"{tag_name_2}\"],\n    \"start_date\": \"2023-01-01\",\n    \"end_date\": \"2023-01-31\",\n    \"time_interval_rate\": \"15\",\n    \"time_interval_unit\": \"minute\",\n    \"agg_method\": \"first\",\n    \"interpolation_method\": \"forward_fill\",\n    \"include_bad_data\": True,\n}\nx = interpolate.get(connection, parameters)\nprint(x)\n</code></pre>"},{"location":"sdk/examples/query/Interpolation-at-Time/","title":"Interpolation at Time","text":"<p>Interpolation at Time - works out the linear interpolation at a specific time based on the points before and after. This is achieved by providing the following parameter:</p> <p>Timestamps - A list of timestamp or timestamps</p>"},{"location":"sdk/examples/query/Interpolation-at-Time/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have installed the RTDIP SDK as specified in the Getting Started section.</p> <p>This example is using DefaultAuth() and DatabricksSQLConnection() to authenticate and connect. You can find other ways to authenticate here. The alternative built in connection methods are either by PYODBCSQLConnection(), TURBODBCSQLConnection() or SparkConnection().</p>"},{"location":"sdk/examples/query/Interpolation-at-Time/#parameters","title":"Parameters","text":"Name Type Description business_unit str Business unit of the data region str Region asset str Asset data_security_level str Level of data security data_type str Type of the data (float, integer, double, string) tag_names str List of tagname or tagnames [\"tag_1\", \"tag_2\"] timestamps list List of timestamp or timestamps in the format YYY-MM-DDTHH:MM:SS or YYY-MM-DDTHH:MM:SS+zz:zz where %z is the timezone. (Example +00:00 is the UTC timezone) window_length int Add longer window time in days for the start or end of specified date to cater for edge cases. include_bad_data bool Include \"Bad\" data points with True or remove \"Bad\" data points with False"},{"location":"sdk/examples/query/Interpolation-at-Time/#example","title":"Example","text":"<pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import interpolation_at_time\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\nparameters = {\n    \"business_unit\": \"{business_unit}\",\n    \"region\": \"{region}\", \n    \"asset\": \"{asset_name}\", \n    \"data_security_level\": \"{security_level}\", \n    \"data_type\": \"float\",\n    \"tag_names\": [\"{tag_name_1}\", \"{tag_name_2}\"],\n    \"timestamps\": [\"2023-01-01\", \"2023-01-02\"], \n    \"window_length\": 1,\n}\nx = interpolation_at_time.get(connection, parameters)\nprint(x)\n</code></pre>"},{"location":"sdk/examples/query/Metadata/","title":"Metadata","text":"<p>Metadata queries provide contextual information for time series measurements and include information such as names, descriptions and units of measure.</p>"},{"location":"sdk/examples/query/Metadata/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have installed the RTDIP SDK as specified in the Getting Started section.</p> <p>This example is using DefaultAuth() and DatabricksSQLConnection() to authenticate and connect. You can find other ways to authenticate here. The alternative built in connection methods are either by PYODBCSQLConnection(), TURBODBCSQLConnection() or SparkConnection().</p>"},{"location":"sdk/examples/query/Metadata/#parameters","title":"Parameters","text":"Name Type Description business_unit str Business unit region str Region asset str Asset data_security_level str Level of data security tag_names (optional, list) Either pass a list of tagname/tagnames [\"tag_1\", \"tag_2\"] or leave the list blank [] or leave the parameter out completely"},{"location":"sdk/examples/query/Metadata/#example","title":"Example","text":"<pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import metadata\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\nparameters = {\n    \"business_unit\": \"{business_unit}\",\n    \"region\": \"{region}\", \n    \"asset\": \"{asset_name}\", \n    \"data_security_level\": \"{security_level}\", \n    \"tag_names\": [\"{tag_name_1}\", \"{tag_name_2}\"],\n}\nx = metadata.get(connection, parameters)\nprint(x)\n</code></pre>"},{"location":"sdk/examples/query/Raw/","title":"Raw","text":"<p>Raw facilitates performing raw extracts of time series data, typically filtered by a Tag Name or Device Name and an event time.</p>"},{"location":"sdk/examples/query/Raw/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have installed the RTDIP SDK as specified in the Getting Started section.</p> <p>This example is using DefaultAuth() and DatabricksSQLConnection() to authenticate and connect. You can find other ways to authenticate here. The alternative built in connection methods are either by PYODBCSQLConnection(), TURBODBCSQLConnection() or SparkConnection().</p>"},{"location":"sdk/examples/query/Raw/#parameters","title":"Parameters","text":"Name Type Description business_unit str Business unit region str Region asset str Asset data_security_level str Level of data security data_type str Type of the data (float, integer, double, string) tag_names list List of tagname or tagnames [\"tag_1\", \"tag_2\"] start_date str Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz) end_date str End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz) include_bad_data bool Include \"Bad\" data points with True or remove \"Bad\" data points with False"},{"location":"sdk/examples/query/Raw/#example","title":"Example","text":"<pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import raw\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\nparameters = {\n    \"business_unit\": \"{business_unit}\",\n    \"region\": \"{region}\", \n    \"asset\": \"{asset_name}\", \n    \"data_security_level\": \"{security_level}\", \n    \"data_type\": \"float\",\n    \"tag_names\": [\"{tag_name_1}\", \"{tag_name_2}\"],\n    \"start_date\": \"2023-01-01\",\n    \"end_date\": \"2023-01-31\",\n    \"include_bad_data\": True,\n}\nx = raw.get(connection, parameters)\nprint(x)\n</code></pre>"},{"location":"sdk/examples/query/Resample/","title":"Resample","text":"<p>Resample enables changing the frequency of time series observations. This is achieved by providing the following parameters:</p> <p>Sample Rate - (deprecated) Sample Unit - (deprecated) Time Interval Rate - The time interval rate Time Interval Unit - The time interval unit (second, minute, day, hour) Aggregation Method - Aggregations including first, last, avg, min, max</p>"},{"location":"sdk/examples/query/Resample/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have installed the RTDIP SDK as specified in the Getting Started section.</p> <p>This example is using DefaultAuth() and DatabricksSQLConnection() to authenticate and connect. You can find other ways to authenticate here. The alternative built in connection methods are either by PYODBCSQLConnection(), TURBODBCSQLConnection() or SparkConnection().</p>"},{"location":"sdk/examples/query/Resample/#parameters","title":"Parameters","text":"Name Type Description business_unit str Business unit of the data region str Region asset str Asset data_security_level str Level of data security data_type str Type of the data (float, integer, double, string) tag_names list List of tagname or tagnames [\"tag_1\", \"tag_2\"] start_date str Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz) end_date str End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz) sample_rate int (deprecated) Please use time_interval_rate instead. See below. sample_unit str (deprecated) Please use time_interval_unit instead. See below. time_interval_rate str The time interval rate (numeric input) time_interval_unit str The time interval unit (second, minute, day, hour) agg_method str Aggregation Method (first, last, avg, min, max) include_bad_data bool Include \"Bad\" data points with True or remove \"Bad\" data points with False"},{"location":"sdk/examples/query/Resample/#example","title":"Example","text":"<pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import resample\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\nparameters = {\n    \"business_unit\": \"{business_unit}\",\n    \"region\": \"{region}\", \n    \"asset\": \"{asset_name}\", \n    \"data_security_level\": \"{security_level}\", \n    \"data_type\": \"float\",\n    \"tag_names\": [\"{tag_name_1}\", \"{tag_name_2}\"],\n    \"start_date\": \"2023-01-01\",\n    \"end_date\": \"2023-01-31\",\n    \"time_interval_rate\": \"15\",\n    \"time_interval_unit\": \"minute\",\n    \"agg_method\": \"first\",\n    \"include_bad_data\": True,\n}\nx = resample.get(connection, parameters)\nprint(x)\n</code></pre>"},{"location":"sdk/examples/query/Summary/","title":"Summary","text":"<p>Summary facilitates performing a summary of statisics of time series data, typically filtered by a Tag Name or Device Name and an event time.</p>"},{"location":"sdk/examples/query/Summary/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have installed the RTDIP SDK as specified in the Getting Started section.</p> <p>This example is using DefaultAuth() and DatabricksSQLConnection() to authenticate and connect. You can find other ways to authenticate here. The alternative built in connection methods are either by PYODBCSQLConnection(), TURBODBCSQLConnection() or SparkConnection().</p>"},{"location":"sdk/examples/query/Summary/#parameters","title":"Parameters","text":"Name Type Description business_unit str Business unit region str Region asset str Asset data_security_level str Level of data security data_type str Type of the data (float, integer, double, string) tag_names list List of tagname or tagnames [\"tag_1\", \"tag_2\"] start_date str Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz) end_date str End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz) include_bad_data bool Include \"Bad\" data points with True or remove \"Bad\" data points with False"},{"location":"sdk/examples/query/Summary/#example","title":"Example","text":"<pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import summary\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\nparameters = {\n    \"business_unit\": \"{business_unit}\",\n    \"region\": \"{region}\", \n    \"asset\": \"{asset_name}\", \n    \"data_security_level\": \"{security_level}\", \n    \"data_type\": \"float\",\n    \"tag_names\": [\"{tag_name_1}\", \"{tag_name_2}\"],\n    \"start_date\": \"2023-01-01\",\n    \"end_date\": \"2023-01-31\",\n    \"include_bad_data\": True,\n}\nx = summary.get(connection, parameters)\nprint(x)\n</code></pre>"},{"location":"sdk/examples/query/Time-Weighted-Average/","title":"Time Weighted Average","text":"<p>Time Weighted Averages provide an unbiased average when working with irregularly sampled data. The RTDIP SDK requires the following parameters to perform time weighted average queries:</p> <p>Window Size Mins - (deprecated) Time Interval Rate - The time interval rate Time Interval Unit - The time interval unit (second, minute, day, hour) Window Length - Adds a longer window time for the start or end of specified date to cater for edge cases Step - Data points with step \"enabled\" or \"disabled\". The options for step are \"true\", \"false\" or \"metadata\" as string types. For \"metadata\", the query requires that the TagName has a step column configured correctly in the meta data table</p>"},{"location":"sdk/examples/query/Time-Weighted-Average/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have installed the RTDIP SDK as specified in the Getting Started section.</p> <p>This example is using DefaultAuth() and DatabricksSQLConnection() to authenticate and connect. You can find other ways to authenticate here. The alternative built in connection methods are either by PYODBCSQLConnection(), TURBODBCSQLConnection() or SparkConnection().</p>"},{"location":"sdk/examples/query/Time-Weighted-Average/#parameters","title":"Parameters","text":"Name Type Description business_unit str Business unit region str Region asset str Asset data_security_level str Level of data security data_type str Type of the data (float, integer, double, string) tag_names list List of tagname or tagnames [\"tag_1\", \"tag_2\"] start_date str Start date (Either a utc date in the format YYYY-MM-DD or a utc datetime in the format YYYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz) end_date str End date (Either a utc date in the format YYYY-MM-DD or a utc datetime in the format YYYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz) window_size_mins int (deprecated) Window size in minutes. Please use time_interval_rate and time_interval_unit below instead time_interval_rate str The time interval rate (numeric input) time_interval_unit str The time interval unit (second, minute, day, hour) window_length int Add longer window time in days for the start or end of specified date to cater for edge cases include_bad_data bool Include \"Bad\" data points with True or remove \"Bad\" data points with False step str Data points with step \"enabled\" or \"disabled\". The options for step are \"true\", \"false\" or \"metadata\". \"metadata\" will retrieve the step value from the metadata table"},{"location":"sdk/examples/query/Time-Weighted-Average/#example","title":"Example","text":"<pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import time_weighted_average\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\nparameters = {\n    \"business_unit\": \"{business_unit}\",\n    \"region\": \"{region}\", \n    \"asset\": \"{asset_name}\", \n    \"data_security_level\": \"{security_level}\", \n    \"data_type\": \"float\",\n    \"tag_names\": [\"{tag_name_1}\", \"{tag_name_2}\"],\n    \"start_date\": \"2023-01-01\",\n    \"end_date\": \"2023-01-31\",\n    \"time_interval_rate\": \"15\",\n    \"time_interval_unit\": \"minute\",\n    \"window_length\": 1,\n    \"include_bad_data\": True,\n    \"step\": \"true\"\n}\nx = time_weighted_average.get(connection, parameters)\nprint(x)\n</code></pre>"},{"location":"sdk/pipelines/components/","title":"Pipeline Components","text":""},{"location":"sdk/pipelines/components/#overview","title":"Overview","text":"<p>The Real Time Data Ingestion Pipeline Framework supports the following component types:</p> <ul> <li>Sources - connectors to source systems</li> <li>Transformers - perform transformations on data, including data cleansing, data enrichment, data aggregation, data masking, data encryption, data decryption, data validation, data conversion, data normalization, data de-normalization, data partitioning etc</li> <li>Destinations - connectors to sink/destination systems </li> <li>Utilities - components that perform utility functions such as logging, error handling, data object creation, authentication, maintenance etc</li> <li>Secrets - components that facilitate accessing secret stores where sensitive information is stored such as passwords, connectiong strings, keys etc</li> </ul>"},{"location":"sdk/pipelines/components/#component-types","title":"Component Types","text":"Python Apache Spark Databricks <p>Component Types determine system requirements to execute the component:</p> <ul> <li>Python - components that are written in python and can be executed on a python runtime</li> <li>Pyspark - components that are written in pyspark can be executed on an open source Apache Spark runtime</li> <li>Databricks - components that require a Databricks runtime</li> </ul> <p>Note</p> <p>RTDIP are continuously adding more to this list. For detailed information on timelines, read this blog post and check back on this page regularly.</p>"},{"location":"sdk/pipelines/components/#sources","title":"Sources","text":"<p>Sources are components that connect to source systems and extract data from them. These will typically be real time data sources, but also support batch components as these are still important and necessary data souces of time series data in a number of circumstances in the real world.</p> Source Type Python Apache Spark Databricks Azure AWS Delta Delta Sharing Autoloader Eventhub Eventhub Kafka IoT Hub Kafka Kinesis MISO Daily Load ISO MISO Historical Load ISO PJM Daily Load ISO PJM Historical Load ISO CAISO Daily Load ISO CAISO Historical Load ISO ERCOT Daily Load ISO Weather Forecast API V1 Weather Forecast API V1 Multi <p>Note</p> <p>This list will dynamically change as the framework is further developed and new components are added.</p>"},{"location":"sdk/pipelines/components/#transformers","title":"Transformers","text":"<p>Transformers are components that perform transformations on data. These will target certain data models and common transformations that sources or destination components require to be performed on data before it can be ingested or consumed.</p> Transformer Type Python Apache Spark Databricks Azure AWS Binary To String OPC Publisher OPCUA Json To Process Control Data Model Fledge OPCUA Json To Process Control Data Model EdgeX OPCUA Json To Process Control Data Model SSIP PI Binary Files To Process Control Data Model SSIP PI Binary JSON To Process Control Data Model MISO To Meters Data Model Pandas to PySpark DataFrame Conversion PySpark to Pandas DataFrame Conversion MISO To Meters Data Model Raw Forecast to Weather Data Model PJM To Meters Data Model ERCOT To Meters Data Model <p>Note</p> <p>This list will dynamically change as the framework is further developed and new components are added.</p>"},{"location":"sdk/pipelines/components/#destinations","title":"Destinations","text":"<p>Destinations are components that connect to sink/destination systems and write data to them. </p> Destination Type Python Apache Spark Databricks Azure AWS Delta Delta Merge Eventhub Kakfa Kinesis Rest API Process Control Data Model To Delta Process Control Data Model Latest Values To Delta EVM <p>Note</p> <p>This list will dynamically change as the framework is further developed and new components are added.</p>"},{"location":"sdk/pipelines/components/#utilities","title":"Utilities","text":"<p>Utilities are components that perform utility functions such as logging, error handling, data object creation, authentication, maintenance and are normally components that can be executed as part of a pipeline or standalone.</p> Utility Type Python Apache Spark Databricks Azure AWS Spark Session Spark Configuration Delta Table Create Delta Table Optimize Delta Table Vacuum AWS S3 Bucket Policy ADLS Gen 2 ACLs Azure Autoloader Resources Spark ADLS Gen 2 Service Principal Connect <p>Note</p> <p>This list will dynamically change as the framework is further developed and new components are added.</p>"},{"location":"sdk/pipelines/components/#secrets","title":"Secrets","text":"<p>Secrets are components that perform functions to interact with secret stores to manage sensitive information such as passwords, keys and certificates.</p> Secret Type Python Apache Spark Databricks Azure AWS Databricks Secret Scopes Hashicorp Vault Azure Key Vault <p>Note</p> <p>This list will dynamically change as the framework is further developed and new components are added.</p>"},{"location":"sdk/pipelines/components/#conclusion","title":"Conclusion","text":"<p>Components can be used to build RTDIP Pipelines which is described in more detail here.</p>"},{"location":"sdk/pipelines/framework/","title":"RTDIP Ingestion Pipeline Framework","text":"<p>RTDIP has been built to simplify ingesting and querying time series data. The RTDIP Ingestion Pipeline Framework creates streaming and batch ingestion pipelines according to requirements of the source of the data and needs of the data consumer. RTDIP Pipelines focuses on the ingestion of data into the platform.</p>"},{"location":"sdk/pipelines/framework/#prerequisites","title":"Prerequisites","text":"<p>Ensure that you have followed the installation instructions as specified in the Getting Started section and follow the steps which highlight the installation requirements for Pipelines. In particular:</p> <ol> <li>RTDIP SDK Installation</li> <li>Java - If your pipeline steps utilize pyspark then Java must be installed.</li> </ol> <p>RTDIP SDK installation</p> <p>Ensure you have installed the RTDIP SDK, as a minimum, as follows: <pre><code>pip install \"rtdip-sdk[pipelines]\"\n</code></pre></p> <p>For all installation options please see the RTDIP SDK installation instructions.</p>"},{"location":"sdk/pipelines/framework/#overview","title":"Overview","text":"<p>The goal of the RTDIP Ingestion Pipeline framework is to:</p> <ol> <li>Support python and pyspark to build pipeline components</li> <li>Enable execution of sources, transformers, destinations and utilities components in a framework that can execute them in a defined order</li> <li>Create modular components that can be leveraged as a step in a pipeline task using Object Oriented Programming techniques included Interfaces and Implementations per component type</li> <li>Deploy pipelines to popular orchestration engines</li> <li>Ensure pipelines can be constructed and executed using the RTDIP SDK and rest APIs</li> </ol>"},{"location":"sdk/pipelines/framework/#jobs","title":"Jobs","text":"<p>The RTDIP Data Ingestion Pipeline Framework follow sthe typical convention of a job that users will be familiar with if they have used orchestration engines such as Apache Airflow or Databricks Workflows.</p> <p>A pipline job consists of the following components:</p> <pre><code>erDiagram\n  JOB ||--|{ TASK : contains\n  TASK ||--|{ STEP : contains\n  JOB {\n    string name\n    string description\n    list task_list\n  }\n  TASK {\n    string name\n    string description\n    string depends_on_task\n    list step_list\n    bool batch_task\n  }\n  STEP {\n    string name\n    string description\n    list depends_on_step\n    list provides_output_to_step\n    class component\n    dict component_parameters\n  }</code></pre> <p>As per the above, a pipeline job consists of a list of tasks. Each task consists of a list of steps. Each step consists of a component and a set of parameters that are passed to the component. Dependency Injection will ensure that each component is instantiated with the correct parameters. </p> <p>More Information about Pipeline Jobs can be found here.</p>"},{"location":"sdk/pipelines/framework/#runtime-environments","title":"Runtime Environments","text":"Python Apache Spark Databricks Delta Live Tables <p>Note</p> <p>RTDIP are continuously adding more to this list. For detailed information on timelines, read this blog post and check back on this page regularly.</p> <p>Pipelines can run in multiple environment types. These include:</p> <ul> <li>Python: Components written in python and executed on a python runtime</li> <li>Pyspark: Components written in pyspark and executed on an open source Apache Spark runtime</li> <li>Databricks: Components written in pyspark and executed on a Databricks runtime</li> <li>Delta Live Tables: Components written in pyspark and executed on a Databricks Delta Live Tables runtime</li> </ul> <p>Runtimes will take precedence depending on the list of components in a pipeline task.</p> <ul> <li>Pipelines with at least one Databricks or DLT component will be executed in a Databricks environment</li> <li>Pipelines with at least one Pyspark component will be executed in a Pyspark environment</li> <li>Pipelines with only Python components will be executed in a Python environment</li> </ul>"},{"location":"sdk/pipelines/framework/#conclusion","title":"Conclusion","text":"<p>Find out more about the components that can be used by the RTDIP Ingestion Pipeline Framework here.</p>"},{"location":"sdk/pipelines/jobs/","title":"Jobs","text":"<p>In a production environment, pipelines will be run as jobs that are either batch jobs executed on a schedule or a streaming job executed to be run continuously. </p>"},{"location":"sdk/pipelines/jobs/#build-a-pipeline","title":"Build a Pipeline","text":""},{"location":"sdk/pipelines/jobs/#prerequisites","title":"Prerequisites","text":"<p>Ensure that you have followed the installation instructions as specified in the Getting Started section and follow the steps which highlight the installation requirements for Pipelines. In particular:</p> <ol> <li>RTDIP SDK Installation</li> <li>Java - If your pipeline steps utilize pyspark then Java must be installed.</li> </ol> <p>RTDIP SDK installation</p> <p>Ensure you have installed the RTDIP SDK, as a minimum, as follows: <pre><code>pip install \"rtdip-sdk[pipelines]\"\n</code></pre></p> <p>For all installation options please see the RTDIP SDK installation instructions.</p>"},{"location":"sdk/pipelines/jobs/#import","title":"Import","text":"<p>Import the required components of a Pipeline Job.</p> <pre><code>from rtdip_sdk.pipelines.execute import PipelineJob, PipelineStep, PipelineTask\nfrom rtdip_sdk.pipelines.sources import SparkEventhubSource\nfrom rtdip_sdk.pipelines.transformers import BinaryToStringTransformer\nfrom rtdip_sdk.pipelines.destinations import SparkDeltaDestination\nfrom rtdip_sdk.pipelines.secrets import PipelineSecret, DatabricksSecrets\nimport json\n</code></pre>"},{"location":"sdk/pipelines/jobs/#steps","title":"Steps","text":"<p>Pipeline steps are constructed from components and added to a Pipeline task as a list. Each component is created as a <code>PipelineStep</code> and populated with the following information.</p> Parameter Description Requirements Name Each component requires a unique name that also facilitates dependencies between each component Contains only letters, numbers and underscores Description A brief description of each component Will populate certain components of a runtime such as Delta Live Tables Component The component Class Populate with the Class Name Component Parameters Configures the component with specific information, such as connection information and component specific settings Use Pipeline Secrets for sensitive Information Depends On Step Specifies any component names that must be executed prior to this component A python list of component names Provides Output To Step Specifies any component names that require this component's output as an input A python list of component names <pre><code>step_list = []\n\n# read step\neventhub_configuration = {\n    \"eventhubs.connectionString\": PipelineSecret(type=DatabricksSecrets, vault=\"test_vault\", key=\"test_key\"),\n    \"eventhubs.consumerGroup\": \"$Default\",\n    \"eventhubs.startingPosition\": json.dumps({\"offset\": \"0\", \"seqNo\": -1, \"enqueuedTime\": None, \"isInclusive\": True})\n}    \nstep_list.append(PipelineStep(\n    name=\"test_step1\",\n    description=\"test_step1\",\n    component=SparkEventhubSource,\n    component_parameters={\"options\": eventhub_configuration},\n    provide_output_to_step=[\"test_step2\"]\n))\n\n# transform step\nstep_list.append(PipelineStep(\n    name=\"test_step2\",\n    description=\"test_step2\",\n    component=BinaryToStringTransformer,\n    component_parameters={\n        \"source_column_name\": \"body\",\n        \"target_column_name\": \"body\"\n    },\n    depends_on_step=[\"test_step1\"],\n    provide_output_to_step=[\"test_step3\"]\n))\n\n# write step\nstep_list.append(PipelineStep(\n    name=\"test_step3\",\n    description=\"test_step3\",\n    component=SparkDeltaDestination,\n    component_parameters={\n        \"destination\": \"test_table\",\n        \"options\": {},\n        \"mode\": \"overwrite\"    \n    },\n    depends_on_step=[\"test_step2\"]\n))\n</code></pre>"},{"location":"sdk/pipelines/jobs/#tasks","title":"Tasks","text":"<p>Tasks contain a list of steps. Each task is created as a <code>PipelineTask</code> and populated with the following information.</p> Parameter Description Requirements Name Each task requires a unique name Contains only letters, numbers and underscores Description A brief description of the task Will populate certain components of a runtime such as Delta Live Tables Step List A python list of steps that are to be executed by the task A list of step names that contain only letters, numbers and underscores Batch Task The task should be executed as a batch task Optional, defaults to False <pre><code>task = PipelineTask(\n    name=\"test_task\",\n    description=\"test_task\",\n    step_list=step_list,\n    batch_task=True\n)\n</code></pre>"},{"location":"sdk/pipelines/jobs/#jobs_1","title":"Jobs","text":"<p>Jobs contain a list of tasks. A job is created as a <code>PipelineJob</code> and populated with the following information.</p> Parameter Description Requirements Name The Job requires a unique name Contains only letters, numbers and underscores Description A brief description of the job Will populate certain components of a runtime such as Delta Live Tables Version Enables version control of the task for certain environments Follow semantic versioning Task List A python list of tasks that are to be executed by the  job A list of task names that contain only letters, numbers and underscores <pre><code>pipeline_job = PipelineJob(\n    name=\"test_job\",\n    description=\"test_job\", \n    version=\"0.0.1\",\n    task_list=[task]\n)\n</code></pre>"},{"location":"sdk/pipelines/jobs/#execute","title":"Execute","text":"<p>Pipeline Jobs can be executed directly if the run environment where the code has been written facilitates it. To do so, the above Pipeline Job can be executed as follows:</p> <p>Pyspark Installation</p> <p>Ensure you have Java installed in your environment and you have installed pyspark using the below command: <pre><code>pip install \"rtdip-sdk[pipelines,pyspark]\"\n</code></pre></p> <pre><code>from rtdip_sdk.pipelines.execute import PipelineJobExecute\n\npipeline = PipelineJobExecute(pipeline_job)\n\nresult = pipeline.run()\n</code></pre>"},{"location":"sdk/pipelines/jobs/#conclusion","title":"Conclusion","text":"<p>The above sets out how a Pipeline Job can be constructed and executed. Most pipelines, however, will be exevcuted by orchestration engines. See the Deploy section for more information above how Pipeline Jobs can be deployed and executed in this way.</p>"},{"location":"sdk/pipelines/deploy/apache-airflow/","title":"Apache Airflow","text":""},{"location":"sdk/pipelines/deploy/apache-airflow/#databricks-provider","title":"Databricks Provider","text":"<p>Apache Airflow can orchestrate an RTDIP Pipeline that has been deployed as a Databricks Job. For further information on how to deploy an RTDIP Pipeline as a Databricks Job, please see here. </p> <p>Databricks has also provided more information about running Databricks jobs from Apache Airflow here.</p>"},{"location":"sdk/pipelines/deploy/apache-airflow/#prerequisites","title":"Prerequisites","text":"<ol> <li>An Apache Airflow instance must be running.</li> <li>Authentication between Apache Airflow and Databricks must be configured.</li> <li>The python packages <code>apache-airflow</code> and <code>apache-airflow-providers-databricks</code> must be installed.</li> <li>You have created an RTDIP Pipeline and deployed it to Databricks.</li> </ol>"},{"location":"sdk/pipelines/deploy/apache-airflow/#example","title":"Example","text":"<p>The <code>JOB ID</code> in the example below can be obtained from the Databricks Job.</p> <pre><code>from airflow import DAG\nfrom airflow.providers.databricks.operators.databricks import DatabricksRunNowOperator\nfrom airflow.utils.dates import days_ago\n\ndefault_args = {\n  'owner': 'airflow'\n}\n\nwith DAG('databricks_dag',\n  start_date = days_ago(2),\n  schedule_interval = None,\n  default_args = default_args\n  ) as dag:\n\n  opr_run_now = DatabricksRunNowOperator(\n    task_id = 'run_now',\n    databricks_conn_id = 'databricks_default',\n    job_id = JOB_ID\n  )\n</code></pre>"},{"location":"sdk/pipelines/deploy/databricks/","title":"Databricks Workflows","text":"<p>Deploying to Databricks is simplified using the RTDIP SDK as this method of deployment will handle the setup of the libraries and spark configuration directly from the components being used in your pipeline.</p>"},{"location":"sdk/pipelines/deploy/databricks/#prerequisites","title":"Prerequisites","text":"<ul> <li>This deployment method expects to deploy a local file to Databricks Workflows</li> </ul>"},{"location":"sdk/pipelines/deploy/databricks/#import","title":"Import","text":"<pre><code>from rtdip_sdk.pipelines.deploy import DatabricksSDKDeploy, CreateJob, JobCluster, ClusterSpec, Task, NotebookTask, ComputeSpecKind, AutoScale, RuntimeEngine, DataSecurityMode\n</code></pre>"},{"location":"sdk/pipelines/deploy/databricks/#authentication","title":"Authentication","text":"Azure Active DirectoryDatabricks <p>Refer to the Azure Active Directory documentation for further options to perform Azure AD authentication, such as Service Principal authentication using certificates or secrets. Below is an example of performing default authentication that retrieves a token for Azure Databricks. </p> <p>Also refer to the Code Reference for further technical information.</p> <pre><code>from rtdip_sdk.authentication import authenticate as auth\n\nauthentication = auth.DefaultAuth().authenticate()\naccess_token = authentication.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\n</code></pre> <p>Note</p> <p>If you are experiencing any trouble authenticating please see Troubleshooting - Authentication</p> <p>Refer to the Databricks documentation for further information about generating a Databricks PAT Token. Below is an example of performing default authentication that retrieves a token for a Databricks Workspace. </p> <p>Provide your <code>dbapi.....</code> token to the <code>access_token</code> in the examples below.</p> <pre><code>access_token = \"dbapi..........\"\n</code></pre>"},{"location":"sdk/pipelines/deploy/databricks/#deploy","title":"Deploy","text":"<p>Deployments to Databricks are done using the Databricks SDK. The Databricks SDK enables users to control exactly how they deploy their RTDIP Pipelines to Databricks.</p> <p>Any of the Classes below can be imported from the following location:</p> <pre><code>from rtdip_sdk.pipelines.deploy import *\n</code></pre> <p>Parameters for a Databricks Job can be managed using the following Classes:</p> Class Description ClusterSpec Provides Parameters for setting up a Databricks Cluster JobCluster Sets up a Jobs Cluster as defined by the provided <code>DatabricksCluster</code> Task Defines the setup of the Task at the Databricks Task level including Task specific Clusters, Libraries, Schedules, Notifications and Timeouts CreateJob Defines the setup at the Job level including Clusters, Libraries, Schedules, Notifications, Access Controls, Timeouts and Tags NotebookTask Provides the Notebook information to the <code>Task</code> DatabricksSDKDeploy Leverages the Databricks SDK to deploy the job to Databricks Workflows <p>Note</p> <p>All classes for deployment are available from the Databricks SDK and can be accessed using <code>from rtdip_sdk.pipelines.deploy import</code> and choosing the classes you need for your Databricks deployment</p>"},{"location":"sdk/pipelines/deploy/databricks/#example","title":"Example","text":"<p>A simple example of deploying an RTDIP Pipeline Job to an Azure Databricks Job is below.</p> <pre><code>databricks_host_name = \"{databricks-host-url}\" #Replace with your databricks workspace url\n\n# Setup a Cluster for the Databricks Job\ncluster_list = []\ncluster_list.append(JobCluster(\n    job_cluster_key=\"test_cluster\",\n    new_cluster=ClusterSpec(\n        node_type_id=\"Standard_E4ds_v5\",\n        autoscale=AutoScale(min_workers=1, max_workers=3),\n        spark_version=\"13.2.x-scala2.12\",\n        data_security_mode=DataSecurityMode.SINGLE_USER,\n        runtime_engine=RuntimeEngine.PHOTON\n    )\n))\n\n# Define a Notebook Task for the Databricks Job\ntask_list = []\ntask_list.append(Task(\n    task_key=\"test_task\",\n    job_cluster_key=\"test_cluster\",\n    notebook_task=NotebookTask(\n        notebook_path=\"/directory/to/pipeline.py\"\n    )\n))\n\n# Create a Databricks Job for the Task\njob = CreateJob(\n    name=\"test_job_rtdip\",\n    job_clusters=cluster_list,\n    tasks=task_list\n)\n\n# Deploy to Databricks\ndatabricks_job = DatabricksSDKDeploy(databricks_job=job, host=databricks_host_name, token=access_token)\n\ndeploy_result = databricks_job.deploy()\n</code></pre>"},{"location":"sdk/pipelines/deploy/databricks/#launch","title":"Launch","text":"<p>Once a job is deployed to Databricks, it can be executed immediately using the following code.</p> <pre><code># Run/Launch the Job in Databricks\nlaunch_result = databricks_job.launch()\n</code></pre>"},{"location":"sdk/queries/connectors/","title":"Connectors","text":"<p>RTDIP SDK provides functionality to connect to and query its data using connectors. Below is a list of the available connectors.</p>"},{"location":"sdk/queries/connectors/#odbc","title":"ODBC","text":""},{"location":"sdk/queries/connectors/#databricks-sql-connector","title":"Databricks SQL Connector","text":"<p>Enables connectivity to Databricks using the Databricks SQL Connector which does not require any ODBC installation. </p> <p>For more information refer to this documentation and for the specific implementation within the RTDIP SDK, refer to this link</p> <pre><code>from rtdip_sdk.connectors import DatabricksSQLConnection\n\nserver_hostname = \"server_hostname\"\nhttp_path = \"http_path\"\naccess_token = \"token\"\n\nconnection = DatabricksSQLConnection(server_hostname, http_path, access_token)\n</code></pre> <p>Replace server_hostname, http_path and access_token with your own information.</p>"},{"location":"sdk/queries/connectors/#pyodbc-sql-connector","title":"PYODBC SQL Connector","text":"<p>PYDOBC is a popular python package for querying data using ODBC. Refer to their documentation for more information about pyodbc, how to install it and how you can leverage it in your code.</p> <p>Warning</p> <p>The RTDIP SDK does not specify <code>pyodbc</code> as one of its package dependencies. It will need to be installed into your environment separately.</p> <p>View information about how pyodbc is implemented in the RTDIP SDK here.</p> <pre><code>from rtdip_sdk.connectors import PYODBCSQLConnection\n\nserver_hostname = \"server_hostname\"\nhttp_path = \"http_path\"\naccess_token = \"token\"\ndriver_path = \"/Library/simba/spark/lib/libsparkodbc_sbu.dylib\"\n\nconnection = PYODBCSQLConnection(driver_path, sever_hostname, http_path, access_token)\n</code></pre> <p>Replace server_hostname, http_path and access_token with your own information.</p>"},{"location":"sdk/queries/connectors/#turbodbc-sql-connector","title":"TURBODBC SQL Connector","text":"<p>Turbodbc is a powerful python ODBC package that has advanced options for querying performance. Find out more about installing it on your operation system and what Turbodbc can do here and refer to this documentation for more information about how it is implemented in the RTDIP SDK.</p> <p>Warning</p> <p>The RTDIP SDK does not specify <code>turbodbc</code> as one of its package dependencies. It will need to be installed into your environment separately.</p> <pre><code>from rtdip_sdk.connectors import TURBODBCSQLConnection\n\nserver_hostname = \"server_hostname\"\nhttp_path = \"http_path\"\naccess_token = \"token\"\n\nconnection = TURBODBCSQLConnection(server_hostname, http_path, access_token)\n</code></pre> <p>Replace server_hostname, http_path and access_token with your own information.</p>"},{"location":"sdk/queries/connectors/#spark","title":"Spark","text":""},{"location":"sdk/queries/connectors/#spark-connector","title":"Spark Connector","text":"<p>The Spark Connector enables querying of data using a Spark Session. This is useful for querying local instances of Spark or Delta. However, the most useful application of this connector is to leverage Spark Connect to enable connecting to a remote Spark Cluster to provide the compute for the query being run from a local machine.</p> <pre><code>from rtdip_sdk.connectors import SparkConnection\n\nspark_server = \"spark_server\"\naccess_token = \"my_token\"\n\nspark_remote = \"sc://{}:443;token={}\".format(spark_server, access_token)\nconnection = SparkConnection(spark_remote=spark_remote)\n</code></pre> <p>Replace the access_token with your own information.</p>"},{"location":"sdk/queries/connectors/#llms","title":"LLMs","text":""},{"location":"sdk/queries/connectors/#chat-open-ai-databricks-connector","title":"Chat Open AI Databricks Connector","text":"<p>The Chat Open AI Databricks Connector enables querying of Databricks data using Chat GPT. </p> <p>Warning</p> <p>This is experimental and you will likely experience variable responses to your questions depending on the complexity of the data you use in this setup. Start small, with only a 2 - 3 tables before scaling up.</p> <pre><code>from rtdip_sdk.connectors import ChatOpenAIDatabricksConnection\n\nagent = ChatOpenAIDatabricksConnection(\n    catalog=\"&lt;databricks catalog&gt;\", \n    schema=\"&lt;databricks schema&gt;\", \n    server_hostname=\"&lt;databricks host name&gt;\",                   \n    http_path=\"&lt;databricks http path&gt;\",                         \n    access_token=\"&lt;Azure AD token or databricks PAT token&gt;\",\n    openai_api_key=\"&lt;Open AI API key&gt;\",\n    openai_model = \"gpt-4\",                                     \n    sample_rows_in_table_info = 5, \n    verbose_logging = True\n)\n\nresponse = agent.run(\"What was the average actual power generated by Turbine 1 at ACME Wind Farm on 6 May?\")\nprint(response)\n</code></pre> <p>Some notes on the above:</p> <ul> <li><code>server_hostname</code> and <code>http_path</code> can be obtained from your Databricks SQL Warehouse or Databricks Cluster.</li> <li><code>access_token</code> can be either a Databricks PAT Token or Azure AD Token. To obtain an Azure AD token, please refer to this documentation</li> <li><code>open_ai_model</code> defaults to <code>gpt-4</code> but is not easily available at the time of writing. Alternatively, the <code>gpt-3.5-turbo-16k-0613</code> has worked well in our tests</li> <li><code>sample_rows_in_table_info</code> limits the number of rows queried in a table when the SQL Database Agent is looking context in the data. Be careful to not increase this too much as its then possible to exceed token limits on the gpt models</li> </ul>"},{"location":"sdk/queries/functions/","title":"Functions","text":"<p>The RTDIP SDK enables users to perform complex queries, including aggregation on datasets within the Platform. Please find below the various types of queries available for specific dataset types. These SDK Functions are also supported by the RTDIP API Docker Image.</p>"},{"location":"sdk/queries/functions/#time-series-events","title":"Time Series Events","text":""},{"location":"sdk/queries/functions/#raw","title":"Raw","text":"<p>Raw facilitates performing raw extracts of time series data, typically filtered by a Tag Name or Device Name and an event time.</p>"},{"location":"sdk/queries/functions/#latest","title":"Latest","text":"<p>Latest queries provides the latest event values. The RTDIP SDK requires the following parameters to retrieve the latest event values: - TagNames - A list of tag names</p>"},{"location":"sdk/queries/functions/#resample","title":"Resample","text":"<p>Resample enables changing the frequency of time series observations. This is achieved by providing the following parameters:</p> <ul> <li>Sample Rate - (deprecated)</li> <li>Sample Unit - (deprecated)</li> <li>Time Interval Rate - The time interval rate</li> <li>Time Interval Unit - The time interval unit (second, minute, day, hour)</li> <li>Aggregation Method - Aggregations including first, last, avg, min, max</li> </ul> <p>Note</p> <p>Sample Rate and Sample Unit parameters are deprecated and will be removed in v1.0.0. Please use Time Interval Rate and Time Interval Unit instead.</p>"},{"location":"sdk/queries/functions/#interpolate","title":"Interpolate","text":"<p>Interpolate - takes resampling one step further to estimate the values of unknown data points that fall between existing, known data points. In addition to the resampling parameters, interpolation also requires:</p> <ul> <li>Interpolation Method - Forward Fill, Backward Fill or Linear</li> </ul>"},{"location":"sdk/queries/functions/#interpolation-at-time","title":"Interpolation at Time","text":"<p>Interpolation at Time - works out the linear interpolation at a specific time based on the points before and after. This is achieved by providing the following parameter:</p> <ul> <li>Timestamps - A list of timestamp or timestamps</li> </ul>"},{"location":"sdk/queries/functions/#time-weighted-averages","title":"Time Weighted Averages","text":"<p>Time Weighted Averages provide an unbiased average when working with irregularly sampled data. The RTDIP SDK requires the following parameters to perform time weighted average queries:</p> <ul> <li>Window Size Mins - (deprecated)</li> <li>Time Interval Rate - The time interval rate</li> <li>Time Interval Unit - The time interval unit (second, minute, day, hour)</li> <li>Window Length - Adds a longer window time for the start or end of specified date to cater for edge cases</li> <li>Step - Data points with step \"enabled\" or \"disabled\". The options for step are \"true\", \"false\" or \"metadata\" as string types. For \"metadata\", the query requires that the TagName has a step column configured correctly in the meta data table</li> </ul> <p>Note</p> <p>Window Size Mins is deprecated and will be removed in v1.0.0. Please use Time Interval Rate and Time Interval Unit instead.</p>"},{"location":"sdk/queries/functions/#circular-averages","title":"Circular Averages","text":"<p>Circular Averages computes the circular average for samples in a range. The RTDIP SDK requires the following parameters to perform circular average queries:</p> <ul> <li>Time Interval Rate - The time interval rate</li> <li>Time Interval Unit - The time interval unit (second, minute, day, hour)</li> <li>Lower Bound - The lower boundary for the sample range</li> <li>Upper Bound - The upper boundary for the sample range</li> </ul>"},{"location":"sdk/queries/functions/#circular-standard-deviations","title":"Circular Standard Deviations","text":"<p>Circular Standard Deviations computes the circular standard deviations for samples assumed to be in the range. The RTDIP SDK requires the following parameters to perform circular average queries:</p> <ul> <li>Time Interval Rate - The time interval rate</li> <li>Time Interval Unit - The time interval unit (second, minute, day, hour)</li> <li>Lower Bound - The lower boundary for the sample range</li> <li>Upper Bound - The upper boundary for the sample range</li> </ul>"},{"location":"sdk/queries/functions/#time-series-metadata","title":"Time Series Metadata","text":""},{"location":"sdk/queries/functions/#metadata","title":"Metadata","text":"<p>Metadata queries provide contextual information for time series measurements and include information such as names, descriptions and units of measure.</p> <p>Note</p> <p>RTDIP are continuously adding more to this list so check back regularly.</p>"},{"location":"sdk/queries/functions/#query-examples","title":"Query Examples","text":"<p>For examples of how to use the RTDIP functions, click the following links:</p> <ul> <li> <p>Raw</p> </li> <li> <p>Resample</p> </li> <li> <p>Interpolate</p> </li> <li> <p>Interpolation at Time</p> </li> <li> <p>Time Weighted Averages</p> </li> <li> <p>Circular Averages</p> </li> <li> <p>Circular Standard Deviations</p> </li> <li> <p>Metadata</p> </li> </ul>"},{"location":"sdk/queries/databricks/databricks-sql/","title":"Query Databricks SQL using the RTDIP SDK","text":"<p>This article provides a guide on how to use RTDIP SDK to query data via Databricks SQL. Before getting started, ensure you have installed the RTDIP Python Package and check the RTDIP Installation Page for all the required prerequisites.</p>"},{"location":"sdk/queries/databricks/databricks-sql/#how-to-use-rtdip-sdk-with-databricks-sql","title":"How to use RTDIP SDK with Databricks SQL","text":"<p>The RTDIP SDK has rich support of querying data using Databricks SQL, such as allowing the user to authenticate, connect and/or use the most commonly requested methods for manipulating time series data accessible via Databricks SQL.</p>"},{"location":"sdk/queries/databricks/databricks-sql/#authentication","title":"Authentication","text":"Azure Active DirectoryDatabricks <p>Refer to the Azure Active Directory documentation for further options to perform Azure AD authentication, such as Service Principal authentication using certificates or secrets. Below is an example of performing default authentication that retrieves a token for Azure Databricks. </p> <p>Also refer to the Code Reference for further technical information.</p> <pre><code>from rtdip_sdk.authentication import authenticate as auth\n\nauthentication = auth.DefaultAuth().authenticate()\naccess_token = authentication.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\n</code></pre> <p>Note</p> <p>If you are experiencing any trouble authenticating please see Troubleshooting - Authentication</p> <p>Refer to the Databricks documentation for further information about generating a Databricks PAT Token. Below is an example of performing default authentication that retrieves a token for a Databricks Workspace. </p> <p>Provide your <code>dbapi.....</code> token to the <code>access_token</code> in the examples below.</p> <pre><code>access_token = \"dbapi..........\"\n</code></pre>"},{"location":"sdk/queries/databricks/databricks-sql/#connect-to-databricks-sql","title":"Connect to Databricks SQL","text":"<p>The RTDIP SDK offers several ways to connect to a Databricks SQL Warehouse.</p> Databricks SQL ConnectorPYODBCTURBODBC <p>The simplest method to connect to RTDIP and does not require any additional installation steps.</p> <pre><code>from rtdip_sdk.connectors import DatabricksSQLConnection\n\nserver_hostname = \"server_hostname\"\nhttp_path = \"http_path\"\naccess_token = \"token\"\n\nconnection = DatabricksSQLConnection(server_hostname, http_path, access_token)\n</code></pre> <p>Replace server_hostname, http_path and access_token with your own information.</p> <p>For more information about each of the connection methods, please see Code Reference and navigate to the required section.</p> <p>A popular library that python developers use for ODBC connectivity but requires more setup steps.</p> <p>ODBC or JDBC are required to leverage PYODBC. Follow these instructions to install the drivers in your environment.</p> <ul> <li> <p>Microsoft Visual C++ 14.0 or greater is required. Get it from Microsoft C++ Build Tools</p> </li> <li> <p>Driver paths can be found on PYODBC Driver Paths</p> </li> </ul> <pre><code>from rtdip_sdk.connectors import PYODBCSQLConnection\n\nserver_hostname = \"server_hostname\"\nhttp_path = \"http_path\"\naccess_token = \"token\"\ndriver_path = \"/Library/simba/spark/lib/libsparkodbc_sbu.dylib\"\n\nconnection = PYODBCSQLConnection(driver_path, sever_hostname, http_path, access_token)\n</code></pre> <p>Replace server_hostname, http_path and access_token with your own information.</p> <p>For more information about each of the connection methods, please see Code Reference and navigate to the required section.</p> <p>The RTDIP development team have found this to be the most performant method of connecting to RTDIP leveraging the arrow implementation within Turbodbc to obtain data, but requires a number of additional installation steps to get working on OSX, Linux and Windows</p> <ul> <li>ODBC or JDBC are required to leverage TURBODBC. Follow these instructions to install the drivers in your environment.</li> <li>Boost needs to be installed locally to use the TURBODBC SQL Connector (Optional)</li> </ul> <pre><code>from rtdip_sdk.connectors import TURBODBCSQLConnection\n\nserver_hostname = \"server_hostname\"\nhttp_path = \"http_path\"\naccess_token = \"token\"\n\nconnection = TURBODBCSQLConnection(server_hostname, http_path, access_token)\n</code></pre> <p>Replace server_hostname, http_path and access_token with your own information.</p> <p>For more information about each of the connection methods, please see Code Reference and navigate to the required section.</p>"},{"location":"sdk/queries/databricks/databricks-sql/#functions","title":"Functions","text":"<p>Finally, after authenticating and connecting using one of the methods above, you have access to the commonly requested RTDIP functions such as Resample, Interpolate, Raw, Time Weighted Averages or Metadata. </p> <p>1. To use any of the RTDIP functions, use the commands below.</p> <pre><code>from rtdip_sdk.queries import resample\nfrom rtdip_sdk.queries import interpolate\nfrom rtdip_sdk.queries import raw\nfrom rtdip_sdk.queries import time_weighted_average\nfrom rtdip_sdk.queries import metadata\n</code></pre> <p>2. From functions you can use any of the following methods.</p>"},{"location":"sdk/queries/databricks/databricks-sql/#resample","title":"Resample","text":"<pre><code>resample.get(connection, parameters_dict)\n</code></pre>"},{"location":"sdk/queries/databricks/databricks-sql/#interpolate","title":"Interpolate","text":"<pre><code>interpolate.get(connection, parameters_dict)\n</code></pre>"},{"location":"sdk/queries/databricks/databricks-sql/#raw","title":"Raw","text":"<pre><code>raw.get(connection, parameters_dict)\n</code></pre>"},{"location":"sdk/queries/databricks/databricks-sql/#time-weighted-average","title":"Time Weighted Average","text":"<pre><code>time_weighted_average.get(connection, parameter_dict)\n</code></pre>"},{"location":"sdk/queries/databricks/databricks-sql/#metadata","title":"Metadata","text":"<pre><code>metadata.get(connection, parameter_dict)\n</code></pre> <p>For more information about the function parameters see Code Reference and navigate through the required function.</p>"},{"location":"sdk/queries/databricks/databricks-sql/#example","title":"Example","text":"<p>This is a code example of the RTDIP SDK Interpolate function. You will need to replace the parameters with your own requirements and details. If you are unsure on the options please see Code Reference - Interpolate and navigate to the attributes section. </p> <pre><code>from rtdip_sdk.authentication import authenticate as auth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import interpolate\n\nauthentication = auth.DefaultAuth().authenticate()\naccess_token = authentication.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", access_token)\n\nparameters = {\n    \"business_unit\": \"{business_unit}\", \n    \"region\": \"{region}\",\n    \"asset\": \"{asset}\", \n    \"data_security_level\": \"{date_security_level}\",\n    \"data_type\": \"{data_type}\", #options are float, integer, string and double (the majority of data is float)\n    \"tag_names\": [\"{tag_name_1}, {tag_name_2}\"],\n    \"start_date\": \"2022-03-08\", #start_date can be a date in the format \"YYYY-MM-DD\" or a datetime in the format \"YYYY-MM-DDTHH:MM:SS\"\n    \"end_date\": \"2022-03-10\", #end_date can be a date in the format \"YYYY-MM-DD\" or a datetime in the format \"YYYY-MM-DDTHH:MM:SS\"\n    \"time_interval_rate\": \"1\", #numeric input\n    \"time_interval_unit\": \"hour\", #options are second, minute, day or hour\n    \"agg_method\": \"first\", #options are first, last, avg, min, max\n    \"interpolation_method\": \"forward_fill\", #options are forward_fill, backward_fill or linear\n    \"include_bad_data\": True #boolean options are True or False\n}\n\nresult = interpolate.get(connection, parameters)\nprint(result)\n</code></pre> <p>Note</p> <p>If you are having problems please see Troubleshooting for more information.</p>"},{"location":"sdk/queries/databricks/databricks-sql/#conclusion","title":"Conclusion","text":"<p>Congratulations! You have now learnt how to use the RTDIP SDK. Please check back for regular updates and if you would like to contribute, you can open an issue on GitHub. See the Contributing Guide for more help.</p>"},{"location":"sdk/queries/databricks/sql-warehouses/","title":"SQL Warehouses","text":"<p>In order to connect to the data using the RTDIP SDK you will require Databricks SQL Warehouse information. Retrieve this information from your Databricks Workspace by following the steps below:</p> <ol> <li>Login to your Databricks Workspace</li> <li>Switch to the SQL Option in the Workspace</li> <li>Select the SQL Warehouse</li> <li>Click on the Details tab</li> <li>Copy the Host Name and HTTP Path details</li> </ol>"},{"location":"sdk/queries/databricks/troubleshooting/","title":"Troubleshooting","text":""},{"location":"sdk/queries/databricks/troubleshooting/#cannot-install-pyodbc","title":"Cannot install pyodbc","text":"<p>Microsoft Visual C++ 14.0 or greater is required to install pyodbc. Get it with Microsoft C++ Build Tools</p>"},{"location":"sdk/queries/databricks/troubleshooting/#cannot-build-wheels-using-legacy-setuppy","title":"Cannot build wheels (Using legacy setup.py)","text":"<p>To install rtdip-sdk using setup.py, you need to have wheel installed using the following command:</p> <pre><code>pip install wheel\n</code></pre>"},{"location":"sdk/queries/databricks/troubleshooting/#authentication","title":"Authentication","text":"<p>For Default Credential authentication, a number of troubleshooting options are available here.</p> <p>For Visual Studio Code errors, the version of Azure Account extension is installed(0.9.11) - To authenticate in Visual Studio Code, ensure version 0.9.11 or earlier of the Azure Account extension is installed. To track progress toward supporting newer extension versions, see this GitHub issue. Once installed, open the Command Palette and run the Azure: Sign In command</p>"},{"location":"sdk/queries/databricks/troubleshooting/#exception-has-occurred-typeerror-module-object-is-not-callable","title":"Exception has occurred: TypeError 'module' object is not callable","text":"<p>Ensure you are importing and using the RTDIP SDK functions correctly. You will need to give the module a name and reference it when using the function. See below for a code example. </p> <pre><code>from rtdip_sdk.authentication import authenticate as auth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import interpolate\n\nauthentication = auth.DefaultAuth().authenticate()\naccess_token = authentication.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", access_token)\n\ndict = {\n    \"business_unit\": \"{business_unit}\", \n    \"region\": \"{region}\", \n    \"asset\": \"{asset}\", \n    \"data_security_level\": \"{date_security_level}\",\n    \"data_type\": \"{data_type}\", #options are float, integer, string and double (the majority of data is float)\n    \"tag_names\": [\"{tag_name_1}, {tag_name_2}\"],\n    \"start_date\": \"2022-03-08\", #start_date can be a date in the format \"YYYY-MM-DD\" or a datetime in the format \"YYYY-MM-DDTHH:MM:SS\"\n    \"end_date\": \"2022-03-10\", #end_date can be a date in the format \"YYYY-MM-DD\" or a datetime in the format \"YYYY-MM-DDTHH:MM:SS\"\n    \"time_interval_rate\": \"1\", #numeric input\n    \"time_interval_unit\": \"hour\", #options are second, minute, day, hour\n    \"agg_method\": \"first\", #options are first, last, avg, min, max\n    \"interpolation_method\": \"forward_fill\", #options are forward_fill or backward_fill\n    \"include_bad_data\": True #boolean options are True or False\n}\n\nresult = interpolate.get(connection, dict)\nprint(result)\n</code></pre>"},{"location":"sdk/queries/databricks/troubleshooting/#databricks-odbcjdbc-driver-issues","title":"Databricks ODBC/JDBC Driver issues","text":""},{"location":"sdk/queries/databricks/troubleshooting/#general-troubleshooting","title":"General Troubleshooting","text":"<p>Most issues related to the installation or performance of the ODBC/JDBC driver are documented here.</p>"},{"location":"sdk/queries/databricks/troubleshooting/#odbc-with-a-proxy","title":"ODBC with a proxy","text":"<p>Follow this document to use the ODBC driver with a proxy.  </p>"},{"location":"sdk/queries/llm/chatopenai_databricks_connector/","title":"Chat Open AI Databricks Connector","text":"<p>The Chat Open AI(Chat GPT) Databricks Connector leverages langchain to execute SQL queries on tables in Databricks. Keep in mind that you can try this out on any data in Databricks, it is not limited to data ingested by RTDIP pipelines.</p> <p>Note</p> <p>This is experimental and you will likely experience variable responses to your questions depending on the complexity of the data you use in this setup. Start small, with only a 2 - 3 tables before scaling up.</p>"},{"location":"sdk/queries/llm/chatopenai_databricks_connector/#prerequisites","title":"Prerequisites","text":"<ol> <li>Obtain an Open AI API key. Register on the Open AI platform to obtain an Open AI API key, follow these instructions to obtain your API key and familiarize yourself with the Open AI documentation. </li> <li>Access to Databricks SQL or a Databricks Cluster and data stored as tables in Unity Catalog or Hive Metastore. </li> </ol> <p>Warning</p> <p>Consider the implications of exposing your data with Open AI models and seek approval prior to registering with any of these services. </p>"},{"location":"sdk/queries/llm/chatopenai_databricks_connector/#setup-the-sql-ai-agent","title":"Setup the SQL AI Agent","text":"<p>With all the prerequisites in place, it's time to setup the SQL AI Agent.</p> <p>Firstly, import the required RTDIP SDK components</p> <pre><code>from rtdip_sdk.connectors import ChatOpenAIDatabricksConnection\n</code></pre> <p>Next, configure the component with all the relevant connection information:</p> <pre><code>agent = ChatOpenAIDatabricksConnection(\n    catalog=\"&lt;databricks catalog&gt;\", \n    schema=\"&lt;databricks schema&gt;\", \n    server_hostname=\"&lt;databricks host name&gt;\",                   \n    http_path=\"&lt;databricks http path&gt;\",                         \n    access_token=\"&lt;Azure AD token or databricks PAT token&gt;\",\n    openai_api_key=\"&lt;Open AI API key&gt;\",\n    openai_model = \"gpt-4\",                                     \n    sample_rows_in_table_info = 5, \n    verbose_logging = True\n)\n</code></pre> <p>Some notes on the above:</p> <ul> <li><code>server_hostname</code> and <code>http_path</code> can be obtained from your Databricks SQL Warehouse or Databricks Cluster.</li> <li><code>access_token</code> can be either a Databricks PAT Token or Azure AD Token. To obtain an Azure AD token, please refer to this documentation</li> <li><code>open_ai_model</code> defaults to <code>gpt-4</code> but is not easily available at the time of writing. Alternatively, the <code>gpt-3.5-turbo-16k-0613</code> has worked well in our tests</li> <li><code>sample_rows_in_table_info</code> limits the number of rows queried in a table when the SQL Database Agent is looking context in the data. Be careful to not increase this too much as its then possible to exceed token limits on the gpt models</li> </ul>"},{"location":"sdk/queries/llm/chatopenai_databricks_connector/#execute-queries","title":"Execute Queries","text":"<p>With the <code>ChatOpenAIDatabricksConnection</code> class configured, its now possible to execute queries. Try out some questions with the following code:</p> <pre><code>response = agent.run(\"What was the average actual power generated by Turbine 1 at ACME Wind Farm on 6 May?\")\nprint(response)\n</code></pre> <p>And the answer:</p> <pre><code>\"The average power generated for turbine 1 at Acme on 6 May is 847.4923418083226.\"\n</code></pre>"},{"location":"sdk/queries/llm/chatopenai_databricks_connector/#limitations","title":"Limitations","text":"<p>The biggest limitation experienced to date is the token limit on the Chat GPT models. These limits can be quickly exceeded as the SQL AI Agent queries the data and uses the responses as tokens to find context and answers to questions. Some of this can be reduced by using the Chat GPT models that allow for higher token limits, like the <code>gpt-3.5-turbo-16k-0613</code> model. However, if the Databricks schema has many tables and needs to do a number queries to determine the answer, 16k tokens is still easily exceeded.</p> <p>The connector does not perform much cross querying of tables to solve answers. For example, the agent does not try to query a metadata table to then solve a question for related data in another table. </p> <p>The answers are not always the same and asking the same question multiple times did not always provide the same answer(however, because its an Intelligence based solution, there could be good reasons for that).</p>"},{"location":"sdk/queries/spark/spark-connect/","title":"Spark Connect","text":"<p>Spark Connect was released in Apache Spark 3.4.0 to enable a decoupled client-server architecture that allows remote connectivity to Spark clusters using the Spark DataFrame API.</p> <p>This means any Spark cluster could provide compute to a spark job and therefore enables options such as Spark on Kubernetes, Spark running locally or Databricks Interactive Clusters to be leveraged in the RTDIP SDK to perform time series queries.</p>"},{"location":"sdk/queries/spark/spark-connect/#prerequisites","title":"Prerequisites","text":"<p>Please ensure that you have followed the instructions to enable Spark Connect on your Spark cluster and that you are using a <code>pyspark&gt;=3.4.0</code>. If you are connecting to Databricks, then install <code>databricks-connect&gt;=13.0.1</code> instead of <code>pyspark</code>.</p>"},{"location":"sdk/queries/spark/spark-connect/#example","title":"Example","text":"<p>This <pre><code>from rtdip_sdk.connectors import SparkConnection\n\nspark_server = \"sparkserver.com\"\naccess_token = \"my_token\"\n\nspark_remote = \"sc://{}:443;token={}\".format(spark_server, access_token)\nconnection = SparkConnection(spark_remote=spark_remote)\n</code></pre></p> <p>Replace the access_token with your own information(this assumes an access token is required to authenticate with the remote Spark server).</p>"},{"location":"blog/archive/2023/","title":"2023","text":""},{"location":"blog/archive/2022/","title":"2022","text":""}]}